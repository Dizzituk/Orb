# Voice Feature Configuration

# ============================================================================
# Model Configuration
# ============================================================================

# Default model to load on startup (leave empty to not auto-load)
# Examples: "base", "small", "medium", "large-v2", "large-v3"
DEFAULT_WHISPER_MODEL=base

# Model directory for caching downloaded models
# If not set, uses the default cache directory (~/.cache/huggingface)
MODEL_CACHE_DIR=

# Device to use for inference: "auto", "cuda", or "cpu"
# "auto" will detect GPU availability automatically
TRANSCRIPTION_DEVICE=auto

# Compute type for faster-whisper: "int8", "float16", "float32"
# int8 is fastest but less accurate, float16 requires GPU, float32 is most accurate
COMPUTE_TYPE=int8

# Number of CPU threads to use when running on CPU
CPU_THREADS=4

# ============================================================================
# Audio Processing Configuration
# ============================================================================

# Audio sample rate for processing (Hz)
AUDIO_SAMPLE_RATE=16000

# Number of audio channels (1 = mono, 2 = stereo)
# Whisper expects mono audio
AUDIO_CHANNELS=1

# Audio chunk duration for streaming (seconds)
AUDIO_CHUNK_DURATION=0.5

# ============================================================================
# Wake Word Detection Configuration
# ============================================================================

# Enable wake word detection (true/false)
ENABLE_WAKE_WORD=true

# Wake word model to use (openwakeword model name)
# Examples: "hey_jarvis", "alexa", "hey_mycroft"
WAKE_WORD_MODEL=hey_jarvis

# Wake word detection threshold (0.0-1.0)
# Higher values require more confident detections
WAKE_WORD_THRESHOLD=0.5

# Voice Activity Detection (VAD) threshold (0.0-1.0)
VAD_THRESHOLD=0.5

# ============================================================================
# Transcription Configuration
# ============================================================================

# Default language for transcription (ISO 639-1 code)
# Leave empty for automatic language detection
# Examples: "en", "es", "fr", "de", "zh"
DEFAULT_LANGUAGE=en

# Beam size for transcription (higher = more accurate but slower)
BEAM_SIZE=5

# Enable timestamps in transcription segments
ENABLE_TIMESTAMPS=true

# Minimum silence duration to split transcription (seconds)
MIN_SILENCE_DURATION=1.0

# ============================================================================
# API Configuration
# ============================================================================

# Maximum audio file size for upload (bytes)
# Default: 10MB
MAX_AUDIO_FILE_SIZE=10485760

# WebSocket connection timeout (seconds)
WEBSOCKET_TIMEOUT=300

# Maximum concurrent transcription requests
MAX_CONCURRENT_REQUESTS=5

# ============================================================================
# Frontend Configuration
# ============================================================================

# Backend API URL (used by frontend)
VITE_API_BASE_URL=http://localhost:8000

# WebSocket URL (used by frontend)
VITE_WS_BASE_URL=ws://localhost:8000

# Enable debug logging in frontend
VITE_DEBUG=false