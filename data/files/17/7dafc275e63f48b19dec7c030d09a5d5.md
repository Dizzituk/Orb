# Orb Architecture Map

**Version:** 0.12.17 (Architecture Map v23)  
**Last Updated:** 07 December 2025  
**Purpose:** Canonical architecture reference for the Orb multi-LLM AI assistant system

---

## Table of Contents

1. [System Overview](#system-overview)  
2. [Component Architecture](#component-architecture)  
3. [Backend (`Orb`)](#backend-orb)  
4. [Desktop Client (`orb-desktop`)](#desktop-client-orb-desktop)  
5. [Data Layer](#data-layer)  
6. [Authentication System](#authentication-system)  
7. [Database Encryption](#database-encryption)  
8. [Multi-LLM Orchestration](#multi-llm-orchestration)  
9. [5-Type Routing System](#5-type-routing-system)  
10. [Legacy Job Types & Migration](#legacy-job-types--migration)  
11. [Job Classifier](#job-classifier)  
12. [Router Debug Mode](#router-debug-mode)  
13. [Phase 4: Job Engine & Artefacts](#phase-4-job-engine--artefacts)  
14. [Provider Registry](#provider-registry)  
15. [DateTime Context](#datetime-context)  
16. [Streaming Responses](#streaming-responses)  
17. [Reasoning Panel](#reasoning-panel)  
18. [Markdown Rendering](#markdown-rendering)  
19. [Web Search](#web-search)  
20. [Chat History](#chat-history)  
21. [File Ingestion Pipeline](#file-ingestion-pipeline)  
22. [Image Analysis Pipeline](#image-analysis-pipeline)  
23. [Video Analysis Pipeline](#video-analysis-pipeline)  
24. [Semantic Search System](#semantic-search-system)  
25. [Error Taxonomy](#error-taxonomy)  
26. [Quick Start](#quick-start)  
27. [Troubleshooting](#troubleshooting)  
28. [Testing Routing](#testing-routing)  
29. [Future Capabilities](#future-capabilities)  
30. [Changelog](#changelog)  
31. [Testing Checklist](#testing-checklist)

---

## System Overview

Orb is a personal AI assistant built as a multi-component system with specialized LLM roles:

- **GPT (OpenAI)** — Fast/lightweight reasoning, conversational interface, linguistics, embeddings  
- **Claude (Anthropic)** — Flagship engineer, complex code, architecture design  
- **Gemini (Google)** — Critic, reviewer, analyst, vision specialist, video analysis, web search  

### Core Vision

Different AI models specialize in distinct roles while sharing a unified memory layer, enabling:

- Persistent knowledge management across conversations  
- Task coordination and project organization  
- **5-type job classification system** (TEXT_ADMIN, SMALL_CODE, BIG_ARCHITECTURE, SIMPLE_VISION, HEAVY_MULTIMODAL_CRITIQUE)
- Automatic job-type classification from message content and attachments
- Job-type-based automatic model routing  
- Policy-driven routing with JSON configuration
- **Router debug mode with comprehensive routing visibility** (v0.12.17)
- **Clean message classification without filename injection** (v0.12.17)
- File upload with text extraction and document analysis  
- CV parsing with structured data extraction  
- Image analysis via Gemini Vision (with OpenAI fallback)
- **Video analysis via Gemini File API** (v0.12.7)
- **Video deep-analysis semantic detection** (v0.12.8)
- **Attachment safety rules preventing silent Claude fallback** (v0.12.8)
- Binary file detection (skip text extraction for media files)
- Semantic search (RAG) with vector embeddings
- Auto-indexing of notes, messages, and files into embeddings
- Password-based authentication with session tokens
- Project selector for multi-project management
- Database encryption at rest (Security Level 4: Master Key)
- Streaming LLM responses with routing and reasoning capture
- Web search with real-time grounding
- Automatic datetime context in all LLM calls
- Chat history persistence and loading
- Markdown rendering for assistant messages with syntax highlighting
- Reasoning/thinking panel showing LLM chain-of-thought
- Model badges identifying which LLM answered each message
- Phase 4: Structured job system with envelopes, results, and artefacts (optional)

### Technology Stack

**Backend:**

- FastAPI (Python 3.13)  
- SQLAlchemy ORM  
- SQLite database  
- Pydantic schemas  
- bcrypt for password hashing  
- cryptography (Fernet) for field-level encryption  
- `python-multipart` for `multipart/form-data` file uploads  
- `python-docx` for DOCX text extraction  
- `PyMuPDF` (fitz) for PDF text extraction  
- `Pillow` for image processing (Gemini vision)
- `google-genai` for Gemini Web Search (new SDK, preferred)  
- `google-generativeai` for Gemini Vision + Video + fallback (old SDK)  
- OpenAI `text-embedding-3-small` for semantic embeddings

**Desktop Client:**

- Electron (v33.0.0)  
- React 18 with TypeScript 5  
- Vite 5 (build tool and dev server)  
- Custom React hooks for state management  
- `keytar` for Windows Credential Manager integration
- `react-markdown` for markdown rendering
- `remark-gfm` for GitHub-flavored markdown
- `react-syntax-highlighter` for code block highlighting

**Platform:** Windows 11

---

## Component Architecture

```text
D:/
├── Orb/                        # Backend (FastAPI server)
│   ├── main.py                 # Application entrypoint, chat endpoints, RAG, classification (v0.12.17 message fix)
│   ├── .env                    # API keys + model configuration + debug flags (v0.12.17)
│   ├── app/
│   │   ├── db.py               # Database configuration
│   │   ├── auth/               # Authentication module (password-based)
│   │   ├── crypto/             # Encryption module (master key)
│   │   ├── memory/             # Memory subsystem (projects/notes/tasks/files/messages)
│   │   ├── llm/                # LLM routing + vision + video + streaming + web search
│   │   │   ├── __init__.py     # Module exports
│   │   │   ├── schemas.py      # JobType, LLMTask, LLMResult, RoutingOptions, RoutingConfig
│   │   │   ├── clients.py      # Provider API wrappers (via registry)
│   │   │   ├── router.py       # Main router with call_llm(), _select_provider_for_job_type(), _check_attachment_safety(), debug logging (v0.12.17)
│   │   │   ├── job_classifier.py  # Automatic job classification (v0.12.7), video deep-analysis (v0.12.8), debug logging (v0.12.17)
│   │   │   ├── stream_router.py   # Streaming endpoint with routing (_classify_job_type)
│   │   │   ├── streaming.py       # Stream generators, THINKING/ANSWER tag extraction
│   │   │   ├── policy.py          # JSON-based routing policy (v0.12.12)
│   │   │   ├── gemini_vision.py   # Image + Video analysis via Gemini
│   │   │   ├── file_analyzer.py   # Text extraction + binary detection + MIME helpers
│   │   │   └── web_search_router.py # Web search endpoints
│   │   ├── embeddings/         # Semantic search with vector embeddings + auto-indexing
│   │   │   ├── models.py       # Embedding ORM model
│   │   │   ├── service.py      # Indexing and search logic
│   │   │   └── router.py       # Embeddings API endpoints
│   │   ├── jobs/               # Phase 4: Job engine (OPTIONAL)
│   │   │   ├── schemas.py      # JobEnvelope, JobResult, etc.
│   │   │   ├── engine.py       # Job orchestration
│   │   │   └── router.py       # Jobs API endpoints
│   │   ├── artefacts/          # Phase 4: Artefact storage (OPTIONAL)
│   │   │   ├── schemas.py      # Artefact schemas
│   │   │   ├── service.py      # Artefact CRUD
│   │   │   └── router.py       # Artefacts API endpoints
│   │   ├── providers/          # Phase 4: Provider registry (OPTIONAL)
│   │   │   └── registry.py     # Single source of truth for LLM calls
│   │   └── tools/              # Tool integrations
│   ├── data/                   # SQLite DB + file storage + auth config
│   │   ├── orb_memory.db       # SQLite database
│   │   ├── auth.json           # Password hash + session storage
│   │   ├── encryption_salt.bin # Legacy key derivation (migration only)
│   │   ├── routing_policy.json # LLM routing policy configuration
│   │   └── files/              # Per-project file storage
│   └── scripts/                # Migration and utility scripts
│       └── add_model_column.py # Migration script for model column
│
├── orb-desktop/                # Electron + React desktop client
│   ├── main.js                 # Electron main process
│   ├── src/
│   │   ├── App.tsx             # Root component
│   │   ├── components/         # React components
│   │   ├── hooks/              # Custom React hooks
│   │   ├── services/           # API client layer
│   │   │   └── api.ts          # HTTP client
│   │   ├── types/              # TypeScript interfaces
│   │   │   └── index.ts        # Message, ChatResponse, etc.
│   │   └── styles/             # CSS styles
│   └── ...
│
└── Windows Credential Manager
    └── OrbMasterKey/default    # 32-byte master key (URL-safe base64)
```

### Data Flow (Three Paths with Unified Routing)

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 1: Non-Streaming Chat (/chat, /chat_with_attachments)                  │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /chat                                                             │
│ main.py /chat endpoint                                                       │
│    ↓ _classify_job_type() → job_classifier.classify_message()               │
│    ↓ builds LLMTask with classified job_type                                │
│ app/llm/router.py call_llm() → call_llm_async()                             │
│    ↓ classify_and_route() — 5-type classification                           │
│    ↓ _check_attachment_safety() — prevent silent Claude fallback (v0.12.8) │
│    ↓ [router-debug] logs (if ORB_ROUTER_DEBUG=1, v0.12.17)                  │
│    ↓ synthesizes JobEnvelope                                                │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs (OpenAI / Anthropic / Google)                                 │
│    ↓                                                                        │
│ Returns: ChatResponse { provider, model, reply, ... }                       │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 1b: Media Analysis (/chat_with_attachments with images/video)          │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /chat_with_attachments (multipart/form-data)                      │
│ main.py                                                                      │
│    ↓ Clean message handling (v0.12.17): NO filename injection               │
│    ↓ Attachment metadata passed separately to classifier                    │
│    ↓ is_image_mime_type() / is_video_mime_type() detection                  │
│    ↓ check_vision_available()                                               │
│    ↓ Video: analyze_video() → Gemini File API                               │
│    ↓ Image: ask_about_image() → Gemini Vision                               │
│    ↓ Media without message → uses default prompt (v0.12.8)                  │
│ app/llm/gemini_vision.py                                                    │
│    ↓ Model tier selection (file size + semantic analysis)                   │
│ Gemini Vision API                                                            │
│    ↓                                                                        │
│ Returns: ChatResponse { provider: "google", model: "gemini-2.x-...", ... }  │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 2: Streaming Chat (/stream/chat)                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /stream/chat                                                      │
│ app/llm/stream_router.py                                                    │
│    ↓ _classify_job_type() — 5-type classification                           │
│    ↓ _select_provider_for_job_type() — route based on job type              │
│ app/llm/streaming.py stream_llm(provider, model)                            │
│    ↓ streaming to selected provider                                         │
│    ↓ THINKING/ANSWER tag extraction for reasoning                           │
│ Provider APIs (streaming)                                                    │
│    ↓                                                                        │
│ SSE Events: metadata → tokens → reasoning → done                            │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 3: Phase 4 Jobs (/jobs) — OPTIONAL                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /jobs/create                                                      │
│ app/jobs/router.py                                                          │
│    ↓ creates JobEnvelope                                                    │
│ app/jobs/engine.py                                                          │
│    ↓ orchestrates job                                                       │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Backend (`Orb`)

**Location:** `D:/Orb/`  
**Framework:** FastAPI  
**Entry Point:** `main.py`  
**Version:** 0.12.17

### Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/ping` | GET | Health check |
| `/auth/setup` | POST | Set password |
| `/auth/validate` | POST | Login |
| `/chat` | POST | Non-streaming chat (with routing) |
| `/stream/chat` | POST | Streaming chat (with routing) |
| `/chat_with_attachments` | POST | Upload files + chat (images/video route to Gemini) |
| `/llm` | POST | Direct LLM call |
| `/memory/projects` | GET/POST | Project management |
| `/memory/messages` | GET | Message history |
| `/embeddings/search` | POST | Semantic search |
| `/search/web` | POST | Web search |
| `/jobs/*` | * | Phase 4 (optional) |
| `/artefacts/*` | * | Phase 4 (optional) |

### Environment Variables

```env
# API Keys
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AI...

# Model Configuration (5-type system)
OPENAI_DEFAULT_MODEL=gpt-4.1-mini
ANTHROPIC_SONNET_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20251101

# Gemini Vision Models
GEMINI_VISION_MODEL_FAST=gemini-2.0-flash
GEMINI_VISION_MODEL=gemini-2.0-flash
GEMINI_VISION_MODEL_COMPLEX=gemini-2.5-pro
GEMINI_VIDEO_DEEP_MODEL=gemini-3.0-pro-preview

# Debug Mode (v0.12.17)
ORB_ROUTER_DEBUG=1  # Enable comprehensive routing logs

# Optional
ORB_ENABLE_PHASE4=false
```

---

## Desktop Client (`orb-desktop`)

**Location:** `D:/orb-desktop/`  
**Framework:** Electron + React + TypeScript  
**Version:** (matches backend)

### Key Components

| Component | Purpose |
|-----------|---------|
| `App.tsx` | Root component with routing |
| `ChatInterface.tsx` | Main chat UI with message display |
| `MessageList.tsx` | Message rendering with markdown |
| `ProjectSelector.tsx` | Switch between projects |
| `FileUpload.tsx` | Drag-and-drop file handling |
| `ReasoningPanel.tsx` | Display LLM thinking process |
| `api.ts` | HTTP client for backend communication |

### UI Features

- Real-time markdown rendering with syntax highlighting
- Model badges (GPT/Claude/Gemini) on each message
- Collapsible reasoning panel for chain-of-thought
- File drag-and-drop with preview
- Project-scoped chat history
- Responsive layout

---

## Data Layer

**Database:** SQLite (`data/orb_memory.db`)

### Schema

| Table | Columns | Purpose |
|-------|---------|---------|
| `projects` | id, name, description, created_at, updated_at | Project management |
| `notes` | id, project_id, title, content, tags, created_at, updated_at | Note storage |
| `tasks` | id, project_id, title, description, status, priority, due_date, created_at, updated_at | Task tracking |
| `files` | id, project_id, filename, filepath, size_bytes, mime_type, doc_type, summary, created_at | File metadata |
| `messages` | id, project_id, role, content, provider, model, created_at | Chat history |
| `embeddings` | id, entity_type, entity_id, chunk_text, embedding, created_at | Vector embeddings for semantic search |

**Note:** `model` column added in v0.12.1 for model badge display

---

## Authentication System

**Location:** `app/auth/`

**Mode:** Password-based authentication with session tokens

**Flow:**

1. User enters password in Electron UI
2. Frontend sends password to `/auth/login`
3. Backend verifies bcrypt hash from `data/auth.json`
4. On success, generates session token (UUID)
5. Token stored in-memory on backend, returned to frontend
6. Frontend includes token in `Authorization` header for all requests

**Session Management:**

- Sessions stored in `data/auth.json`
- No expiration (persistent across restarts)
- `/auth/check` endpoint validates token

---

## Database Encryption

**Location:** `app/crypto/`

**Security Level:** 4 (Master Key)

### Architecture

```text
┌─────────────────────────────────────────────────────────┐
│ Windows Credential Manager (OrbMasterKey/default)       │
│   ↓                                                      │
│ 32-byte master key (URL-safe base64, 43 chars)         │
│   ↓                                                      │
│ Fernet encryption (field-level)                         │
│   ↓                                                      │
│ Encrypted fields: notes.content, tasks.description      │
└─────────────────────────────────────────────────────────┘
```

**Key Points:**

- Master key generated once by Electron, stored in Windows Credential Manager
- Password used ONLY for authentication, NOT for encryption
- Backend retrieves master key from Credential Manager on startup
- Fernet symmetric encryption for field-level protection

**Migration:** Security Level 3 (password-derived key) → Level 4 (master key) completed

---

## Multi-LLM Orchestration

### Provider Roles

| Provider | Primary Use Cases |
|----------|-------------------|
| **OpenAI (GPT)** | Chat, embeddings, fast responses, casual questions, text/admin tasks |
| **Anthropic (Claude)** | Complex code, architecture, engineering, code review |
| **Gemini (Google)** | Vision, video analysis, web search, review/critique |

### Default Models (Environment Variables)

| Provider | Environment Variable | Default |
|----------|---------------------|---------|
| OpenAI (light) | `OPENAI_DEFAULT_MODEL` | `gpt-4.1-mini` |
| Anthropic (code) | `ANTHROPIC_SONNET_MODEL` | `claude-sonnet-4-5-20250929` |
| Anthropic (arch) | `ANTHROPIC_OPUS_MODEL` | `claude-opus-4-5-20251101` |
| Gemini (fast) | `GEMINI_VISION_MODEL_FAST` | `gemini-2.0-flash` |
| Gemini (default) | `GEMINI_VISION_MODEL` | `gemini-2.0-flash` |
| Gemini (complex) | `GEMINI_VISION_MODEL_COMPLEX` | `gemini-2.5-pro` |
| Gemini (video deep) | `GEMINI_VIDEO_DEEP_MODEL` | `gemini-3.0-pro-preview` |

---

## 5-Type Routing System

**Location:** `app/llm/schemas.py` (RoutingConfig class)

The routing system classifies all requests into exactly 5 job types, each with a designated provider/model:

| Job Type | Provider | Model | Use Cases |
|----------|----------|-------|-----------|
| `TEXT_ADMIN` | OpenAI | `gpt-4.1-mini` | Emails, blogs, docs, spreadsheets, planning, explanations, casual chat |
| `SMALL_CODE` | Anthropic | `claude-sonnet-4-5-20250929` | 1-3 files, clearly scoped, low risk patches, bug fixes |
| `BIG_ARCHITECTURE` | Anthropic | `claude-opus-4-5-20251101` | Multi-file, system-level, routing/memory/security, major features |
| `SIMPLE_VISION` | Google | `gemini-2.0-flash` | Single screenshot, small PDF, basic OCR, short clips |
| `HEAVY_MULTIMODAL_CRITIQUE` | Google | `gemini-2.5-pro` | Long video (>10MB), complex PDFs, mixed media, critique/review |

### Hard Rules (Enforced)

1. **Never send big code to GPT or Sonnet** → Always Opus for architecture
2. **Never send simple screenshots to Gemini 2.5 Pro** → Use Flash for simple vision
3. **Never send deep video to Gemini Flash** → Large videos use Pro
4. **If ambiguous between SMALL_CODE and BIG_ARCHITECTURE** → Choose BIG_ARCHITECTURE
5. **Images/PDFs never go to Claude** → Route to Gemini
6. **PDFs without images** → Route to GPT as text.heavy
7. **Attachments without job_type never silently fall back to Claude** → Route to GPT (v0.12.8)
8. **Filename metadata never injected into user message for classification** → Prevents false keyword matches (v0.12.17)

### Keyword Detection

```python
TEXT_ADMIN_KEYWORDS = {
    "email", "letter", "report", "blog", "readme", "documentation",
    "plan", "brainstorm", "summary", "summarize", "explain",
    "note", "copywriting", "prompt", "rewriting", "research",
    "spreadsheet", "planning", "casual"
}

SMALL_CODE_KEYWORDS = {
    "fix this bug", "small fix", "quick fix", "add a function",
    "helper function", "tweak", "minor change", "single file",
    "debug", "small bugfix", "implement function"
}

BIG_ARCHITECTURE_KEYWORDS = {
    "architect", "architecture", "multi-file", "refactor",
    "routing", "memory", "database", "schema", "security",
    "system design", "migration", "phase", "v0.",
    "microservice", "infrastructure", "scalab", "production-ready",
    "complete implementation", "from scratch"
}

SIMPLE_VISION_KEYWORDS = {
    "screenshot", "what is this", "ocr", "read this",
    "small image", "describe this", "single image"
}

HEAVY_MULTIMODAL_CRITIQUE_KEYWORDS = {
    "video", "youtube", "reel", "lecture", "complex pdf",
    "critique", "review this design", "deep analysis",
    "multiple images", "mixed media"
}

# NEW in v0.12.8: Video Deep Analysis Detection
VIDEO_DEEP_ANALYSIS_KEYWORDS = {
    "find best shots", "extract narrative", "segment scenes",
    "identify key scenes", "select highlight moments",
    "analyse storyline", "structure this video into chapters",
    "find the most impactful moments", "create a summary of key points",
    "identify transitions", "extract quotes", "find specific moments",
    "analyze pacing", "identify themes", "scene breakdown",
    "moment detection", "highlight reel", "key frames"
}
```

---

## Legacy Job Types & Migration

**Location:** `app/llm/schemas.py` (RoutingConfig.LEGACY_TO_PRIMARY)

All legacy job types map to one of the 5 primary types for backward compatibility:

### GPT_ONLY_JOBS → TEXT_ADMIN

| Legacy Job Type | Description |
|-----------------|-------------|
| `casual_chat` | General conversation |
| `quick_question` | Simple factual queries |
| `summary` | Summarization tasks |
| `explanation` | Explaining concepts |
| `note_cleanup` | Organizing notes |
| `copywriting` | Marketing/copy text |
| `prompt_shaping` | Prompt engineering |
| `summarization` | Document summarization |
| `rewriting` | Text rewriting |
| `documentation` | Writing docs |
| `research` | Research tasks |

### CLAUDE_PRIMARY_JOBS → BIG_ARCHITECTURE

| Legacy Job Type | Description |
|-----------------|-------------|
| `complex_code_change` | Major code changes |
| `codegen_full_file` | Generate complete files |
| `architecture_design` | System architecture |
| `code_review` | Review code for issues |
| `spec_review` | Review specifications |
| `refactor` | Code refactoring |
| `implementation_plan` | Planning implementation |

### MEDIUM_DEV_JOBS → SMALL_CODE

| Legacy Job Type | Description |
|-----------------|-------------|
| `simple_code_change` | Small code changes |
| `small_bugfix` | Minor bug fixes |

### HIGH_STAKES_JOBS → BIG_ARCHITECTURE

| Legacy Job Type | Description |
|-----------------|-------------|
| `high_stakes_infra` | Critical infrastructure |
| `security_sensitive_change` | Security-related code |
| `privacy_sensitive_change` | Privacy-related code |
| `public_app_packaging` | Release packaging |

### GEMINI_JOBS → SIMPLE_VISION or HEAVY_MULTIMODAL_CRITIQUE

| Legacy Job Type | Maps To | Description |
|-----------------|---------|-------------|
| `image_analysis` | SIMPLE_VISION | Analyze single image |
| `screenshot_analysis` | SIMPLE_VISION | Analyze screenshot |
| `video_analysis` | HEAVY_MULTIMODAL_CRITIQUE | Analyze video |
| `vision` | SIMPLE_VISION | General vision |
| `ui_analysis` | SIMPLE_VISION | UI screenshot analysis |
| `document_analysis` | SIMPLE_VISION | Document analysis |
| `ocr` | SIMPLE_VISION | Text extraction |
| `web_search` | (special) | Web search integration |
| `critique` | HEAVY_MULTIMODAL_CRITIQUE | Deep critique/review |
| `analysis` | HEAVY_MULTIMODAL_CRITIQUE | Complex analysis |
| `cv_parsing` | SIMPLE_VISION | Resume parsing |

---

## Job Classifier

**Location:** `app/llm/job_classifier.py` (NEW in v0.12.7, enhanced v0.12.8, v0.12.17)

The job classifier analyzes message content and attachments to determine the appropriate job type and routing.

### Key Functions

| Function | Purpose |
|----------|---------|
| `classify_job(message, attachments, requested_type)` | Main classification entry point |
| `classify_message(message, requested_type)` | Simplified interface for main.py |
| `get_routing_for_job_type(job_type)` | Returns (Provider, model) tuple |
| `analyze_attachments(attachments)` | Counts images, videos, docs, code files |
| `analyze_message_text(message)` | Scores message against keyword lists |
| `detect_user_override(message)` | Detects "force Opus", "use GPT", etc. |
| `is_claude_allowed(job_type)` | Check if job type can route to Claude (v0.12.8) |
| `needs_deep_video_analysis(message)` | Detect semantic deep-analysis keywords (v0.12.8) |

### Classification Flow

```text
1. Check for user override ("force Opus", "use GPT")
2. Analyze attachments:
   - Video present? → Check size AND semantic keywords (v0.12.8)
     - >10MB OR needs_deep_analysis → HEAVY_MULTIMODAL_CRITIQUE (gemini-3.0-pro-preview)
     - else → SIMPLE_VISION (gemini-2.0-flash)
   - Image present? → SIMPLE_VISION or HEAVY_MULTIMODAL_CRITIQUE (if complex)
   - PDF present? → SIMPLE_VISION (for image-heavy) or TEXT_ADMIN (text-only)
3. Analyze message text against keyword lists
4. Score each job type, pick highest
5. Apply hard rules (ambiguous code → BIG_ARCHITECTURE)
6. Return RoutingDecision(job_type, provider, model, reason)
```

### CRITICAL (v0.12.17): Clean Message Handling

**Classifier receives clean user message only.** Attachment metadata passed separately via `attachments` parameter. Filenames are **NOT** injected into message text to prevent false keyword matches.

**Before (BROKEN in v0.12.8):**
```python
# main.py lines 678-683
full_message = f"{user_message}\n\n[User uploaded:]\n[Uploaded: architecture_map.md]..."
classify_message(full_message, None)  # Classifier sees filename → False positive
```

**After (FIXED in v0.12.17):**
```python
# main.py
full_message = user_message  # Clean message only
attachment_metadata = [{"filename": "architecture_map.md", ...}]  # Separate
classify_message(full_message, attachment_metadata)  # Correct classification
```

**Impact:** Files with keywords in names (e.g., "architecture_map.md") no longer cause false positive routing. "Simple Summary" with architecture file now correctly routes to GPT, not Opus.

### Debug Logging (v0.12.17)

When `ORB_ROUTER_DEBUG=1` in `.env`, job_classifier logs:

- User message (first 200 chars for privacy)
- Attachment metadata (filename, MIME type, size, file type flags)
- Section-by-section classification checks
- Keyword matches with matched terms
- Final routing decision

**Example output:**
```
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] ======================================================================
[router-debug] Message (first 200 chars): 'Simple Summary'
[router-debug] Message length: 14 chars
[router-debug] Total attachments: 1
[router-debug]   Attachment 1:
[router-debug]     filename: orb_architecture_map_v22.md
[router-debug]     mime_type: application/octet-stream
[router-debug]     size_bytes: 46932
[router-debug]     is_code: False
[router-debug] Section 6: DOCUMENT FILES - Found 1 document(s)
[router-debug]   Architecture check: False (matched: [])
[router-debug]   Simple doc check: True (matched: ['summari'])
[router-debug]   → Returning CHAT_LIGHT
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: chat_light
[router-debug]   Provider: openai
[router-debug]   Model: gpt-4.1-mini
[router-debug]   Reason: Simple document summary
[router-debug] ======================================================================
```

### Video Deep-Analysis Override (v0.12.8)

Small videos (<10MB) requesting deep analysis now route to the heavy model:

```python
# In job_classifier.py
VIDEO_DEEP_ANALYSIS_KEYWORDS = {
    "find best shots", "extract narrative", "segment scenes",
    "identify key scenes", "select highlight moments",
    "analyse storyline", "structure this video into chapters",
    # ... 18 total keywords
}

def needs_deep_video_analysis(message: str) -> bool:
    """Check if message requests deep video analysis."""
    message_lower = message.lower()
    return any(kw in message_lower for kw in VIDEO_DEEP_ANALYSIS_KEYWORDS)

# Video classification logic:
if total_size > VIDEO_SIZE_THRESHOLD:
    → VIDEO_HEAVY (gemini-3.0-pro-preview)
elif needs_deep_video_analysis(message):  # NEW in v0.12.8
    → VIDEO_HEAVY (gemini-3.0-pro-preview)
else:
    → IMAGE_SIMPLE (gemini-2.0-flash)
```

### Attachment Safety Rule (v0.12.8)

Prevents silent fallback to Claude when attachments are present without explicit job_type:

```python
# In router.py - _check_attachment_safety()
CLAUDE_ALLOWED_WITH_ATTACHMENTS = {"code.medium", "orchestrator"}

def _check_attachment_safety(
    has_attachments: bool,
    job_type_specified: bool,
    provider_id: str,
    decision: RoutingDecision,
) -> tuple[str, str, str]:
    """
    Safety check: Attachments without job_type should not silently go to Claude.
    Returns: (provider_id, model_id, reason)
    """
    if has_attachments and not job_type_specified and provider_id == "anthropic":
        if not is_claude_allowed(decision.job_type):
            # Redirect to GPT instead
            return (
                "openai",
                "gpt-4.1-mini",
                "SAFETY: Attachments present without job_type, redirected from Claude to GPT"
            )
    return (provider_id, decision.model, decision.reason)
```

### Classification Function Signature

```python
def _classify_job_type(message: str, requested_type: str) -> JobType:
    """
    Classify message to determine appropriate job type.
    Priority: explicit request > message content analysis > default
    """
```

### Integration Points

- `main.py._classify_job_type()` calls `job_classifier.classify_message()`
- `router.py.classify_and_route()` calls `job_classifier.classify_job()`
- `router.py._check_attachment_safety()` enforces attachment safety rules
- `stream_router.py._classify_job_type()` uses same classification logic

---

## Router Debug Mode

**Location:** `app/llm/job_classifier.py`, `app/llm/router.py` (NEW in v0.12.17)

**Purpose:** Comprehensive visibility into routing decisions for troubleshooting false positives, misrouting, and classification issues.

### Enabling Debug Mode

Add to `.env`:
```env
ORB_ROUTER_DEBUG=1
```

Then restart the backend:
```powershell
# Stop backend (Ctrl+C in PowerShell)
cd D:\Orb
.venv\Scripts\python main.py
```

### What Gets Logged

**In job_classifier.py:**
- User message content (first 200 chars for privacy)
- Total attachment count
- Per-attachment metadata:
  - filename
  - MIME type
  - size_bytes
  - File type flags (is_code, is_image, is_video, is_pdf)
- Section-by-section classification:
  - Section name (e.g., "Section 6: DOCUMENT FILES")
  - Keyword match results with matched terms
  - Decision at each stage (e.g., "→ Returning ORCHESTRATOR")
- Final classification result:
  - Job Type
  - Provider
  - Model
  - Reason

**In router.py:**
- Task properties (job_type, attachments count, force_provider)
- Pre-classification vs re-classification paths
- Override triggers (e.g., "CHAT_LIGHT → GPT mini")
- Final routing decision

### Log Format

```text
[router-debug] ======================================================================
[router-debug] ROUTER START
[router-debug] ======================================================================
[router-debug] Task job_type: unknown
[router-debug] Task attachments: 1
[router-debug] Task force_provider: None
[router-debug] Priority 2: Using job classifier
[router-debug]   No valid pre-classification, calling classifier...
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] ======================================================================
[router-debug] Message (first 200 chars): "Design a revised high-level architecture..."
[router-debug] Message length: 171 chars
[router-debug] Requested job_type: unknown
[router-debug] Metadata: {}
[router-debug] Total attachments: 1
[router-debug]   Attachment 1:
[router-debug]     filename: orb_architecture_map_v22.md
[router-debug]     mime_type: application/octet-stream
[router-debug]     size_bytes: 46932
[router-debug]     is_code: False
[router-debug]     is_image: False
[router-debug]     is_video: False
[router-debug]     is_pdf: False
[router-debug] Section 6: DOCUMENT FILES - Found 1 document(s)
[router-debug]   Architecture check: True (matched: ['architecture design', 'high-level architecture'])
[router-debug]   → Returning ORCHESTRATOR (architecture design)
[router-debug] ======================================================================
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: orchestrator
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
[router-debug]   Reason: Document with architecture design request: 1 file(s)
[router-debug] ======================================================================
[router-debug]   Classifier returned:
[router-debug]     Job Type: orchestrator
[router-debug]     Provider: anthropic
[router-debug]     Model: claude-opus-4-5-20251101
[router-debug] [router] Final routing: orchestrator → anthropic/claude-opus-4-5-20251101
[router-debug] ======================================================================
[router-debug] ROUTING DECISION FINAL
[router-debug]   Job Type: orchestrator
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
[router-debug]   Reason: Document with architecture design request: 1 file(s)
[router-debug] ======================================================================
```

### Use Cases

1. **Diagnosing misrouting:** See exactly why a message routed to wrong model
2. **Verifying keyword detection:** Confirm which keywords triggered classification
3. **Testing new keywords:** Add keywords and verify they match correctly
4. **Attachment handling:** Verify filename metadata isn't causing false positives (v0.12.17 fix)
5. **Performance analysis:** Count classification attempts and timing

### Performance Impact

**Minimal.** Debug logging adds ~5-10ms per request. Only string formatting and console writes. No impact on production workloads.

### Disabling Debug Mode

Remove or set to 0 in `.env`:
```env
ORB_ROUTER_DEBUG=0
# or just delete the line
```

Restart backend. No code changes required.

---

## Phase 4: Job Engine & Artefacts

**Status:** OPTIONAL (requires `ORB_ENABLE_PHASE4=true`)

### Provider Registry (app/providers/registry.py)

Single source of truth for LLM calls in Phase 4:

- **API key encryption/decryption** — Keys stored encrypted at rest
- **Rate limiting** — Per-provider, per-minute limits
- **Retry logic** — Exponential backoff on failures
- **Bounded usage log** — `deque(maxlen=5000)` for memory safety
- **Usage tracking** — Job/session/project context for billing

### Key Schema Fixes in v0.12.1

```python
# JobBudget field names:
JobBudget(
    max_tokens=8000,
    max_cost_estimate=1.0,
    max_wall_time_seconds=60,
)

# JobEnvelope:
modalities_in=[Modality.TEXT]  # NOT 'modalities'
needs_tools=[]                 # Must be list[str], NOT False

# Use RoutingOptions for per-task options, not RoutingConfig
task = LLMTask(
    routing=RoutingOptions(),  # NOT RoutingConfig()
)
```

---

## Provider Registry

**Location:** `app/providers/registry.py` (Phase 4 only)

**Purpose:** Single source of truth for all LLM API calls in Phase 4 mode

### Capabilities

| Provider | Capabilities |
|----------|-------------|
| OpenAI | `chat`, `streaming`, `embeddings` |
| Anthropic | `chat`, `streaming`, `vision` (via prompt injection) |
| Gemini | `chat`, `streaming`, `vision`, `video`, `web_search` |

### Registry Functions

| Function | Purpose |
|----------|---------|
| `get_provider_capabilities(provider)` | List what a provider can do |
| `call_provider(provider, capability, params)` | Generic provider call |
| `supports_modality(provider, modality)` | Check if provider handles modality |

**Note:** Phase 4 only. Main chat flow uses `router.py` and `clients.py`.

---

## DateTime Context

**Location:** `main.py`, `stream_router.py`

All LLM calls include current datetime in system prompt:

```python
datetime_context = f"\n\nCurrent date and time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
system_prompt = DEFAULT_SYSTEM_PROMPT + datetime_context
```

**Purpose:** LLMs can reference current date/time in responses (e.g., "today", "this week")

---

## Streaming Responses

**Location:** `app/llm/streaming.py`, `stream_router.py`

### Architecture

```text
Frontend (SSE listener)
    ↓
/stream/chat endpoint
    ↓
stream_*_llm() in clients.py
    ↓
extract_thinking_and_answer() in streaming.py
    ↓
SSE chunks (thinking + answer)
    ↓
Frontend displays in reasoning panel + message
```

### SSE Event Types

| Event | Data | Purpose |
|-------|------|---------|
| `metadata` | `{type: "metadata", provider: "...", model: "..."}` | First event |
| `token` | `{type: "token", content: "..."}` | Incremental response |
| `reasoning` | `{type: "reasoning", content: "..."}` | Chain-of-thought (v0.12.4) |
| `done` | `{type: "done", provider: "...", model: "...", total_length: N}` | Complete |
| `error` | `{type: "error", error: "..."}` | Failed |

### Reasoning Tags (v0.12.4)

```xml
<THINKING>
Your reasoning process here...
</THINKING>
<ANSWER>
Your visible answer here...
</ANSWER>
```

The streaming system extracts content between these tags and sends:
- `<THINKING>` content → `reasoning` SSE events (displayed in reasoning panel)
- `<ANSWER>` content → `token` SSE events (displayed in chat)

### Streaming Routing

1. `stream_router.py` receives request
2. `_classify_job_type()` analyzes message (5-type system)
3. `_select_provider_for_job_type()` determines provider/model
4. `streaming.py` streams from selected provider with tag extraction
5. Provider/model sent in metadata event

---

## Reasoning Panel

**Location:** `orb-desktop/src/components/ReasoningPanel.tsx`

**Purpose:** Display LLM chain-of-thought during streaming

**Features:**

- Real-time updates as thinking streams
- Collapsible panel
- Markdown rendering for structured thinking
- Syntax highlighting for code in reasoning

**Trigger:** Appears when streaming response contains `<THINKING>` tags

---

## Markdown Rendering

**Location:** `orb-desktop/src/components/MessageList.tsx`

**Libraries:**

- `react-markdown` — Markdown parser
- `remark-gfm` — GitHub-flavored markdown (tables, strikethrough, task lists)
- `react-syntax-highlighter` — Code block highlighting

**Features:**

- Inline code: `backticks`
- Code blocks with language detection
- Tables, lists, headers
- Links (auto-linkified)
- Images (from markdown)

**Example:**

```markdown
Here's a Python function:

\`\`\`python
def greet(name):
    return f"Hello, {name}!"
\`\`\`

And a table:

| Column 1 | Column 2 |
|----------|----------|
| Value A  | Value B  |
```

---

## Web Search

**Location:** `app/llm/web_search_router.py`, `app/llm/clients.py`

**Provider:** Gemini (via Google AI SDK)

**Endpoint:** `POST /web_search`

**Request:**
```json
{
  "query": "Latest news on AI models",
  "project_id": 1
}
```

**Response:**
```json
{
  "provider": "gemini",
  "content": "Here's what I found...",
  "grounding_metadata": {
    "search_queries": ["AI models 2025"],
    "grounding_chunks": [...]
  }
}
```

**Features:**

- Real-time web grounding
- Automatic citation of sources
- Saved to message history
- Model badge shows "Gemini (Search)"

---

## Chat History

**Location:** `app/memory/router.py` (messages endpoints)

**Endpoints:**

- `GET /memory/messages?project_id=1&limit=50` — Retrieve history
- `POST /memory/messages` — Save new message

**Storage:** Messages table with columns:

- `id`, `project_id`, `role`, `content`, `provider`, `model`, `created_at`

**Frontend:** Loads last 50 messages on project switch

---

## File Ingestion Pipeline

**Location:** `main.py` (upload handling), `app/llm/file_analyzer.py` (extraction)

### Flow

```text
1. User uploads file via /chat_with_attachments
2. Save to data/files/{project_id}/{uuid}.{ext}
3. Detect MIME type + doc_type
4. If binary (video/audio/image) → Skip text extraction, route to vision
5. If text/document → Extract text:
   - .txt/.md/.py/.js/.etc → Read as UTF-8
   - .docx → python-docx
   - .pdf → PyMuPDF (fitz)
6. Generate summary via GPT (100-word)
7. Save file record to DB
8. Index file content for semantic search
9. Attach to LLM context (separate from message, v0.12.17)
10. Route based on file type + message
```

### Supported File Types

| Type | Extraction Method | Notes |
|------|-------------------|-------|
| `.docx` | python-docx | Full text extraction |
| `.pdf` | PyMuPDF (fitz) | Text + basic structure |
| `.txt`, `.md`, `.csv` | Direct read | UTF-8 encoding |
| Code files | Direct read | .py, .js, .ts, etc. |
| Images | Vision analysis | Returns description |
| Video | **Skip text extraction** | Routes to Gemini Video |
| Audio | **Skip text extraction** | Future: transcription |

### Binary File Handling (v0.12.7)

Binary files (video, audio, images) are detected and skipped from text extraction:

```python
# file_analyzer.py
def extract_text_content(filepath: str, mime_type: str = None) -> Optional[str]:
    if is_binary_file(filepath, mime_type):
        print(f"[file_analyzer] Skipping binary file (route to vision): {filepath}")
        return None
    # ... normal text extraction
```

### Binary Detection (v0.12.7)

```python
# In file_analyzer.py
BINARY_EXTENSIONS = {
    # Video
    ".mp4", ".avi", ".mov", ".mkv", ".webm", ".m4v", ".wmv", ".flv", ".mpeg", ".mpg",
    # Audio
    ".mp3", ".wav", ".flac", ".aac", ".ogg", ".wma", ".m4a",
    # Images
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".tiff", ".ico", ".svg", ".heic",
}

def is_binary_file(filepath: str, mime_type: str = None) -> bool:
    """Check if file is binary (should skip text extraction)."""
```

Files detected as binary return `None` from `extract_text_content()` and are routed to vision/video analysis instead.

### PDF Library Priority

```python
# file_analyzer.py tries libraries in order:
try:
    import fitz  # PyMuPDF - preferred
except ImportError:
    try:
        import pypdf  # fallback
    except ImportError:
        # Returns error: "No PDF library available"
```

---

## Image Analysis Pipeline

**Location:** `app/llm/gemini_vision.py`

### Functions

| Function | Purpose |
|----------|---------|
| `analyze_image(image_source, mime_type, user_prompt)` | Structured analysis (summary, tags, type) |
| `ask_about_image(image_source, user_question, mime_type, context)` | Q&A about image |
| `check_vision_available()` | Check if vision providers are configured |
| `get_vision_model_for_complexity(question)` | Select model tier based on question |

### Provider Fallback

1. Try Gemini Vision first
2. On failure (rate limit, quota, error), fall back to OpenAI GPT-4o
3. OpenAI does NOT support video (images only)

### Model Tiers

```python
def _get_model_name(tier: str = "default") -> str:
    if tier == "fast":
        return os.getenv("GEMINI_VISION_MODEL_FAST", "gemini-2.0-flash")
    elif tier == "complex":
        return os.getenv("GEMINI_VISION_MODEL_COMPLEX", "gemini-2.5-pro")
    else:
        return os.getenv("GEMINI_VISION_MODEL", "gemini-2.0-flash")
```

### Media Routing Without User Message (v0.12.8)

Images/videos now route to Gemini even without a user message:

```python
# In main.py
if has_media:  # Route ALL media to Gemini
    vision_prompt = user_message if user_message else "Describe this image/video in detail."
```

---

## Video Analysis Pipeline

**Location:** `app/llm/gemini_vision.py` (NEW in v0.12.7, enhanced v0.12.8)

### Video Detection

```python
# In file_analyzer.py
def is_video_mime_type(mime_type: str) -> bool:
    return mime_type.startswith("video/") if mime_type else False

VIDEO_EXTENSIONS = {".mp4", ".avi", ".mov", ".mkv", ".webm", ".m4v", ".wmv", ".flv"}
```

### Video Analysis Flow

```text
1. Upload Detection
   - main.py detects video via is_video_mime_type()
   - Video saved to disk (data/files/{project_id}/{uuid}.ext)
   - NO text extraction attempted (binary file)

2. Routing (Enhanced in v0.12.8)
   - Video attachments tracked separately from images
   - Prefer video over images when both present (more complex)
   - Model tier based on file size AND semantic analysis:
     - ≤10MB + normal query → gemini-2.0-flash (default tier)
     - ≤10MB + deep analysis keywords → gemini-3.0-pro-preview (heavy tier)
     - >10MB → gemini-3.0-pro-preview (heavy tier)

3. Analysis (analyze_video function)
   - Upload video to Gemini File API: genai.upload_file(path)
   - Wait for processing (poll state until not "PROCESSING")
   - Generate response: model.generate_content([video_file, prompt])
   - Cleanup: genai.delete_file(video_file.name)

4. Response
   - Returns: {answer, provider: "google", model: "gemini-2.x-..."}
   - Error handling for quota (429), processing failures
```

### Key Function: analyze_video()

```python
def analyze_video(
    video_path: Union[str, Path],
    user_question: Optional[str] = None,
    context: Optional[str] = None,
) -> dict:
    """
    Analyze a video file using Gemini Vision.
    
    Returns:
        dict with: answer, provider, model, error (optional)
    """
```

---

## File Analyzer Helper Functions

**Location:** `app/llm/file_analyzer.py` (Enhanced in v0.12.8)

### MIME Type Helpers

```python
def is_video_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is video."""
    return mime_type.startswith("video/") if mime_type else False

def is_audio_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is audio."""
    return mime_type.startswith("audio/") if mime_type else False

def is_image_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is image."""
    return mime_type.startswith("image/") if mime_type else False

def is_pdf_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is PDF."""
    return mime_type == "application/pdf" if mime_type else False
```

### Text Extraction Wrapper (v0.12.8)

```python
def extract_text_content(file_path: str, mime_type: Optional[str] = None) -> Optional[str]:
    """
    Extract text content from a file.
    Wrapper around extract_text() that returns just the text or None.
    """
    text, error = extract_text(file_path=file_path)
    if error:
        logger.warning(f"Text extraction error: {error}")
    return text if text else None
```

### Document Functions (v0.12.8 Signature Fixes)

```python
def detect_document_type(
    content_or_path: Optional[str] = None,  # Can be text OR path
    filename: Optional[str] = None,
    mime_type: Optional[str] = None,
) -> str:
    """Detect document type from content or filename."""

def generate_document_summary(
    raw_text: Optional[str],  # Pre-extracted text
    filename: str,
    doc_type: str,
    llm_call: Callable[[str], str],  # LLM callback
    max_length: int = 500,
) -> str:
    """Generate a summary of document content using LLM."""

def parse_cv_with_llm(
    raw_text: Optional[str],
    filename: str,
    llm_call: Callable[[str], str],
) -> dict:
    """Parse CV/resume content into structured data using LLM."""
```

---

## Semantic Search System

**Location:** `app/embeddings/`

### Architecture

```text
1. Content indexed (notes, messages, files)
2. OpenAI text-embedding-3-small generates 1536-dim vectors
3. Stored in embeddings table
4. Search: Query → Embedding → Cosine similarity
5. Return top-k matches
```

### Auto-Indexing (v0.15.0)

- **Notes:** Indexed on create/update
- **Messages:** Indexed after LLM response
- **Files:** Indexed after text extraction

**Chunking:** Content split into 500-token chunks for better granularity

### Search Endpoints

- `POST /search_notes` — Search notes by semantic similarity
- `POST /search_messages` — Search messages by semantic similarity

**Request:**
```json
{
  "query": "How do I set up authentication?",
  "project_id": 1,
  "top_k": 5
}
```

**Response:**
```json
{
  "results": [
    {
      "entity_type": "note",
      "entity_id": 42,
      "chunk_text": "Authentication uses password-based...",
      "similarity": 0.87
    }
  ]
}
```

---

## Error Taxonomy

**Location:** `app/providers/registry.py` (Phase 4)

### Error Types

| Error | Description | Recovery |
|-------|-------------|----------|
| `AUTHENTICATION_ERROR` | Invalid API key | Check `.env` file |
| `RATE_LIMIT_ERROR` | Too many requests | Wait and retry |
| `CONTEXT_LENGTH_ERROR` | Input too long | Truncate context |
| `INVALID_REQUEST_ERROR` | Malformed request | Fix request params |
| `TIMEOUT_ERROR` | API timeout | Retry with backoff |
| `PROVIDER_ERROR` | Provider-specific error | Check provider status |

---

## Quick Start

### Starting Orb

```powershell
cd D:\orb-desktop
npm run electron:dev
```

### Manual Backend (Development Only)

```powershell
# Get master key
cd D:\orb-desktop
node -e "require('keytar').getPassword('OrbMasterKey','default').then(k=>console.log(k))"

# Start backend
cd D:\Orb
$env:ORB_MASTER_KEY = "your-43-char-key"
.\.venv\Scripts\Activate.ps1
uvicorn main:app --host 127.0.0.1 --port 8000
```

### Installing Dependencies

```powershell
cd D:\Orb
.venv\Scripts\pip install pymupdf Pillow --break-system-packages
```

---

## Troubleshooting

### Router Misrouting / False Positives (v0.12.17)

**Enable debug mode to diagnose:**

1. Add to `.env`: `ORB_ROUTER_DEBUG=1`
2. Restart backend: `.venv\Scripts\python main.py`
3. Send test message
4. Check console for `[router-debug]` output
5. Verify:
   - Message text is clean (no filenames injected)
   - Keyword matches are accurate
   - Attachment metadata is separate
   - Final routing decision is correct

**Common issues:**

- **Filenames in message:** Fixed in v0.12.17 — classifier receives clean message
- **Keyword false positives:** Refine keyword lists in `job_classifier.py`
- **Attachment type misdetection:** Check MIME type detection in `file_analyzer.py`

### Video Analysis Not Working

1. Check Gemini API key is set:
   ```powershell
   type D:\Orb\.env | findstr "GOOGLE_API"
   ```

2. Check model name is valid:
   ```powershell
   type D:\Orb\.env | findstr "GEMINI_VISION_MODEL_COMPLEX"
   # Should be: gemini-2.5-pro (NOT gemini-1.5-pro or gemini-3-pro-preview)
   ```

3. Check logs for video detection:
   ```
   [chat_with_attachments] Detected VIDEO: filename.mp4 (19922944 bytes)
   [vision] Uploading video to Gemini File API...
   [vision] Waiting for video processing...
   [vision] Video ready, generating response...
   [vision] Initialized Gemini model: gemini-2.5-pro (tier=complex)
   ```

4. Check for quota errors (429):
   ```
   [vision] Video analysis failed: 429 Resource exhausted
   ```

### Model Badge Not Showing

1. Verify `model` column exists in messages table:
   ```powershell
   sqlite3 D:\Orb\data\orb_memory.db "PRAGMA table_info(messages);"
   ```

2. If missing, run migration:
   ```powershell
   python scripts/add_model_column.py
   ```

3. Check that `memory/schemas.py` `MessageCreate` has `model` field

### Routing Not Working

1. Check console for classification logs:
   ```
   [classify] job_classifier: big_architecture -> anthropic/claude-opus-4-5-20251101
   [chat] Job type: big_architecture
   [chat] Response from: anthropic / claude-opus-4-5-20251101
   ```

2. For streaming:
   ```
   [stream_router] Classified: SMALL_CODE
   [stream_chat] Job-type routing: small_code -> anthropic/claude-sonnet-4-5-20250929
   ```

3. Clear Python cache if code changes not taking effect:
   ```powershell
   Remove-Item -Recurse -Force D:\Orb\app\llm\__pycache__
   ```

### Claude Opus 404 Error

If you see `404 - model: claude-opus-4-5-20250514`, update your .env:

```env
# Wrong:
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20250514

# Correct:
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20251101
```

### PDF Extraction Error

If you see `No PDF library available`:

```powershell
cd D:\Orb
.venv\Scripts\pip install pymupdf
```

### Pillow/PIL Missing

If Gemini vision fails with PIL import error:

```powershell
cd D:\Orb
.venv\Scripts\pip install Pillow
```

### Master Key Issues

- Key must be exactly 43 characters
- Delete and regenerate: `cmdkey /delete:OrbMasterKey/default`
- Restart Electron

### Phase 4 Validation Errors

- Check `modalities_in` (not `modalities`)
- Check `needs_tools` is `list[str]` (not `False`)
- Check `JobBudget` field names

### Debug Mode Not Working (v0.12.17)

**Check `.env` file:**
```env
ORB_ROUTER_DEBUG=1  # Must be exactly this
```

**Restart backend:**
```powershell
# Stop backend (Ctrl+C)
cd D:\Orb
.venv\Scripts\python main.py
```

**Verify console output contains `[router-debug]` lines**

---

## Testing Routing

### Test Prompts

| Prompt | Expected Job Type | Expected Provider |
|--------|-------------------|-------------------|
| "What's a good recipe for pasta?" | TEXT_ADMIN | OpenAI |
| "Write a Python binary search function" | SMALL_CODE | Anthropic Sonnet |
| "Design a microservices architecture for e-commerce" | BIG_ARCHITECTURE | Anthropic Opus |
| "Review this code for bugs: def divide(a,b): return a/b" | BIG_ARCHITECTURE | Anthropic Opus |
| "Debug this small bug" | SMALL_CODE | Anthropic Sonnet |
| [Upload image] "What is this?" | SIMPLE_VISION | Gemini Flash |
| [Upload image] (no message) | SIMPLE_VISION | Gemini Flash (default prompt) |
| [Upload video >10MB] "Analyze this video" | HEAVY_MULTIMODAL_CRITIQUE | Gemini Pro |
| [Upload video <10MB] "What's in this video?" | SIMPLE_VISION | Gemini Flash |
| [Upload video <10MB] "Find the best shots" | HEAVY_MULTIMODAL_CRITIQUE | Gemini Pro (deep analysis) |
| [Upload doc] "Simple Summary" | TEXT_ADMIN | OpenAI (no filename keyword match, v0.12.17) |

### Console Output to Watch

**Without debug mode:**
```
[classify] job_classifier: big_architecture -> anthropic/claude-opus-4-5-20251101 (keyword: architecture)
[chat] Job type: big_architecture
[router] Classifier: big_architecture → anthropic / claude-opus-4-5-20251101 (keyword match)
```

**With debug mode (v0.12.17):**
```
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] Message (first 200 chars): 'Design a microservices...'
[router-debug] Total attachments: 0
[router-debug] Section 1: USER OVERRIDES
[router-debug]   No override detected
[router-debug] Section 8: CODE DETECTION
[router-debug]   Architecture check: True (matched: ['microservices', 'architecture'])
[router-debug]   → Returning BIG_ARCHITECTURE
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: big_architecture
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
```

---

## Future Capabilities

### Planned Features

- [ ] Multi-turn conversations with memory
- [ ] Tool integration (web search, code execution, file system)
- [ ] Advanced RAG with query rewriting
- [ ] Voice input/output
- [ ] Mobile app (React Native)
- [ ] Cloud sync for projects
- [ ] Team collaboration features
- [ ] API rate limiting per user
- [ ] Cost tracking and budgeting
- [ ] Custom model fine-tuning

### Phase 4 Completion

- [ ] Migrate `/chat` to `/jobs/execute`
- [ ] Enable artefact storage for all responses
- [ ] Tool integration via Phase 4 framework
- [ ] Provider registry as single source of truth

---

## Changelog

### v0.12.17 (07 December 2025) — Message Injection Bug Fix + Debug Mode

**CRITICAL BUG FIX:**
- Fixed filename metadata being injected into user message for classification
- **Root Cause:** `main.py` lines 678-683 concatenated filenames like `[Uploaded: architecture_map.md]` into message text
- **Impact:** Filenames containing keywords (e.g., "architect") caused false positive routing
- **Example:** "Simple Summary" + file "architecture_map.md" incorrectly routed to Opus instead of GPT mini
- **Fix (3-part surgical change in main.py):**
  - Line 425: Initialize `attachment_metadata: List[dict] = []`
  - Line 700: Populate metadata dict in file upload loop
  - Line 744: Remove filename injection, keep message clean
  - Lines 770-773: Move file info to context (for LLM) instead of message (for classifier)
  - Line 790: Pass `attachment_metadata` to LLMTask separately

**Before (BROKEN):**
```python
full_message = f"{user_message}\n\n[User uploaded:]\n[Uploaded: architecture_map.md]..."
classify_message(full_message, None)  # Classifier sees filename → False positive
```

**After (FIXED):**
```python
full_message = user_message  # Clean message only
attachment_metadata = [{"filename": "architecture_map.md", ...}]
classify_message(full_message, attachment_metadata)  # Classifier sees metadata separately
```

**NEW FEATURE: Router Debug Mode (v0.12.17)**
- Added `ORB_ROUTER_DEBUG=1` environment flag for comprehensive routing visibility
- Debug logging in `job_classifier.py` and `router.py`
- Logs: message text, attachment metadata, keyword matches, routing decisions
- Example output format with section-by-section classification breakdown
- Performance impact: ~5-10ms per request (minimal)
- Zero behavioral change when disabled

**Emergency Patch:**
- Added missing `attachment_metadata` initialization (line 425)

**Files Changed:**
- `main.py` — Lines 425, 700, 744, 770-773, 790 (message injection fix)
- `job_classifier.py` — Debug logging instrumentation
- `router.py` — Debug logging instrumentation
- `.env.example` — Added ORB_ROUTER_DEBUG documentation

### v0.12.16 (06 December 2025) — Classifier Heuristics Fix

**Bug Fixes:**
- Fixed legacy classification function interfering with new classifier
- Router now respects job_classifier.py decisions without re-classification
- Removed `_classify_job_type()` from router.py (duplicate logic)
- Fixed attachment metadata being ignored in classification

### v0.12.15 (06 December 2025) — Attachment Metadata Passing

**Features:**
- Router now passes attachment metadata to classifier
- Classifier receives structured attachment info (filename, MIME, size)
- Fixed attachment detection not triggering vision routing

### v0.12.14 (06 December 2025) — Pre-Classification Respect

**Bug Fixes:**
- Router now respects pre-classified job_type from main.py
- Removed fallback that was overriding correct classifications
- Fixed double classification in `/chat_with_attachments`

### v0.12.13 (06 December 2025) — Environment Variable Override Fix

**Bug Fixes:**
- Fixed `OPENAI_MINI_MODEL` env var overriding code defaults
- Router now uses code defaults unless env var explicitly set
- Fixed GPT mini not being used for CHAT_LIGHT jobs

### v0.12.12 (06 December 2025) — Policy-Based Routing

**Features:**
- Implemented policy-based routing with `routing_policy.json`
- Supports override rules for specific job types
- Backward compatible with hard-coded routing

### v0.12.11 (06 December 2025) — Legacy Classifier Removal

**Bug Fixes:**
- Removed legacy `_classify_job_type()` from main.py
- All classification now uses `job_classifier.py`
- Fixed conflicts between old and new classification logic

### v0.12.10 (06 December 2025) — Provider Override Logic

**Features:**
- Added provider override detection
- Fixed routing respecting pre-classification

### v0.12.9 (06 December 2025) — Dead Model Name Fix

**Bug Fixes:**
- Fixed dead model names in routing tables
- Updated `gpt-4.1-mini` references (was `gpt-4o-mini`)
- Updated `claude-opus-4-5-20251101` (was `20250514`)
- Fixed model names in `job_classifier.py` and `router.py`

**Files Changed:**
- `app/llm/job_classifier.py` — Model name constants
- `app/llm/router.py` — Routing table definitions
- `.env.example` — Model name documentation

### v0.12.8 (06 December 2025) — Routing Refinements

**Features:**
- Added video deep-analysis semantic detection for small videos (<10MB)
- Added `VIDEO_DEEP_ANALYSIS_KEYWORDS` (18 keywords) in job_classifier.py
- Added `needs_deep_video_analysis()` function for semantic keyword matching
- Added `is_claude_allowed()` function to check allowed job types for Claude
- Added `_check_attachment_safety()` in router.py to prevent silent Claude fallback
- Media uploads now work without user message (uses default prompt)

**Model Updates:**
- Fixed Claude Opus model name: `claude-opus-4-5-20251101` (was incorrectly 20250514)
- Added `GEMINI_VIDEO_DEEP_MODEL` env var (default: gemini-3.0-pro-preview)

**Bug Fixes:**
- Fixed media routing when no user message provided (now uses default prompt)
- Fixed `extract_text_content` wrapper function returning tuple instead of string
- Fixed `detect_document_type` function signature mismatch
- Fixed `generate_document_summary` and `parse_cv_with_llm` function signatures
- Added defensive `isinstance(raw_text, str)` checks to prevent AttributeError
- Added MIME helper functions: `is_video_mime_type()`, `is_audio_mime_type()`, `is_image_mime_type()`, `is_pdf_mime_type()`

**Hard Rules Added:**
- Attachments without job_type never silently fall back to Claude → Route to GPT
- Only `code.medium` and `orchestrator` job types allowed for Claude with attachments

**Dependencies:**
- Added PyMuPDF (fitz) for PDF text extraction
- Added Pillow for Gemini vision image processing

### v0.12.7 (06 December 2025) — Video Support + Job Classifier

**Features:**
- Added `analyze_video()` function for video analysis via Gemini File API
- Added `is_video_mime_type()` and `is_audio_mime_type()` helpers
- Added `is_binary_file()` detection to skip text extraction for media files
- Created `app/llm/job_classifier.py` module for automatic job classification
- Integrated job_classifier into router.py `classify_and_route()`
- Updated main.py `_classify_job_type()` to use job_classifier
- Video routing with tiered model selection (>10MB → complex model)
- Binary files (video/audio/images) skip text extraction, route to vision

**Model Updates:**
- Default complex model: `gemini-2.5-pro` (gemini-1.5-pro deprecated April 2025)
- Default fast model: `gemini-2.0-flash`
- Added env var: `GEMINI_VISION_MODEL_COMPLEX`

**Bug Fixes:**
- Fixed video files being read as text (produced 50000 chars of garbage)
- Fixed routing to wrong provider when video detected

**Architecture Doc Updates (v21):**
- Reconciled v19 and v20 architecture maps
- Restored Key Endpoints table
- Restored Multi-LLM Orchestration provider roles table
- Restored Provider Registry capabilities detail
- Added Legacy Job Types & Migration section
- Documented LEGACY_TO_PRIMARY mapping

### v0.12.4 (05 December 2025) — Streaming Reasoning + Image Routing

**Features:**
- Streaming now extracts THINKING/ANSWER tags for reasoning panel
- Separate reasoning capture and display logic
- Image routing fixes for tiered model selection

**Bug Fixes:**
- Fixed image analysis not routing to correct vision model
- Fixed reasoning panel not populating during stream

### v0.12.3 (05 December 2025) — Attachments Endpoint Fix

**Bug Fixes:**
- Fixed /chat_with_attachments returning empty response
- Fixed file record creation for uploaded attachments

### v0.12.2 (04 December 2025) — Unified Routing

**Features:**
- Added automatic job-type classification from message content
- Both `/chat` and `/stream/chat` now use same routing logic
- Code/architecture/review prompts automatically route to Anthropic
- Casual chat routes to OpenAI
- Added `_classify_job_type()` function to `main.py` and `stream_router.py`
- Added `_select_provider_for_job_type()` function to `stream_router.py`

### v0.12.1 (03 December 2025) — Model Badge Fix

**Bug Fixes:**
- Added `model` column to Message ORM model
- `/chat` endpoint now returns and saves `model` field
- `/chat_with_attachments` endpoint now returns and saves `model` field
- Created migration script `scripts/add_model_column.py`

### v0.17.0 (02 December 2025) — Security Level 4

- Master key stored in Windows Credential Manager
- Password used only for authentication
- Electron manages master key lifecycle

### v0.16.0 (01 December 2025)

- Reasoning panel for chain-of-thought
- Model badges
- Markdown rendering

### v0.15.0 (30 November 2025)

- Semantic search with embeddings
- Auto-indexing

---

## Testing Checklist

After updates, verify:

- [ ] `model` column exists in messages table
- [ ] `/chat` returns both `provider` and `model`
- [ ] Messages saved to DB include `model` column
- [ ] Model badge displays in UI
- [ ] Non-stream chat routes code prompts to Anthropic
- [ ] Non-stream chat routes casual prompts to OpenAI
- [ ] Streaming chat routes code prompts to Anthropic
- [ ] Streaming chat routes casual prompts to OpenAI
- [ ] Video upload routes to Gemini Vision
- [ ] Video >10MB uses gemini-2.5-pro (complex tier)
- [ ] Video <10MB uses gemini-2.0-flash (default tier)
- [ ] Video <10MB + deep analysis keywords uses gemini-3.0-pro-preview
- [ ] Image upload routes to Gemini Flash
- [ ] Image upload without message uses default prompt
- [ ] Binary files skip text extraction
- [ ] Console shows classification logs with job_classifier
- [ ] Attachments without job_type don't silently fall back to Claude
- [ ] Phase 4 endpoints work (if enabled)
- [ ] PDF extraction works (PyMuPDF installed)
- [ ] Claude Opus model name is correct (20251101)
- [ ] **Router debug mode works (ORB_ROUTER_DEBUG=1)** (v0.12.17)
- [ ] **Filenames not injected into message text** (v0.12.17)
- [ ] **Attachment metadata passed separately to classifier** (v0.12.17)
- [ ] **Debug output shows clean messages and separate attachment info** (v0.12.17)

---

*Document maintained as part of Orb development. Update with each architectural change.*
