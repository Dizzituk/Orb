# Orb Architecture Map

**Version:** 0.14.2 (Architecture Map v29)  
**Last Updated:** 10 December 2025  
**Purpose:** Canonical architecture reference for the Orb multi-LLM AI assistant system

---

## Table of Contents

1. [System Overview](#system-overview)  
2. [Component Architecture](#component-architecture)  
3. [Backend (`Orb`)](#backend-orb)  
4. [Desktop Client (`orb-desktop`)](#desktop-client-orb-desktop)  
5. [Data Layer](#data-layer)  
6. [Authentication System](#authentication-system)  
7. [Database Encryption](#database-encryption)  
8. [Multi-LLM Orchestration](#multi-llm-orchestration)  
9. [Multimodal Routing System](#multimodal-routing-system)  
10. [Legacy Job Types & Migration](#legacy-job-types--migration)  
11. [Job Classifier](#job-classifier)  
12. [Router Debug Mode](#router-debug-mode)  
13. [High-Stakes Critique Pipeline](#high-stakes-critique-pipeline)  
14. [Critical Pipeline Specification Modules](#critical-pipeline-specification-modules)  
15. [Phase 4: Job Engine & Artefacts](#phase-4-job-engine--artefacts)  
16. [Provider Registry](#provider-registry)  
17. [DateTime Context](#datetime-context)  
18. [Streaming Responses](#streaming-responses)  
19. [Reasoning Panel](#reasoning-panel)  
20. [Markdown Rendering](#markdown-rendering)  
21. [Web Search](#web-search)  
22. [Chat History](#chat-history)  
23. [File Ingestion Pipeline](#file-ingestion-pipeline)  
24. [Image Analysis Pipeline](#image-analysis-pipeline)  
25. [Video+Code Debug Pipeline](#videocode-debug-pipeline)  
26. [Video Analysis Pipeline](#video-analysis-pipeline)  
27. [Semantic Search System](#semantic-search-system)  
28. [Error Taxonomy](#error-taxonomy)  
29. [Quick Start](#quick-start)  
30. [Troubleshooting](#troubleshooting)  
31. [Testing Routing](#testing-routing)  
32. [Future Capabilities](#future-capabilities)  
33. [Changelog](#changelog)  
34. [Testing Checklist](#testing-checklist)

---

## System Overview

Orb is a personal AI assistant built as a multi-component system with specialized LLM roles:

- **GPT (OpenAI)** — Fast/lightweight reasoning, conversational interface, linguistics, embeddings  
- **Claude (Anthropic)** — Flagship engineer, complex code, architecture design  
- **Gemini (Google)** — Critic, reviewer, analyst, vision specialist, video analysis, web search  

### Core Vision

Different AI models specialize in distinct roles while sharing a unified memory layer, enabling:

- Persistent knowledge management across conversations  
- Task coordination and project organization  
- **Multi-type job classification system** with modality-aware routing (v0.14.1)
- Automatic job-type classification from message content and attachments
- Job-type-based automatic model routing  
- Policy-driven routing with JSON configuration
- **Router debug mode with comprehensive routing visibility** (v0.12.17)
- **Clean message classification without filename injection** (v0.12.17)
- **High-stakes critique pipeline: Opus → Gemini 3 Pro critique → Opus revision** (v0.13.2)
- **Critique pipeline integrated into streaming endpoints** (v0.13.4)
- **Document content injection for file uploads** (v0.13.5)
- **Anthropic system message format fix** (v0.13.5)
- **High-stakes job type normalization for orchestrator routing** (v0.13.6)
- **Environment-aware architecture critique with hard-coded constraints** (v0.13.7)
- **Separate critique prompts for architecture vs security reviews** (v0.13.7)
- **Fixed security classification over-triggering** (v0.13.7)
- **Strict regex-based override detection preventing false positives** (v0.13.8)
- **Security review classification with dedicated keywords** (v0.13.3)
- **Critique pipeline logging with print() visibility** (v0.13.2.2)
- **Video+Code Debug Pipeline: Gemini3 transcribe → Sonnet code** (v0.14.1)
- **Simplified multimodal routing: ALL videos → Gemini 3, ALL images → Gemini 2.5** (v0.14.1)
- **Code > Images priority: code+images routes to Sonnet, not Gemini** (v0.14.1)
- **High-stakes video pre-step: transcribe videos before Opus draft** (v0.14.1)
- **Critical Pipeline Specification modules: file classification, audit logging, fallbacks** (v0.14.2)
- **Stable file naming with [FILE_X] identifiers for multimodal prompts** (v0.14.2)
- **MIXED_FILE detection: PDFs/DOCX with embedded images → vision routing** (v0.14.2)
- **Fallback chains for graceful degradation on model failures** (v0.14.2)
- **Audit trail logging for high-stakes pipeline transparency** (v0.14.2)
- File upload with text extraction and document analysis  
- CV parsing with structured data extraction  
- Image analysis via Gemini Vision (with OpenAI fallback)
- **Video analysis via Gemini File API** (v0.12.7)
- **Video deep-analysis semantic detection** (v0.12.8)
- **Attachment safety rules preventing silent Claude fallback** (v0.12.8)
- Binary file detection (skip text extraction for media files)
- Semantic search (RAG) with vector embeddings
- Auto-indexing of notes, messages, and files into embeddings
- Password-based authentication with session tokens
- Project selector for multi-project management
- Database encryption at rest (Security Level 4: Master Key)
- Streaming LLM responses with routing and reasoning capture
- Web search with real-time grounding
- Automatic datetime context in all LLM calls
- Chat history persistence and loading
- Markdown rendering for assistant messages with syntax highlighting
- Reasoning/thinking panel showing LLM chain-of-thought
- Model badges identifying which LLM answered each message
- Phase 4: Structured job system with envelopes, results, and artefacts (optional)

### Technology Stack

**Backend:**

- FastAPI (Python 3.13)  
- SQLAlchemy ORM  
- SQLite database  
- Pydantic schemas  
- bcrypt for password hashing  
- cryptography (Fernet) for field-level encryption  
- `python-multipart` for `multipart/form-data` file uploads  
- `python-docx` for DOCX text extraction  
- `PyMuPDF` (fitz) for PDF text extraction  
- `Pillow` for image processing (Gemini vision)
- `google-genai` for Gemini Web Search (new SDK, preferred)  
- `google-generativeai` for Gemini Vision + Video + fallback (old SDK)  
- OpenAI `text-embedding-3-small` for semantic embeddings

**Desktop Client:**

- Electron (v33.0.0)  
- React 18 with TypeScript 5  
- Vite 5 (build tool and dev server)  
- Custom React hooks for state management  
- `keytar` for Windows Credential Manager integration
- `react-markdown` for markdown rendering
- `remark-gfm` for GitHub-flavored markdown
- `react-syntax-highlighter` for code block highlighting

**Platform:** Windows 11

---

## Component Architecture

```text
D:/
├── Orb/                        # Backend (FastAPI server)
│   ├── main.py                 # Application entrypoint, chat endpoints, RAG, classification (v0.14.2)
│   ├── .env                    # API keys + model configuration + debug flags (v0.14.2)
│   ├── app/
│   │   ├── db.py               # Database configuration
│   │   ├── auth/               # Authentication module (password-based)
│   │   ├── crypto/             # Encryption module (master key)
│   │   ├── memory/             # Memory subsystem (projects/notes/tasks/files/messages)
│   │   ├── llm/                # LLM routing + vision + video + streaming + web search
│   │   │   ├── __init__.py     # Module exports (v0.14.2: Critical Pipeline exports)
│   │   │   ├── schemas.py      # JobType, LLMTask, LLMResult, RoutingOptions, RoutingConfig
│   │   │   ├── clients.py      # Provider API wrappers (via registry)
│   │   │   ├── router.py       # Main router with critique pipeline, file map injection (v0.14.2)
│   │   │   ├── job_classifier.py  # Job classification with file_classifier integration (v0.14.2)
│   │   │   ├── stream_router.py   # Streaming endpoint with routing, critique pipeline
│   │   │   ├── streaming.py       # Stream generators, THINKING/ANSWER tag extraction
│   │   │   ├── policy.py          # JSON-based routing policy
│   │   │   ├── gemini_vision.py   # Image + Video analysis via Gemini
│   │   │   ├── file_analyzer.py   # Text extraction + binary detection + MIME helpers
│   │   │   ├── web_search_router.py # Web search endpoints
│   │   │   │
│   │   │   │   # === Critical Pipeline Specification Modules (v0.14.2) ===
│   │   │   ├── file_classifier.py     # File classification + stable [FILE_X] naming (Spec §1-§2)
│   │   │   ├── audit_logger.py        # Audit trail logging for pipeline transparency (Spec §12)
│   │   │   ├── relationship_detector.py # File relationship detection (Spec §3)
│   │   │   ├── preprocessor.py        # Task preprocessing + context building (Spec §5-§6)
│   │   │   ├── token_budgeting.py     # Token budget allocation + truncation (Spec §7)
│   │   │   ├── task_extractor.py      # Multi-task extraction from messages (Spec §4)
│   │   │   └── fallbacks.py           # Fallback chains + graceful degradation (Spec §11)
│   │   ├── embeddings/         # Semantic search with vector embeddings + auto-indexing
│   │   │   ├── models.py       # Embedding ORM model
│   │   │   ├── service.py      # Indexing and search logic
│   │   │   └── router.py       # Embeddings API endpoints
│   │   ├── jobs/               # Phase 4: Job engine (OPTIONAL)
│   │   │   ├── schemas.py      # JobEnvelope, JobResult, etc.
│   │   │   ├── engine.py       # Job orchestration
│   │   │   └── router.py       # Jobs API endpoints
│   │   ├── artefacts/          # Phase 4: Artefact storage (OPTIONAL)
│   │   │   ├── schemas.py      # Artefact schemas
│   │   │   ├── service.py      # Artefact CRUD
│   │   │   └── router.py       # Artefacts API endpoints
│   │   ├── providers/          # Phase 4: Provider registry (OPTIONAL)
│   │   │   └── registry.py     # Single source of truth for LLM calls
│   │   └── tools/              # Tool integrations
│   ├── data/                   # SQLite DB + file storage + auth config
│   │   ├── orb_memory.db       # SQLite database
│   │   ├── auth.json           # Password hash + session storage
│   │   ├── encryption_salt.bin # Legacy key derivation (migration only)
│   │   ├── routing_policy.json # LLM routing policy configuration
│   │   └── files/              # Per-project file storage
│   └── scripts/                # Migration and utility scripts
│       └── add_model_column.py # Migration script for model column
│
├── orb-desktop/                # Electron + React desktop client
│   ├── main.js                 # Electron main process
│   ├── src/
│   │   ├── App.tsx             # Root component
│   │   ├── components/         # React components
│   │   ├── hooks/              # Custom React hooks
│   │   ├── services/           # API client layer
│   │   │   └── api.ts          # HTTP client
│   │   ├── types/              # TypeScript interfaces
│   │   │   └── index.ts        # Message, ChatResponse, etc.
│   │   └── styles/             # CSS styles
│   └── ...
│
└── Windows Credential Manager
    └── OrbMasterKey/default    # 32-byte master key (URL-safe base64)
```

### Data Flow (Three Paths with Unified Routing)

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 1: Non-Streaming Chat (/chat, /chat_with_attachments)                  │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /chat                                                             │
│ main.py /chat endpoint                                                       │
│    ↓ _classify_job_type() → job_classifier.classify_message()               │
│    ↓ builds LLMTask with classified job_type                                │
│ app/llm/router.py call_llm() → call_llm_async()                             │
│    ↓ classify_and_route() — multimodal classification (v0.14.1)      │
│    ↓ _check_attachment_safety() — prevent silent Claude fallback (v0.12.8) │
│    ↓ [router-debug] logs (if ORB_ROUTER_DEBUG=1, v0.12.17)                  │
│    ↓ synthesizes JobEnvelope                                                │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs (OpenAI / Anthropic / Google)                                 │
│    ↓                                                                        │
│ Returns: ChatResponse { provider, model, reply, ... }                       │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 1b: Media Analysis (/chat_with_attachments with images/video)          │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /chat_with_attachments (multipart/form-data)                      │
│ main.py                                                                      │
│    ↓ Clean message handling (v0.12.17): NO filename injection               │
│    ↓ Attachment metadata passed separately to classifier                    │
│    ↓ is_image_mime_type() / is_video_mime_type() detection                  │
│    ↓ check_vision_available()                                               │
│    ↓ Video: analyze_video() → Gemini File API                               │
│    ↓ Image: ask_about_image() → Gemini Vision                               │
│    ↓ Media without message → uses default prompt (v0.12.8)                  │
│ app/llm/gemini_vision.py                                                    │
│    ↓ Model tier selection (file size + semantic analysis)                   │
│ Gemini Vision API                                                            │
│    ↓                                                                        │
│ Returns: ChatResponse { provider: "google", model: "gemini-2.x-...", ... }  │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 2: Streaming Chat (/stream/chat)                                       │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /stream/chat                                                      │
│ app/llm/stream_router.py                                                    │
│    ↓ _classify_job_type() — multimodal classification                 │
│    ↓ _select_provider_for_job_type() — route based on job type              │
│ app/llm/streaming.py stream_llm(provider, model)                            │
│    ↓ streaming to selected provider                                         │
│    ↓ THINKING/ANSWER tag extraction for reasoning                           │
│ Provider APIs (streaming)                                                    │
│    ↓                                                                        │
│ SSE Events: metadata → tokens → reasoning → done                            │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 3: Phase 4 Jobs (/jobs) — OPTIONAL                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /jobs/create                                                      │
│ app/jobs/router.py                                                          │
│    ↓ creates JobEnvelope                                                    │
│ app/jobs/engine.py                                                          │
│    ↓ orchestrates job                                                       │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## Backend (`Orb`)

**Location:** `D:/Orb/`  
**Framework:** FastAPI  
**Entry Point:** `main.py`  
**Version:** 0.14.2

### Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/ping` | GET | Health check |
| `/auth/setup` | POST | Set password |
| `/auth/validate` | POST | Login |
| `/chat` | POST | Non-streaming chat (with routing) |
| `/stream/chat` | POST | Streaming chat (with routing) |
| `/chat_with_attachments` | POST | Upload files + chat (images/video route to Gemini) |
| `/llm` | POST | Direct LLM call |
| `/memory/projects` | GET/POST | Project management |
| `/memory/messages` | GET | Message history |
| `/embeddings/search` | POST | Semantic search |
| `/search/web` | POST | Web search |
| `/jobs/*` | * | Phase 4 (optional) |
| `/artefacts/*` | * | Phase 4 (optional) |

### Environment Variables

```env
# API Keys
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AI...

# Model Configuration (multimodal routing system)
OPENAI_DEFAULT_MODEL=gpt-4.1-mini
ANTHROPIC_SONNET_MODEL=claude-sonnet-4-5-20250929
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20251101

# Gemini Vision Models
GEMINI_VISION_MODEL_FAST=gemini-2.0-flash
GEMINI_VISION_MODEL=gemini-2.0-flash
GEMINI_VISION_MODEL_COMPLEX=gemini-2.5-pro
GEMINI_VIDEO_DEEP_MODEL=gemini-3.0-pro-preview

# Debug Mode (v0.12.17)
ORB_ROUTER_DEBUG=1  # Enable comprehensive routing logs

# Critical Pipeline Configuration (v0.14.2)
ORB_AUDIT_ENABLED=1           # Enable audit trail logging (default: 1)
ORB_FALLBACK_ENABLED=1        # Enable fallback behavior (default: 1)
ORB_MAX_FALLBACK_ATTEMPTS=3   # Max attempts before giving up (default: 3)

# Optional
ORB_ENABLE_PHASE4=false
```

---

## Desktop Client (`orb-desktop`)

**Location:** `D:/orb-desktop/`  
**Framework:** Electron + React + TypeScript  
**Version:** (matches backend)

### Key Components

| Component | Purpose |
|-----------|---------|
| `App.tsx` | Root component with routing |
| `ChatInterface.tsx` | Main chat UI with message display |
| `MessageList.tsx` | Message rendering with markdown |
| `ProjectSelector.tsx` | Switch between projects |
| `FileUpload.tsx` | Drag-and-drop file handling |
| `ReasoningPanel.tsx` | Display LLM thinking process |
| `api.ts` | HTTP client for backend communication |

### UI Features

- Real-time markdown rendering with syntax highlighting
- Model badges (GPT/Claude/Gemini) on each message
- Collapsible reasoning panel for chain-of-thought
- File drag-and-drop with preview
- Project-scoped chat history
- Responsive layout

---

## Data Layer

**Database:** SQLite (`data/orb_memory.db`)

### Schema

| Table | Columns | Purpose |
|-------|---------|---------|
| `projects` | id, name, description, created_at, updated_at | Project management |
| `notes` | id, project_id, title, content, tags, created_at, updated_at | Note storage |
| `tasks` | id, project_id, title, description, status, priority, due_date, created_at, updated_at | Task tracking |
| `files` | id, project_id, filename, filepath, size_bytes, mime_type, doc_type, summary, created_at | File metadata |
| `messages` | id, project_id, role, content, provider, model, created_at | Chat history |
| `embeddings` | id, entity_type, entity_id, chunk_text, embedding, created_at | Vector embeddings for semantic search |

**Note:** `model` column added in v0.12.1 for model badge display

---

## Authentication System

**Location:** `app/auth/`

**Mode:** Password-based authentication with session tokens

**Flow:**

1. User enters password in Electron UI
2. Frontend sends password to `/auth/login`
3. Backend verifies bcrypt hash from `data/auth.json`
4. On success, generates session token (UUID)
5. Token stored in-memory on backend, returned to frontend
6. Frontend includes token in `Authorization` header for all requests

**Session Management:**

- Sessions stored in `data/auth.json`
- No expiration (persistent across restarts)
- `/auth/check` endpoint validates token

---

## Database Encryption

**Location:** `app/crypto/`

**Security Level:** 4 (Master Key)

### Architecture

```text
┌─────────────────────────────────────────────────────────┐
│ Windows Credential Manager (OrbMasterKey/default)       │
│   ↓                                                      │
│ 32-byte master key (URL-safe base64, 43 chars)         │
│   ↓                                                      │
│ Fernet encryption (field-level)                         │
│   ↓                                                      │
│ Encrypted fields: notes.content, tasks.description      │
└─────────────────────────────────────────────────────────┘
```

**Key Points:**

- Master key generated once by Electron, stored in Windows Credential Manager
- Password used ONLY for authentication, NOT for encryption
- Backend retrieves master key from Credential Manager on startup
- Fernet symmetric encryption for field-level protection

**Migration:** Security Level 3 (password-derived key) → Level 4 (master key) completed

---

## Multi-LLM Orchestration

### Provider Roles

| Provider | Primary Use Cases |
|----------|-------------------|
| **OpenAI (GPT)** | Chat, embeddings, fast responses, casual questions, text/admin tasks |
| **Anthropic (Claude)** | Complex code, architecture, engineering, code review |
| **Gemini (Google)** | Vision, video analysis, web search, review/critique |

### Default Models (Environment Variables)

| Provider | Environment Variable | Default |
|----------|---------------------|---------|
| OpenAI (light) | `OPENAI_DEFAULT_MODEL` | `gpt-4.1-mini` |
| Anthropic (code) | `ANTHROPIC_SONNET_MODEL` | `claude-sonnet-4-5-20250929` |
| Anthropic (arch) | `ANTHROPIC_OPUS_MODEL` | `claude-opus-4-5-20251101` |
| Gemini (fast) | `GEMINI_VISION_MODEL_FAST` | `gemini-2.0-flash` |
| Gemini (default) | `GEMINI_VISION_MODEL` | `gemini-2.0-flash` |
| Gemini (complex) | `GEMINI_VISION_MODEL_COMPLEX` | `gemini-2.5-pro` |
| Gemini (video deep) | `GEMINI_VIDEO_DEEP_MODEL` | `gemini-3.0-pro-preview` |

---

## Multimodal Routing System

**Location:** `app/llm/schemas.py` (RoutingConfig class), `app/llm/job_classifier.py`

**Version:** v0.14.1 — Unified multimodal routing with modality priority

The routing system classifies requests based on content AND attachments, with clear priority rules:

### Modality Priority Order (v0.14.1)

```
1. Video+Code → VIDEO_CODE_DEBUG (Gemini3 transcribe → Sonnet code)
2. Video only → VIDEO_HEAVY (Gemini 3.0 Pro)
3. Code (with or without images) → CODE_MEDIUM/ORCHESTRATOR (Sonnet/Opus)
4. Images only → IMAGE_COMPLEX (Gemini 2.5 Pro)
5. Text/docs → TEXT_HEAVY/CHAT_LIGHT (GPT)
```

### Primary Job Types

| Job Type | Provider | Model | Use Cases |
|----------|----------|-------|-----------|
| `CHAT_LIGHT` | OpenAI | `gpt-4.1-mini` | Casual chat, quick questions |
| `TEXT_HEAVY` | OpenAI | `gpt-4.1` | Heavy text work, text-only PDFs |
| `CODE_MEDIUM` | Anthropic | `claude-sonnet-4-5-20250929` | 1-3 files, scoped code, bug fixes |
| `ORCHESTRATOR` | Anthropic | `claude-opus-4-5-20251101` | Multi-file, architecture, system design |
| `IMAGE_COMPLEX` | Google | `gemini-2.5-pro` | ALL images (v0.14.1: no more Flash) |
| `VIDEO_HEAVY` | Google | `gemini-3.0-pro-preview` | ALL videos (v0.14.1: no more Flash) |
| `VIDEO_CODE_DEBUG` | Pipeline | Gemini3 → Sonnet | Video + code debugging (v0.14.1) |
| `IMAGE_SIMPLE` | Google | `gemini-2.0-flash` | LEGACY ONLY - manual override |
| `OPUS_CRITIC` | Google | `gemini-3.0-pro-preview` | Explicit Opus review only |

### Hard Rules (Enforced)

1. **Video + Code → VIDEO_CODE_DEBUG pipeline** (Gemini3 transcribe → Sonnet code)
2. **ALL videos → VIDEO_HEAVY** (Gemini 3.0 Pro) — no more Flash for videos
3. **Code + images (no video) → CODE_MEDIUM/ORCHESTRATOR** — code wins over images
4. **ALL images (no code) → IMAGE_COMPLEX** (Gemini 2.5 Pro) — no more Flash for images
5. **Images/video/PDFs NEVER go to Claude** (enforced in classifier)
6. **PDFs without images → TEXT_HEAVY** (GPT)
7. **Attachments without job_type never silently fall back to Claude** → Route to GPT (v0.12.8)
8. **Filename metadata never injected into user message for classification** (v0.12.17)

### Keyword Detection

```python
TEXT_ADMIN_KEYWORDS = {
    "email", "letter", "report", "blog", "readme", "documentation",
    "plan", "brainstorm", "summary", "summarize", "explain",
    "note", "copywriting", "prompt", "rewriting", "research",
    "spreadsheet", "planning", "casual"
}

SMALL_CODE_KEYWORDS = {
    "fix this bug", "small fix", "quick fix", "add a function",
    "helper function", "tweak", "minor change", "single file",
    "debug", "small bugfix", "implement function"
}

BIG_ARCHITECTURE_KEYWORDS = {
    "architect", "architecture", "multi-file", "refactor",
    "routing", "memory", "database", "schema", "security",
    "system design", "migration", "phase", "v0.",
    "microservice", "infrastructure", "scalab", "production-ready",
    "complete implementation", "from scratch"
}

SIMPLE_VISION_KEYWORDS = {
    "screenshot", "what is this", "ocr", "read this",
    "small image", "describe this", "single image"
}

HEAVY_MULTIMODAL_CRITIQUE_KEYWORDS = {
    "video", "youtube", "reel", "lecture", "complex pdf",
    "critique", "review this design", "deep analysis",
    "multiple images", "mixed media"
}

# NEW in v0.12.8: Video Deep Analysis Detection
VIDEO_DEEP_ANALYSIS_KEYWORDS = {
    "find best shots", "extract narrative", "segment scenes",
    "identify key scenes", "select highlight moments",
    "analyse storyline", "structure this video into chapters",
    "find the most impactful moments", "create a summary of key points",
    "identify transitions", "extract quotes", "find specific moments",
    "analyze pacing", "identify themes", "scene breakdown",
    "moment detection", "highlight reel", "key frames"
}
```

---

## Legacy Job Types & Migration

**Location:** `app/llm/schemas.py` (RoutingConfig.LEGACY_TO_PRIMARY)

All legacy job types map to one of the 5 primary types for backward compatibility:

### GPT_ONLY_JOBS → TEXT_ADMIN

| Legacy Job Type | Description |
|-----------------|-------------|
| `casual_chat` | General conversation |
| `quick_question` | Simple factual queries |
| `summary` | Summarization tasks |
| `explanation` | Explaining concepts |
| `note_cleanup` | Organizing notes |
| `copywriting` | Marketing/copy text |
| `prompt_shaping` | Prompt engineering |
| `summarization` | Document summarization |
| `rewriting` | Text rewriting |
| `documentation` | Writing docs |
| `research` | Research tasks |

### CLAUDE_PRIMARY_JOBS → BIG_ARCHITECTURE

| Legacy Job Type | Description |
|-----------------|-------------|
| `complex_code_change` | Major code changes |
| `codegen_full_file` | Generate complete files |
| `architecture_design` | System architecture |
| `code_review` | Review code for issues |
| `spec_review` | Review specifications |
| `refactor` | Code refactoring |
| `implementation_plan` | Planning implementation |

### MEDIUM_DEV_JOBS → SMALL_CODE

| Legacy Job Type | Description |
|-----------------|-------------|
| `simple_code_change` | Small code changes |
| `small_bugfix` | Minor bug fixes |

### HIGH_STAKES_JOBS → BIG_ARCHITECTURE

| Legacy Job Type | Description |
|-----------------|-------------|
| `high_stakes_infra` | Critical infrastructure |
| `security_sensitive_change` | Security-related code |
| `privacy_sensitive_change` | Privacy-related code |
| `public_app_packaging` | Release packaging |

### GEMINI_JOBS → SIMPLE_VISION or HEAVY_MULTIMODAL_CRITIQUE

| Legacy Job Type | Maps To | Description |
|-----------------|---------|-------------|
| `image_analysis` | SIMPLE_VISION | Analyze single image |
| `screenshot_analysis` | SIMPLE_VISION | Analyze screenshot |
| `video_analysis` | HEAVY_MULTIMODAL_CRITIQUE | Analyze video |
| `vision` | SIMPLE_VISION | General vision |
| `ui_analysis` | SIMPLE_VISION | UI screenshot analysis |
| `document_analysis` | SIMPLE_VISION | Document analysis |
| `ocr` | SIMPLE_VISION | Text extraction |
| `web_search` | (special) | Web search integration |
| `critique` | HEAVY_MULTIMODAL_CRITIQUE | Deep critique/review |
| `analysis` | HEAVY_MULTIMODAL_CRITIQUE | Complex analysis |
| `cv_parsing` | SIMPLE_VISION | Resume parsing |

---

## Job Classifier

**Location:** `app/llm/job_classifier.py` (NEW in v0.12.7, enhanced v0.12.8, v0.12.17)

The job classifier analyzes message content and attachments to determine the appropriate job type and routing.

### Key Functions

| Function | Purpose |
|----------|---------|
| `classify_job(message, attachments, requested_type)` | Main classification entry point |
| `classify_message(message, requested_type)` | Simplified interface for main.py |
| `get_routing_for_job_type(job_type)` | Returns (Provider, model) tuple |
| `analyze_attachments(attachments)` | Counts images, videos, docs, code files |
| `analyze_message_text(message)` | Scores message against keyword lists |
| `detect_user_override(message)` | Detects "force Opus", "use GPT", etc. (v0.13.8: strict regex) |
| `is_claude_allowed(job_type)` | Check if job type can route to Claude (v0.12.8) |
| `needs_deep_video_analysis(message)` | Detect semantic deep-analysis keywords (v0.12.8) |

### User Override Detection (v0.13.8 - Strict Regex)

**CRITICAL FIX:** Override detection was triggering on ANY mention of model names in text, causing false positives.

**Problem (v0.13.7):**
```python
# OLD: Substring matching triggered anywhere in text
if "use gemini" in message_lower:
    return override  # ❌ Matches "Multi-image uploads use Gemini..."
```

**Test Case That Failed:**
- Message: "...Multi-image uploads route to Gemini 2.5 Pro Vision... add overrides such as `OVERRIDE SEND TO GEMINI`..."
- Attachment: orb_architecture_map.md (text file)
- **Wrong Result:** Job Type: image.simple, Provider: google, Reason: "User requested Gemini explicitly"
- **Expected:** Job Type: orchestrator → architecture_design, Provider: anthropic

**Solution (v0.13.8):**
```python
# NEW: Strict regex requiring commands at line start
import re

override_patterns = [
    # OVERRIDE commands (must be at line start)
    (r'(?:^|\n)\s*OVERRIDE\s+(?:SEND\s+TO|USE)\s+OPUS\b', ...),
    (r'(?:^|\n)\s*OVERRIDE\s+(?:SEND\s+TO|USE)\s+GEMINI\b', ...),
    
    # FORCE/USE commands (must be at line start)
    (r'(?:^|\n)\s*(?:FORCE|USE)\s+OPUS\b', ...),
    (r'(?:^|\n)\s*(?:FORCE|USE)\s+GEMINI\b', ...),
]

for pattern, job_type, reason in override_patterns:
    if re.search(pattern, message, re.IGNORECASE):
        return override
```

**Pattern Components:**
- `(?:^|\n)` - Must be at start of message or after newline
- `\s*` - Optional whitespace
- `OVERRIDE\s+SEND\s+TO\s+<MODEL>` - Override command format
- `(?:FORCE|USE)\s+<MODEL>` - Force/Use command format
- `\b` - Word boundary (prevents partial matches)
- `re.IGNORECASE` - Case-insensitive

**Examples That SHOULD Trigger:**
- `"OVERRIDE SEND TO GEMINI"`
- `"Force Opus"`
- `"USE GPT"`
- `"\nOVERRIDE SEND TO GEMINI"` (after newline)

**Examples That Should NOT Trigger:**
- `"Multi-image uploads route to Gemini 2.5 Pro Vision"` (documentation)
- `"The system uses Gemini for video analysis"` (lowercase "uses")
- `"Add overrides like \`OVERRIDE SEND TO GEMINI\`"` (example in backticks)
- `"When you use gemini it works better"` (mid-sentence)

**Impact:**
- Architecture planning prompts with model mentions now route correctly
- Only explicit override commands trigger override detection
- Documentation and examples don't cause false positives

**Files Changed:**
- `app/llm/job_classifier.py` (v0.13.8)
  - Line 175: Updated to pass `message` (not `message_lower`) to `_detect_user_override()`
  - Lines 466-518: Replaced substring matching with strict regex patterns
  - Added debug logging for override detection results

### Classification Flow

```text
1. Check for user override ("force Opus", "use GPT")
2. Analyze attachments:
   - Video present? → Check size AND semantic keywords (v0.12.8)
     - >10MB OR needs_deep_analysis → HEAVY_MULTIMODAL_CRITIQUE (gemini-3.0-pro-preview)
     - else → SIMPLE_VISION (gemini-2.0-flash)
   - Image present? → SIMPLE_VISION or HEAVY_MULTIMODAL_CRITIQUE (if complex)
   - PDF present? → SIMPLE_VISION (for image-heavy) or TEXT_ADMIN (text-only)
3. Analyze message text against keyword lists
4. Score each job type, pick highest
5. Apply hard rules (ambiguous code → BIG_ARCHITECTURE)
6. Return RoutingDecision(job_type, provider, model, reason)
```

### CRITICAL (v0.12.17): Clean Message Handling

**Classifier receives clean user message only.** Attachment metadata passed separately via `attachments` parameter. Filenames are **NOT** injected into message text to prevent false keyword matches.

**Before (BROKEN in v0.12.8):**
```python
# main.py lines 678-683
full_message = f"{user_message}\n\n[User uploaded:]\n[Uploaded: architecture_map.md]..."
classify_message(full_message, None)  # Classifier sees filename → False positive
```

**After (FIXED in v0.12.17):**
```python
# main.py
full_message = user_message  # Clean message only
attachment_metadata = [{"filename": "architecture_map.md", ...}]  # Separate
classify_message(full_message, attachment_metadata)  # Correct classification
```

**Impact:** Files with keywords in names (e.g., "architecture_map.md") no longer cause false positive routing. "Simple Summary" with architecture file now correctly routes to GPT, not Opus.

### Debug Logging (v0.12.17)

When `ORB_ROUTER_DEBUG=1` in `.env`, job_classifier logs:

- User message (first 200 chars for privacy)
- Attachment metadata (filename, MIME type, size, file type flags)
- Section-by-section classification checks
- Keyword matches with matched terms
- Final routing decision

**Example output:**
```
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] ======================================================================
[router-debug] Message (first 200 chars): 'Simple Summary'
[router-debug] Message length: 14 chars
[router-debug] Total attachments: 1
[router-debug]   Attachment 1:
[router-debug]     filename: orb_architecture_map_v22.md
[router-debug]     mime_type: application/octet-stream
[router-debug]     size_bytes: 46932
[router-debug]     is_code: False
[router-debug] Section 6: DOCUMENT FILES - Found 1 document(s)
[router-debug]   Architecture check: False (matched: [])
[router-debug]   Simple doc check: True (matched: ['summari'])
[router-debug]   → Returning CHAT_LIGHT
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: chat_light
[router-debug]   Provider: openai
[router-debug]   Model: gpt-4.1-mini
[router-debug]   Reason: Simple document summary
[router-debug] ======================================================================
```

### Video Deep-Analysis Override (v0.12.8)

Small videos (<10MB) requesting deep analysis now route to the heavy model:

```python
# In job_classifier.py
VIDEO_DEEP_ANALYSIS_KEYWORDS = {
    "find best shots", "extract narrative", "segment scenes",
    "identify key scenes", "select highlight moments",
    "analyse storyline", "structure this video into chapters",
    # ... 18 total keywords
}

def needs_deep_video_analysis(message: str) -> bool:
    """Check if message requests deep video analysis."""
    message_lower = message.lower()
    return any(kw in message_lower for kw in VIDEO_DEEP_ANALYSIS_KEYWORDS)

# Video classification logic:
if total_size > VIDEO_SIZE_THRESHOLD:
    → VIDEO_HEAVY (gemini-3.0-pro-preview)
elif needs_deep_video_analysis(message):  # NEW in v0.12.8
    → VIDEO_HEAVY (gemini-3.0-pro-preview)
else:
    → IMAGE_SIMPLE (gemini-2.0-flash)
```

### Attachment Safety Rule (v0.12.8)

Prevents silent fallback to Claude when attachments are present without explicit job_type:

```python
# In router.py - _check_attachment_safety()
CLAUDE_ALLOWED_WITH_ATTACHMENTS = {"code.medium", "orchestrator"}

def _check_attachment_safety(
    has_attachments: bool,
    job_type_specified: bool,
    provider_id: str,
    decision: RoutingDecision,
) -> tuple[str, str, str]:
    """
    Safety check: Attachments without job_type should not silently go to Claude.
    Returns: (provider_id, model_id, reason)
    """
    if has_attachments and not job_type_specified and provider_id == "anthropic":
        if not is_claude_allowed(decision.job_type):
            # Redirect to GPT instead
            return (
                "openai",
                "gpt-4.1-mini",
                "SAFETY: Attachments present without job_type, redirected from Claude to GPT"
            )
    return (provider_id, decision.model, decision.reason)
```

### Classification Function Signature

```python
def _classify_job_type(message: str, requested_type: str) -> JobType:
    """
    Classify message to determine appropriate job type.
    Priority: explicit request > message content analysis > default
    """
```

### Integration Points

- `main.py._classify_job_type()` calls `job_classifier.classify_message()`
- `router.py.classify_and_route()` calls `job_classifier.classify_job()`
- `router.py._check_attachment_safety()` enforces attachment safety rules
- `stream_router.py._classify_job_type()` uses same classification logic

---

## Router Debug Mode

**Location:** `app/llm/job_classifier.py`, `app/llm/router.py` (NEW in v0.12.17)

**Purpose:** Comprehensive visibility into routing decisions for troubleshooting false positives, misrouting, and classification issues.

### Enabling Debug Mode

Add to `.env`:
```env
ORB_ROUTER_DEBUG=1
```

Then restart the backend:
```powershell
# Stop backend (Ctrl+C in PowerShell)
cd D:\Orb
.venv\Scripts\python main.py
```

### What Gets Logged

**In job_classifier.py:**
- User message content (first 200 chars for privacy)
- Total attachment count
- Per-attachment metadata:
  - filename
  - MIME type
  - size_bytes
  - File type flags (is_code, is_image, is_video, is_pdf)
- Section-by-section classification:
  - Section name (e.g., "Section 6: DOCUMENT FILES")
  - Keyword match results with matched terms
  - Decision at each stage (e.g., "→ Returning ORCHESTRATOR")
- Final classification result:
  - Job Type
  - Provider
  - Model
  - Reason

**In router.py:**
- Task properties (job_type, attachments count, force_provider)
- Pre-classification vs re-classification paths
- Override triggers (e.g., "CHAT_LIGHT → GPT mini")
- Final routing decision

### Log Format

```text
[router-debug] ======================================================================
[router-debug] ROUTER START
[router-debug] ======================================================================
[router-debug] Task job_type: unknown
[router-debug] Task attachments: 1
[router-debug] Task force_provider: None
[router-debug] Priority 2: Using job classifier
[router-debug]   No valid pre-classification, calling classifier...
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] ======================================================================
[router-debug] Message (first 200 chars): "Design a revised high-level architecture..."
[router-debug] Message length: 171 chars
[router-debug] Requested job_type: unknown
[router-debug] Metadata: {}
[router-debug] Total attachments: 1
[router-debug]   Attachment 1:
[router-debug]     filename: orb_architecture_map_v22.md
[router-debug]     mime_type: application/octet-stream
[router-debug]     size_bytes: 46932
[router-debug]     is_code: False
[router-debug]     is_image: False
[router-debug]     is_video: False
[router-debug]     is_pdf: False
[router-debug] Section 6: DOCUMENT FILES - Found 1 document(s)
[router-debug]   Architecture check: True (matched: ['architecture design', 'high-level architecture'])
[router-debug]   → Returning ORCHESTRATOR (architecture design)
[router-debug] ======================================================================
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: orchestrator
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
[router-debug]   Reason: Document with architecture design request: 1 file(s)
[router-debug] ======================================================================
[router-debug]   Classifier returned:
[router-debug]     Job Type: orchestrator
[router-debug]     Provider: anthropic
[router-debug]     Model: claude-opus-4-5-20251101
[router-debug] [router] Final routing: orchestrator → anthropic/claude-opus-4-5-20251101
[router-debug] ======================================================================
[router-debug] ROUTING DECISION FINAL
[router-debug]   Job Type: orchestrator
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
[router-debug]   Reason: Document with architecture design request: 1 file(s)
[router-debug] ======================================================================
```

### Use Cases

1. **Diagnosing misrouting:** See exactly why a message routed to wrong model
2. **Verifying keyword detection:** Confirm which keywords triggered classification
3. **Testing new keywords:** Add keywords and verify they match correctly
4. **Attachment handling:** Verify filename metadata isn't causing false positives (v0.12.17 fix)
5. **Performance analysis:** Count classification attempts and timing

### Performance Impact

**Minimal.** Debug logging adds ~5-10ms per request. Only string formatting and console writes. No impact on production workloads.

### Disabling Debug Mode

Remove or set to 0 in `.env`:
```env
ORB_ROUTER_DEBUG=0
# or just delete the line
```

Restart backend. No code changes required.

---


---

## High-Stakes Critique Pipeline

**Version:** v0.13.6 (Job Type Normalization: v0.13.6, Streaming: v0.13.4, Non-streaming: v0.13.2.2)  
**Purpose:** Quality assurance layer for critical Opus outputs using independent Gemini 3 Pro review

### Overview

The High-Stakes Critique Pipeline is a 3-step quality assurance system for high-stakes Claude Opus outputs. When triggered, it provides an independent review layer before delivering the final answer to the user.

**Supported Endpoints:**
- **Non-streaming:** `/chat` endpoint (router.py v0.13.2.2)
- **Streaming:** `/stream/chat` endpoint (stream_router.py v0.13.4)

### Pipeline Flow

```
User Request
    ↓
┌─────────────────────────────────────────────────────────┐
│ Router: call_llm_async()                                │
│                                                          │
│ 1. Classification & Routing                             │
│    ↓                                                     │
│ 2. Check: Opus + High-Stakes Job Type?                  │
│    ├─ NO → Normal LLM Call                              │
│    └─ YES → Run Critique Pipeline                       │
│                                                          │
│    ┌─────────────────────────────────────────┐         │
│    │ run_high_stakes_with_critique()         │         │
│    │                                          │         │
│    │ Step 1: Opus Draft                      │         │
│    │    └─ registry_llm_call(anthropic/opus) │         │
│    │                                          │         │
│    │ Step 2: Length Check                    │         │
│    │    ├─ <1500 chars → Return draft        │         │
│    │    └─ >=1500 chars → Continue           │         │
│    │                                          │         │
│    │ Step 3: Gemini Critique                 │         │
│    │    └─ call_gemini_critic()              │         │
│    │       └─ registry_llm_call(google/3-pro)│         │
│    │       ├─ Fail → Return draft            │         │
│    │       └─ Success → Continue             │         │
│    │                                          │         │
│    │ Step 4: Opus Revision                   │         │
│    │    └─ call_opus_revision()              │         │
│    │       └─ registry_llm_call(anthropic/opus)│       │
│    │       ├─ Fail → Return draft            │         │
│    │       └─ Success → Return revision      │         │
│    └─────────────────────────────────────────┘         │
│                                                          │
└─────────────────────────────────────────────────────────┘
    ↓
Final Answer to User
```

### Trigger Conditions

The critique pipeline activates when **ALL** of these conditions are met:

1. **Provider Check:** `provider_id == "anthropic"` (only Anthropic calls)
2. **Model Check:** `is_opus_model(model_id)` (True if "opus" in model_id.lower())
3. **Job Type Check:** `is_high_stakes_job(classified_type.value)` (fine-grained string check)
4. **Length Check:** `is_long_enough_for_critique(draft.content)` (>= 1500 chars)

### High-Stakes Job Types

**CRITICAL:** Uses **fine-grained job type STRINGS** (not the coarse 5-type enum):

```python
HIGH_STAKES_JOB_TYPES = {
    "architecture_design",      # System architecture design
    "big_architecture",         # Large-scale architecture changes
    "high_stakes_infra",        # Infrastructure with high impact
    "security_review",          # Security audits and reviews
    "privacy_sensitive_change", # Privacy-impacting changes
    "security_sensitive_change",# Alias for security work
    "complex_code_change",      # Complex multi-file code changes
    "implementation_plan",      # Detailed implementation plans
    "spec_review",              # Specification reviews
    "architecture",             # Legacy alias
    "deep_planning",            # Deep strategic planning
    "orchestrator",             # Generic type (v0.13.6: normalized to specific types)
}
```

**Why fine-grained strings?**
- The 5-type enum (`JobType.ORCHESTRATOR`) collapses many distinct meanings
- Cannot distinguish `"security_review"` from `"casual_chat"` at enum level
- Classifier returns fine-grained strings before enum mapping
- Critique is semantic post-processing that needs semantic intent

**Job Type Normalization (v0.13.6):**
Generic "orchestrator" job type is now normalized to specific high-stakes types based on classification reason keywords:
- Architecture keywords → `"architecture_design"`
- Security keywords → `"security_review"`
- Infrastructure keywords → `"high_stakes_infra"`
- Default → `"architecture_design"`

This ensures architecture design requests with file uploads correctly trigger the critique pipeline even when initially classified as generic "orchestrator".

### Implementation Details

**File:** `app/llm/router.py`

**Configuration Constants:**
```python
HIGH_STAKES_JOB_TYPES = { ... }  # 12 specific job types (including orchestrator)
MIN_CRITIQUE_CHARS = 1500        # ~250 tokens
GEMINI_CRITIC_MODEL = "gemini-3-pro"
GEMINI_CRITIC_MAX_TOKENS = 1024
OPUS_REVISION_MAX_TOKENS = 2048
```

**Helper Functions:**
- `normalize_job_type_for_high_stakes(job_type_str: str, reason: str = "") -> str` — (v0.13.6) Map generic types to specific high-stakes types
- `is_high_stakes_job(job_type_str: str) -> bool` — Check if string in HIGH_STAKES_JOB_TYPES
- `is_opus_model(model_id: str) -> bool` — Check if "opus" in model_id.lower()
- `is_long_enough_for_critique(text: str) -> bool` — Check if len(text) >= MIN_CRITIQUE_CHARS

**Core Pipeline Functions:**
1. `call_gemini_critic(original_task, draft_result, job_type_str)` — Builds critique prompt, calls Gemini 3 Pro
2. `call_opus_revision(original_task, draft_result, critique_result, opus_model_id)` — Builds revision messages, calls Opus
3. `run_high_stakes_with_critique(task, provider_id, model_id, envelope, job_type_str)` — Orchestrates 3-step pipeline

**Integration Point in `call_llm_async()`:**
```python
# v0.13.6: Normalize before checking high-stakes
normalized_job_type = normalize_job_type_for_high_stakes(
    classified_type.value,
    reason
)

should_run_critique = (
    provider_id == "anthropic" and
    is_opus_model(model_id) and
    is_high_stakes_job(normalized_job_type)  # ← Check NORMALIZED type
)

if should_run_critique:
    print(f"[router] HIGH-STAKES PIPELINE: {classified_type.value} → {normalized_job_type}")
    return await run_high_stakes_with_critique(
        task=task,
        provider_id=provider_id,
        model_id=model_id,
        envelope=envelope,
        job_type_str=normalized_job_type,  # ← Pass NORMALIZED type
    )
```

### Error Handling & Graceful Degradation

Every failure point returns the original Opus draft to the user:

| Failure Point | Behavior |
|---------------|----------|
| Opus draft fails | Return error (normal behavior) |
| Draft too short | Return draft (no critique needed) |
| Gemini critic fails | Log warning, return draft |
| Gemini returns empty | Log warning, return draft |
| Opus revision fails | Log warning, return draft |
| Revision returns empty | Log warning, return draft |

**No user is ever blocked from receiving a response.**

### Logging

**Version v0.13.2.2:** Added `print()` statements for console visibility

All critique pipeline actions log with `[critic]` prefix using BOTH `print()` and `logger`:

```python
# v0.13.2.2: Added print() for console visibility
print(f"[critic] High-stakes pipeline enabled: job_type={job_type_str} model={model_id}")
logger.info("[critic] High-stakes pipeline enabled: job_type=%s model=%s", ...)

print(f"[critic] Opus draft complete: {len(draft.content)} chars")

print(f"[critic] Draft too short for critique ({len(draft.content)} chars < {MIN_CRITIQUE_CHARS}), returning draft")

print(f"[critic] Calling Gemini 3 Pro for critique of {job_type_str} task")
logger.info("[critic] Calling Gemini 3 Pro for critique of %s task", ...)

print(f"[critic] Gemini 3 Pro critique completed: {len(result.content)} chars")
logger.info("[critic] Gemini 3 Pro critique completed: %d chars", ...)

print("[critic] Calling Opus for revision using Gemini critique")
logger.info("[critic] Calling Opus for revision using Gemini critique")

print(f"[critic] Opus revision complete: {len(result.content)} chars")
logger.info("[critic] Opus revision complete: %d chars", ...)

print("[critic] Gemini critic failed; returning original Opus draft")
logger.warning("[critic] Gemini critic failed; returning original Opus draft")

print("[critic] Opus revision failed; returning original Opus draft")
logger.warning("[critic] Opus revision failed; returning original Opus draft")
```

**Why both print() and logger?**
- Python logging configuration may not output to console
- `print()` ensures visibility regardless of logging level
- `logger` still captures for log files and production monitoring

### Streaming Integration (v0.13.4)

**File:** `app/llm/stream_router.py`

The critique pipeline is fully integrated into streaming endpoints with "fake streaming" of the final result.

**Architecture:**
```python
# Import from router.py (single source of truth)
from app.llm.router import (
    run_high_stakes_with_critique,
    synthesize_envelope_from_task,
    is_high_stakes_job,
    is_opus_model,
    HIGH_STAKES_JOB_TYPES,
)
```

**Streaming Flow:**
```
User sends security review
    ↓
stream_chat endpoint receives request
    ↓
_classify_job_type() → SECURITY_REVIEW (v0.13.3 security keywords)
    ↓
Check: is_high_stakes_job() AND is_opus_model() → YES
    ↓
Route to: generate_high_stakes_critique_stream()
    ↓
  STEP 1: Build LLMTask with proper schema
  STEP 2: Call run_high_stakes_with_critique() (BLOCKING)
          └─ Opus draft (15-20s)
          └─ Gemini 3 Pro critique (10-15s)
          └─ Opus revision (15-20s)
  STEP 3: chunk_text() splits final answer (~80 chars/chunk)
  STEP 4: "Fake stream" chunks via SSE events
    ↓
Frontend receives final revised answer as stream
    ↓
Save to database (final answer only)
```

**Key Implementation Details:**

1. **No Intermediate Stages Visible:**
   - Frontend sees only final revised answer
   - No draft, critique, or pipeline status messages sent
   - User experience: 45-60s silence, then answer streams

2. **Fake Streaming:**
   - Pipeline runs completely (blocking)
   - Final result split into ~80 char chunks
   - Chunks sent via SSE to simulate streaming
   - No artificial delay between chunks

3. **Trigger Conditions (Same as Non-Streaming):**
   - Provider: `anthropic`
   - Model: Contains "opus"
   - Job type in HIGH_STAKES_JOB_TYPES
   - Response length >= 1500 chars (checked after draft)

4. **Security Review Classification Fix (v0.13.3):**
   - Added dedicated `security_keywords` list (19 keywords)
   - Moved security keywords OUT of generic `review_keywords`
   - Priority: Security (PRIORITY 1) → Architecture → General Review
   - Prevents misclassification as CODE_REVIEW

**Security Keywords (v0.13.3):**
```python
security_keywords = [
    "security review", "security audit", "vulnerability", "vulnerabilities",
    "exploit", "exploits", "penetration test", "pentest",
    "threat model", "threat modeling", "attack surface",
    "sql injection", "xss", "csrf", "session fixation",
    "privilege escalation", "authentication bypass",
    "high stakes", "high-stakes", "security-sensitive"
]
```

**Cost Impact:**
- Streaming without critique: ~$0.045 (Opus only)
- Streaming with critique: ~$0.122 (Opus + Gemini + Opus)
- Overhead: 2.7x for high-stakes streaming

**Example Timeline:**
```
t=0s    User sends "SECURITY REVIEW: ..."
t=0s    [stream_router] Classified: SECURITY_REVIEW (high-stakes)
t=0s    [stream] High-stakes detected: routing through critique pipeline
t=0s    [critic] High-stakes pipeline enabled
t=0-15s [critic] Opus draft complete: 2156 chars
t=15s   [critic] Calling Gemini 3 Pro for critique
t=15-28s [critic] Gemini 3 Pro critique completed: 842 chars
t=28s   [critic] Calling Opus for revision
t=28-48s [critic] Opus revision complete: 2876 chars
t=48s   [stream] Critique pipeline complete: 2876 chars
t=48-53s Frontend receives streamed chunks
```

### Environment-Aware Architecture Critique (v0.13.7)

**CRITICAL FIX:** Architecture critique pipeline was over-engineering designs with enterprise infrastructure (Kubernetes, VLANs, Docker orchestration) instead of respecting single-host, solo-developer deployment reality.

**Root Cause:**
- Generic critique prompt: "Focus on risks and oversights"
- No environment context passed to Gemini critic
- Gemini defaulted to maximal theoretical hardening assumptions
- Result: Designs with K8s clusters, multi-host networks, external CI systems

**Solution: Job-Type-Specific Critique Prompts**

**1. Separate Critique Templates**

```python
# Three specialized critique prompts based on normalized job_type
build_critique_prompt_for_architecture()  # Single-host, pragmatic focus
build_critique_prompt_for_security()     # Aggressive hardening focus
build_critique_prompt_for_general()      # General high-stakes review
```

**2. Architecture Critique Template**

HARD-CODED environment constraints explicitly injected:

```
===== MANDATORY ENVIRONMENT CONSTRAINTS =====

DEPLOYMENT CONTEXT (DO NOT DEVIATE):
- Single Windows 11 workstation (D:\Orb\, D:\SandboxOrb\)
- Solo developer, limited time/budget
- No existing Kubernetes, Docker orchestration, or multi-host infrastructure

HARD CONSTRAINTS (ENFORCED):
You MUST NOT recommend the following unless user explicitly requested them:
  - Kubernetes / Docker Swarm / container orchestration
  - Multiple VLANs / network segmentation
  - Separate VMs/hosts / multi-host deployments
  - External CI runners
  - Egress proxies / doc mirrors

ACCEPTABLE SOLUTIONS (PREFER THESE):
  - Windows security features
  - File-level permissions
  - Process isolation
  - Local sandboxing
  - Windows Credential Manager

===== YOUR REVIEW TASK =====

1. OVER-ENGINEERING CHECK (HIGHEST PRIORITY)
   - Does design introduce K8s, containers, VLANs, separate VMs?
   - If YES and user didn't ask → FLAG AS OVER-ENGINEERED
   - Are there simpler alternatives using local controls?

2. ENVIRONMENT FIT
   - Can solo dev implement this in 2-4 weeks?
   - Does it respect single-host deployment?

3. TECHNICAL CORRECTNESS
   - Actual design errors?
   - Sound patterns and technologies?

4. PRAGMATIC SAFETY
   - Security appropriate for local deployment?
   - Defense-in-depth without infrastructure sprawl?
```

**3. Environment Context Injection**

```python
# v0.13.7: Get environment context for architecture jobs
env_context = get_environment_context()  # Returns deployment constraints dict

# Pass to critique prompt builder
critique_prompt = build_critique_prompt(
    draft_text=draft_result.content,
    original_request=original_request,
    job_type_str="architecture_design",  # Normalized type
    env_context=env_context  # ← NEW: Environment awareness
)
```

**4. Environment Context Dict**

```python
{
    "deployment_type": "single_host",
    "os": "Windows 11",
    "repos": ["D:\\Orb\\", "D:\\SandboxOrb\\"],
    "team_size": "solo_developer",
    "infrastructure": "local_only",
    "phase": "early_self_improvement_pipeline",
    "constraints": {
        "no_kubernetes": True,
        "no_docker_orchestration": True,
        "no_multi_host": True,
        # ... 7 total constraints
    },
    "acceptable_infra": [
        "Windows security features",
        "File-level permissions",
        # ... 4 items
    ],
    "forbidden_unless_explicit": [
        "Kubernetes",
        "Docker Swarm",
        # ... 6 items
    ]
}
```

**Security Critique Template (Unchanged)**

Maintains aggressive hardening focus - allowed to recommend enterprise infra if security-justified.

**Expected Behavior After Fix:**

| Scenario | v0.13.6 (Broken) | v0.13.7 (Fixed) |
|----------|------------------|-----------------|
| Architecture Design | Recommends K8s, VLANs, Docker orchestration, external CI | Uses file permissions, process isolation, Windows security features |
| Security Review | Aggressive hardening (correct) | Aggressive hardening (unchanged) |
| Over-engineering | Not detected | Explicitly flagged with simpler alternatives |

**Logs:**
```
[critic] Passing environment context to architecture critique: single_host, solo_developer
[critic] Calling Gemini 3 Pro for critique of architecture_design task
```

**Files Changed:**
- `app/llm/router.py` (v0.13.7)
  - Added `get_environment_context()` - Returns hard-coded deployment constraints
  - Added `build_critique_prompt_for_architecture()` - Environment-aware template
  - Added `build_critique_prompt_for_security()` - Aggressive hardening template  
  - Added `build_critique_prompt_for_general()` - Fallback template
  - Modified `build_critique_prompt()` - Router function selecting correct template
  - Modified `call_gemini_critic()` - Passes environment context for architecture jobs

**stream_router.py Security Classification Fix (v0.13.7)**

Removed generic keywords from security classification to prevent false positives:

**REMOVED from security_keywords:**
- `"high stakes"`, `"high-stakes"`
- `"high risk"`, `"high-risk"`
- `"critical"`

**KEPT (concrete security terms only):**
- `"security review"`, `"security audit"`, `"threat model"`
- `"vulnerability"`, `"exploit"`, `"attack vector"`
- `"sql injection"`, `"xss"`, `"csrf"`
- `"encryption review"`, `"key management"`, `"secrets management"`

**Impact:**
- Architecture prompts with "high-stakes" no longer misclassify as SECURITY_REVIEW
- Only explicit security-focused terms trigger security classification

### Environment Variables

Configuration in `.env`:

```bash
# Gemini critic model (must be gemini-3-pro-preview or better)
# Updated v0.13.4: Requires -preview suffix for Gemini 3 Pro
GEMINI_OPUS_CRITIC_MODEL=gemini-3-pro-preview

# Minimum response length in characters to trigger critique (~375 tokens)
ORB_MIN_CRITIQUE_CHARS=1500

# Max tokens for Gemini critique call (prevents cost explosion)
GEMINI_CRITIC_MAX_TOKENS=1024

# Max tokens for Opus revision call (prevents cost explosion)
OPUS_REVISION_MAX_TOKENS=2048

# Debug mode - shows detailed pipeline logs (v0.13.2.2)
ORB_ROUTER_DEBUG=1
```

**Gemini 3 Pro Notes:**
- Model ID: `gemini-3-pro-preview` (requires -preview suffix)
- Released: December 5, 2024
- Context: 1M input / 64k output tokens
- Pricing: $2/$12 per 1M tokens (<200k context)
- Requires billing enabled (no free tier for API usage)

### Cost Impact

| Step | Model | Tokens In | Tokens Out | Cost (est.) |
|------|-------|-----------|------------|-------------|
| 1. Opus Draft | claude-opus-4.5 | 500 | 1500 | $0.045 |
| 2. Gemini Critique | gemini-3-pro | 2000 | 800 | $0.014 |
| 3. Opus Revision | claude-opus-4.5 | 2500 | 1800 | $0.063 |
| **Total** | | | | **~$0.122** |

**Without critique:** ~$0.045  
**With critique:** ~$0.122  
**Overhead:** ~2.7x cost, but only for high-stakes requests (~10% of total)

**Monthly estimate:** +$25-50 for typical usage patterns

### Testing

See [Testing Checklist](#testing-checklist) for verification tests:
1. Simple summary → No critique (not high-stakes)
2. Architecture design → Critique triggers
3. Security review → Critique triggers
4. Short Opus response → No critique (too short)
5. Sonnet code → No critique (not Opus)

### Troubleshooting

**Critique not triggering:**
1. Enable debug mode: `ORB_ROUTER_DEBUG=1`
2. Check provider is "anthropic" (not OpenAI/Google)
3. Check model contains "opus" (not Sonnet)
4. Check job type string in HIGH_STAKES_JOB_TYPES
5. Check response length >= 1500 chars
6. **Streaming:** Check logs show `[stream] High-stakes detected`

**Critique always failing:**
1. Verify `GOOGLE_API_KEY` is valid
2. Check model name is `gemini-3-pro-preview` (requires -preview suffix)
3. Verify Google API has billing enabled (Gemini 3 Pro requires paid account)
4. Verify Google API quota not exceeded
5. Test network connectivity to Google AI
6. Check for "model not found" errors in logs

**No [critic] logs visible:**
1. Verify router.py is v0.13.2.2 or later (has print() statements)
2. Check console output (print() bypasses logging config)
3. Enable `ORB_ROUTER_DEBUG=1` for additional diagnostics

**Streaming shows silence then sudden response:**
- This is EXPECTED behavior for critique pipeline in streaming
- 45-60s silence while pipeline runs
- Then final answer streams normally
- Check logs to confirm pipeline completed

**Security reviews routing to wrong model:**
1. Verify stream_router.py is v0.13.3 or later
2. Check `security_keywords` list includes your terms
3. Should classify as SECURITY_REVIEW, not CODE_REVIEW
4. Check logs: `[stream_router] Classified: SECURITY_REVIEW`

**High costs:**
1. Increase `ORB_MIN_CRITIQUE_CHARS` (e.g., 2000)
2. Reduce token limits (CRITIC: 512, REVISION: 1024)
3. Remove less critical job types from HIGH_STAKES_JOB_TYPES
4. Use cheaper model: `GEMINI_OPUS_CRITIC_MODEL=gemini-2.5-pro`

### Documentation

- **CRITIQUE_PIPELINE_DOCUMENTATION.md** — Full technical specification (1,000+ lines)
- **CRITIQUE_PIPELINE_DEPLOYMENT_GUIDE.md** — Step-by-step deployment (500+ lines)
- **CRITIQUE_PIPELINE_SUMMARY.md** — Quick reference guide

---

## Critical Pipeline Specification Modules

**Version:** 0.14.2  
**Location:** `app/llm/`  
**Purpose:** Modular infrastructure for enhanced routing, auditing, and graceful degradation

The Critical Pipeline Specification (CPS) defines 7 new modules that enhance Orb's multimodal routing with stable file naming, audit trails, fallback handling, and advanced preprocessing. These modules are designed to be backward-compatible and optional—existing functionality is preserved when modules are unavailable.

### Module Overview

| Module | Spec Section | Purpose | Status |
|--------|--------------|---------|--------|
| `file_classifier.py` | §1-§2 | File type classification + stable [FILE_X] naming | ✅ Integrated |
| `audit_logger.py` | §12 | Audit trail logging for pipeline transparency | ✅ Integrated |
| `relationship_detector.py` | §3 | Detect relationships between files | ✅ Available |
| `preprocessor.py` | §5-§6 | Task preprocessing + context building | ✅ Available |
| `token_budgeting.py` | §7 | Token budget allocation + truncation strategies | ✅ Available |
| `task_extractor.py` | §4 | Multi-task extraction from messages | ✅ Available |
| `fallbacks.py` | §11 | Fallback chains + graceful degradation | ✅ Integrated |

**Integrated** = Wired into router.py and job_classifier.py  
**Available** = Module complete, ready for future integration

### File Classifier (file_classifier.py)

**Purpose:** Classify attachments by type and generate stable file identifiers for multimodal prompts.

**Key Functions:**

| Function | Purpose |
|----------|---------|
| `classify_attachments(attachments)` | Classify list of raw attachments |
| `classify_from_attachment_info(infos)` | Classify from AttachmentInfo objects |
| `build_file_map(result)` | Generate stable [FILE_X] naming map |
| `has_vision_content(result)` | Check if classification contains vision files |
| `get_file_by_id(result, id)` | Retrieve file by stable identifier |

**File Types (FileType enum):**

```python
class FileType(str, Enum):
    IMAGE = "image"           # .jpg, .png, .gif, .webp
    VIDEO = "video"           # .mp4, .mov, .avi, .webm
    AUDIO = "audio"           # .mp3, .wav, .ogg, .m4a
    PDF = "pdf"               # .pdf (may contain images)
    DOCUMENT = "document"     # .docx, .doc, .odt (may contain images)
    SPREADSHEET = "spreadsheet"  # .xlsx, .xls, .csv
    CODE = "code"             # .py, .js, .ts, .java, etc.
    TEXT = "text"             # .txt, .md, .json, .xml
    ARCHIVE = "archive"       # .zip, .tar, .gz
    MIXED_FILE = "mixed_file" # Document with embedded images
    UNKNOWN = "unknown"       # Unrecognized type
```

**Stable File Naming ([FILE_X]):**

```python
# Build file map from classification
result = classify_attachments(attachments)
file_map = build_file_map(result)

# Example output:
"""
=== FILE REFERENCE MAP ===
[FILE_1] screen_recording.mp4 (video, 15.2MB)
[FILE_2] main.py (code, 4.3KB)
[FILE_3] architecture_diagram.png (image, 892KB)

When referring to files, use [FILE_X] identifiers.
"""
```

**MIXED_FILE Detection:**

PDFs and DOCX files with embedded images are classified as `MIXED_FILE` and routed to vision models:

```python
# In job_classifier.py - Section 7.5
if modality_flags.get("has_mixed"):
    return _make_decision(
        JobType.IMAGE_COMPLEX,
        Provider.GOOGLE,
        "gemini-2.5-pro",
        "Mixed document with embedded images requires vision",
        file_map=modality_flags.get("file_map")
    )
```

### Audit Logger (audit_logger.py)

**Purpose:** Comprehensive audit trail for high-stakes pipeline operations.

**Key Components:**

```python
class AuditEventType(str, Enum):
    TASK_RECEIVED = "task_received"
    CLASSIFICATION_START = "classification_start"
    CLASSIFICATION_COMPLETE = "classification_complete"
    ROUTING_DECISION = "routing_decision"
    MODEL_CALL_START = "model_call_start"
    MODEL_CALL_COMPLETE = "model_call_complete"
    FALLBACK_TRIGGERED = "fallback_triggered"
    PIPELINE_COMPLETE = "pipeline_complete"
    ERROR = "error"

class RoutingTrace:
    """Records all events in a single routing operation."""
    trace_id: str
    events: List[AuditEvent]
    
    def add_event(self, event_type, details, ...): ...
    def to_dict(self) -> Dict: ...
```

**Integration in High-Stakes Pipeline:**

```python
# In router.py - run_high_stakes_with_critique()
if AUDIT_AVAILABLE:
    audit_logger = get_audit_logger()
    trace = audit_logger.start_trace(task_id=task.task_id)
    trace.add_event(AuditEventType.TASK_RECEIVED, {"job_type": job_type_str})

# After each model call
trace.add_event(
    AuditEventType.MODEL_CALL_COMPLETE,
    {"model": "claude-opus-4-5", "chars": len(result.content)}
)
```

### Fallbacks Module (fallbacks.py)

**Purpose:** Structured fallback handling for graceful degradation when models fail.

**Failure Types:**

```python
class FailureType(str, Enum):
    VIDEO_TRANSCRIPTION_FAILED = "video_transcription_failed"
    IMAGE_PROCESSING_FAILED = "image_processing_failed"
    MODEL_UNAVAILABLE = "model_unavailable"
    MODEL_RATE_LIMITED = "model_rate_limited"
    MODEL_TIMEOUT = "model_timeout"
    MODEL_ERROR = "model_error"
    OVERWATCHER_UNAVAILABLE = "overwatcher_unavailable"
    CRITIQUE_FAILED = "critique_failed"
    REVISION_FAILED = "revision_failed"
```

**Fallback Chains:**

```python
FALLBACK_CHAINS = {
    "code": ["claude-sonnet-4-5", "claude-opus-4-5", "gpt-4.1"],
    "vision": ["gemini-2.5-pro", "gemini-2.0-flash", "gpt-4.1-vision"],
    "video": ["gemini-3-pro", "gemini-2.5-pro"],
    "critical": ["claude-opus-4-5", "claude-sonnet-4-5", "gpt-4.1"],
    "text": ["gpt-4.1", "claude-sonnet-4-5"],
    "critique": ["gemini-3-pro", "claude-sonnet-4-5"],
}
```

**FallbackHandler Class:**

```python
handler = FallbackHandler(role="code", max_attempts=3)

result = await handler.execute_with_fallback(
    operation=async_model_call,
    initial_provider="anthropic",
    initial_model="claude-sonnet-4-5"
)

if result.fallback_used:
    print(f"Used fallback: {result.final_model}")
    for event in result.fallback_events:
        print(f"  - {event.failure_type}: {event.error_message}")
```

**Video Failure Handling:**

```python
# In router.py - run_video_code_debug_pipeline()
try:
    transcript = await transcribe_video(video_path)
except Exception as e:
    if FALLBACKS_AVAILABLE:
        action, event = handle_video_failure(e, has_other_modalities=has_code)
        if action == FallbackAction.DEGRADE_GRACEFULLY:
            # Continue without video, use code files only
            return await call_code_model(code_files, message)
```

### File Map Injection

**Purpose:** Inject stable [FILE_X] references into all multimodal prompts.

**Implementation in router.py:**

```python
def inject_file_map_into_messages(
    messages: List[Dict[str, Any]],
    file_map: str
) -> List[Dict[str, Any]]:
    """
    Add file map to system message for stable file references.
    """
    if not file_map:
        return messages
    
    file_map_instruction = (
        f"\n\n{file_map}\n\n"
        "When referring to files, always use [FILE_X] identifiers "
        "for clarity and consistency."
    )
    
    # Find or create system message
    for msg in messages:
        if msg.get("role") == "system":
            msg["content"] += file_map_instruction
            return messages
    
    # No system message found, prepend one
    return [{"role": "system", "content": file_map_instruction}] + messages
```

**Data Flow:**

```text
1. Attachments uploaded
       ↓
2. compute_modality_flags() calls file_classifier
       ↓
3. build_file_map() generates [FILE_X] naming
       ↓
4. inject_file_map_into_messages() adds to system prompt
       ↓
5. LLM receives stable file references
       ↓
6. Response uses [FILE_1], [FILE_2], etc.
```

### Integration Points

**job_classifier.py (v0.14.2):**

```python
# Import file classifier with fallback
try:
    from app.llm.file_classifier import (
        classify_from_attachment_info,
        build_file_map,
        has_vision_content,
        FileType,
    )
    FILE_CLASSIFIER_AVAILABLE = True
except ImportError:
    FILE_CLASSIFIER_AVAILABLE = False

def compute_modality_flags(attachments):
    """Enhanced with file classification and file map."""
    # ... existing logic ...
    
    if FILE_CLASSIFIER_AVAILABLE and attachment_infos:
        classification_result = classify_from_attachment_info(attachment_infos)
        file_map = build_file_map(classification_result)
        
        # Check for mixed files (PDFs with images)
        has_mixed = any(
            f.file_type == FileType.MIXED_FILE 
            for f in classification_result.files
        )
        
        return {
            **base_flags,
            "has_mixed": has_mixed,
            "mixed_count": sum(1 for f in classification_result.files if f.file_type == FileType.MIXED_FILE),
            "classification_result": classification_result,
            "file_map": file_map,
        }
    
    return base_flags
```

**router.py (v0.14.2):**

```python
# Import all Critical Pipeline modules with fallback
try:
    from app.llm.audit_logger import get_audit_logger, RoutingTrace, AuditEventType
    AUDIT_AVAILABLE = True
except ImportError:
    AUDIT_AVAILABLE = False

try:
    from app.llm.fallbacks import (
        FallbackHandler, handle_video_failure, handle_vision_failure,
        get_fallback_chain, FailureType, FallbackAction
    )
    FALLBACKS_AVAILABLE = True
except ImportError:
    FALLBACKS_AVAILABLE = False

# ... similar for other modules ...

async def call_llm_async(task: LLMTask) -> LLMResult:
    """Enhanced with file map injection and audit logging."""
    # Compute modality flags with file classification
    modality_flags = compute_modality_flags(task.attachments or [])
    file_map = modality_flags.get("file_map")
    
    # Synthesize envelope with file map
    envelope = synthesize_envelope_from_task(task, file_map=file_map)
    
    # ... rest of routing logic ...
```

### Environment Variables

```bash
# Enable/disable Critical Pipeline features
ORB_AUDIT_ENABLED=1           # Enable audit trail logging (default: 1)
ORB_FALLBACK_ENABLED=1        # Enable fallback behavior (default: 1)
ORB_MAX_FALLBACK_ATTEMPTS=3   # Max attempts before giving up (default: 3)

# Debug mode shows file map generation
ORB_ROUTER_DEBUG=1
```

### Module Availability Flags

All modules are optional. The router checks availability before using:

```python
# In router.py
AUDIT_AVAILABLE = False
FILE_CLASSIFIER_AVAILABLE = False
RELATIONSHIP_DETECTOR_AVAILABLE = False
TOKEN_BUDGETING_AVAILABLE = False
FALLBACKS_AVAILABLE = False

try:
    from app.llm.audit_logger import get_audit_logger
    AUDIT_AVAILABLE = True
except ImportError:
    pass

# Check availability before use
if AUDIT_AVAILABLE:
    trace = get_audit_logger().start_trace()
```

### Testing Critical Pipeline

**File Classification:**
```
Upload: report.pdf (with embedded charts)
Expected: MIXED_FILE classification → Gemini 2.5 Pro
Logs: [router-debug] MIXED_FILE detected, routing to vision
```

**File Map Injection:**
```
Upload: video.mp4, main.py, diagram.png
Expected: System prompt contains:
  === FILE REFERENCE MAP ===
  [FILE_1] video.mp4 (video, 15.2MB)
  [FILE_2] main.py (code, 4.3KB)
  [FILE_3] diagram.png (image, 892KB)
```

**Fallback Handling:**
```
Scenario: Gemini 3 Pro unavailable
Expected: Falls back to Gemini 2.5 Pro
Logs: [fallback] MODEL_UNAVAILABLE, trying next in chain
```

**Audit Logging:**
```
Request: High-stakes architecture design
Expected: Audit trace with all model calls logged
Logs: [audit] trace_id=abc123, events=[TASK_RECEIVED, MODEL_CALL_START, ...]
```

### Future Integration (Not Yet Wired)

The following modules are complete but not yet integrated into routing decisions:

- **relationship_detector.py** — Could inform routing based on file dependencies
- **task_extractor.py** — Could enable parallel processing of multi-task messages
- **preprocessor.py** — Could build optimized context for complex jobs
- **token_budgeting.py** — Could truncate context intelligently for large inputs

These will be wired in future versions as needed.

---

## Phase 4: Job Engine & Artefacts

**Status:** OPTIONAL (requires `ORB_ENABLE_PHASE4=true`)

### Provider Registry (app/providers/registry.py)

Single source of truth for LLM calls in Phase 4:

- **API key encryption/decryption** — Keys stored encrypted at rest
- **Rate limiting** — Per-provider, per-minute limits
- **Retry logic** — Exponential backoff on failures
- **Bounded usage log** — `deque(maxlen=5000)` for memory safety
- **Usage tracking** — Job/session/project context for billing

### Key Schema Fixes in v0.12.1

```python
# JobBudget field names:
JobBudget(
    max_tokens=8000,
    max_cost_estimate=1.0,
    max_wall_time_seconds=60,
)

# JobEnvelope:
modalities_in=[Modality.TEXT]  # NOT 'modalities'
needs_tools=[]                 # Must be list[str], NOT False

# Use RoutingOptions for per-task options, not RoutingConfig
task = LLMTask(
    routing=RoutingOptions(),  # NOT RoutingConfig()
)
```

---

## Provider Registry

**Location:** `app/providers/registry.py` (Phase 4 only)

**Purpose:** Single source of truth for all LLM API calls in Phase 4 mode

### Capabilities

| Provider | Capabilities |
|----------|-------------|
| OpenAI | `chat`, `streaming`, `embeddings` |
| Anthropic | `chat`, `streaming`, `vision` (via prompt injection) |
| Gemini | `chat`, `streaming`, `vision`, `video`, `web_search` |

### Registry Functions

| Function | Purpose |
|----------|---------|
| `get_provider_capabilities(provider)` | List what a provider can do |
| `call_provider(provider, capability, params)` | Generic provider call |
| `supports_modality(provider, modality)` | Check if provider handles modality |

**Note:** Phase 4 only. Main chat flow uses `router.py` and `clients.py`.

### Anthropic System Message Format (v0.13.5)

**CRITICAL FIX:** Anthropic API requires system messages as separate top-level `system` parameter, not in messages array.

**Problem:**
- System prompts were embedded in messages array: `[{"role": "system", "content": "..."}, ...]`
- Anthropic API rejected requests with 400 Bad Request
- All Anthropic calls with system prompts failed

**Solution (registry.py lines ~200-260):**
```python
# Extract system message from messages array
system_content = None
filtered_messages = []
for msg in messages:
    if msg["role"] == "system":
        system_content = msg["content"]
    else:
        filtered_messages.append(msg)

# Pass system separately
response = client.messages.create(
    system=system_content,  # Top-level parameter
    messages=filtered_messages  # No system messages in array
)
```

**Result:** Anthropic API calls now work correctly with system prompts.

---

## DateTime Context

**Location:** `main.py`, `stream_router.py`

All LLM calls include current datetime in system prompt:

```python
datetime_context = f"\n\nCurrent date and time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
system_prompt = DEFAULT_SYSTEM_PROMPT + datetime_context
```

**Purpose:** LLMs can reference current date/time in responses (e.g., "today", "this week")

---

## Streaming Responses

**Location:** `app/llm/streaming.py`, `stream_router.py`

### Architecture

```text
Frontend (SSE listener)
    ↓
/stream/chat endpoint
    ↓
stream_*_llm() in clients.py
    ↓
extract_thinking_and_answer() in streaming.py
    ↓
SSE chunks (thinking + answer)
    ↓
Frontend displays in reasoning panel + message
```

### SSE Event Types

| Event | Data | Purpose |
|-------|------|---------|
| `metadata` | `{type: "metadata", provider: "...", model: "..."}` | First event |
| `token` | `{type: "token", content: "..."}` | Incremental response |
| `reasoning` | `{type: "reasoning", content: "..."}` | Chain-of-thought (v0.12.4) |
| `done` | `{type: "done", provider: "...", model: "...", total_length: N}` | Complete |
| `error` | `{type: "error", error: "..."}` | Failed |

### Reasoning Tags (v0.12.4)

```xml
<THINKING>
Your reasoning process here...
</THINKING>
<ANSWER>
Your visible answer here...
</ANSWER>
```

The streaming system extracts content between these tags and sends:
- `<THINKING>` content → `reasoning` SSE events (displayed in reasoning panel)
- `<ANSWER>` content → `token` SSE events (displayed in chat)

### Streaming Routing

1. `stream_router.py` receives request
2. `_classify_job_type()` analyzes message (5-type system)
3. `_select_provider_for_job_type()` determines provider/model
4. `streaming.py` streams from selected provider with tag extraction
5. Provider/model sent in metadata event

---

## Reasoning Panel

**Location:** `orb-desktop/src/components/ReasoningPanel.tsx`

**Purpose:** Display LLM chain-of-thought during streaming

**Features:**

- Real-time updates as thinking streams
- Collapsible panel
- Markdown rendering for structured thinking
- Syntax highlighting for code in reasoning

**Trigger:** Appears when streaming response contains `<THINKING>` tags

---

## Markdown Rendering

**Location:** `orb-desktop/src/components/MessageList.tsx`

**Libraries:**

- `react-markdown` — Markdown parser
- `remark-gfm` — GitHub-flavored markdown (tables, strikethrough, task lists)
- `react-syntax-highlighter` — Code block highlighting

**Features:**

- Inline code: `backticks`
- Code blocks with language detection
- Tables, lists, headers
- Links (auto-linkified)
- Images (from markdown)

**Example:**

```markdown
Here's a Python function:

\`\`\`python
def greet(name):
    return f"Hello, {name}!"
\`\`\`

And a table:

| Column 1 | Column 2 |
|----------|----------|
| Value A  | Value B  |
```

---

## Web Search

**Location:** `app/llm/web_search_router.py`, `app/llm/clients.py`

**Provider:** Gemini (via Google AI SDK)

**Endpoint:** `POST /web_search`

**Request:**
```json
{
  "query": "Latest news on AI models",
  "project_id": 1
}
```

**Response:**
```json
{
  "provider": "gemini",
  "content": "Here's what I found...",
  "grounding_metadata": {
    "search_queries": ["AI models 2025"],
    "grounding_chunks": [...]
  }
}
```

**Features:**

- Real-time web grounding
- Automatic citation of sources
- Saved to message history
- Model badge shows "Gemini (Search)"

---

## Chat History

**Location:** `app/memory/router.py` (messages endpoints)

**Endpoints:**

- `GET /memory/messages?project_id=1&limit=50` — Retrieve history
- `POST /memory/messages` — Save new message

**Storage:** Messages table with columns:

- `id`, `project_id`, `role`, `content`, `provider`, `model`, `created_at`

**Frontend:** Loads last 50 messages on project switch

---

## File Ingestion Pipeline

**Location:** `main.py` (upload handling), `app/llm/file_analyzer.py` (extraction)

### Flow

```text
1. User uploads file via /chat_with_attachments
2. Save to data/files/{project_id}/{uuid}.{ext}
3. Detect MIME type + doc_type
4. If binary (video/audio/image) → Skip text extraction, route to vision
5. If text/document → Extract text:
   - .txt/.md/.py/.js/.etc → Read as UTF-8
   - .docx → python-docx
   - .pdf → PyMuPDF (fitz)
6. Generate summary via GPT (100-word)
7. Save file record to DB
8. Index file content for semantic search
9. Attach to LLM context (separate from message, v0.12.17; content injection fix v0.13.5)
10. Route based on file type + message
```

### Supported File Types

| Type | Extraction Method | Notes |
|------|-------------------|-------|
| `.docx` | python-docx | Full text extraction |
| `.pdf` | PyMuPDF (fitz) | Text + basic structure |
| `.txt`, `.md`, `.csv` | Direct read | UTF-8 encoding |
| Code files | Direct read | .py, .js, .ts, etc. |
| Images | Vision analysis | Returns description |
| Video | **Skip text extraction** | Routes to Gemini Video |
| Audio | **Skip text extraction** | Future: transcription |

### Binary File Handling (v0.12.7)

Binary files (video, audio, images) are detected and skipped from text extraction:

```python
# file_analyzer.py
def extract_text_content(filepath: str, mime_type: str = None) -> Optional[str]:
    if is_binary_file(filepath, mime_type):
        print(f"[file_analyzer] Skipping binary file (route to vision): {filepath}")
        return None
    # ... normal text extraction
```

### Binary Detection (v0.12.7)

```python
# In file_analyzer.py
BINARY_EXTENSIONS = {
    # Video
    ".mp4", ".avi", ".mov", ".mkv", ".webm", ".m4v", ".wmv", ".flv", ".mpeg", ".mpg",
    # Audio
    ".mp3", ".wav", ".flac", ".aac", ".ogg", ".wma", ".m4a",
    # Images
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp", ".tiff", ".ico", ".svg", ".heic",
}

def is_binary_file(filepath: str, mime_type: str = None) -> bool:
    """Check if file is binary (should skip text extraction)."""
```

Files detected as binary return `None` from `extract_text_content()` and are routed to vision/video analysis instead.

### Document Content Injection (v0.13.5)

**CRITICAL FIX:** Prior to v0.13.5, uploaded files were extracted but not passed to LLM context.

**Problem:** 
- Files were extracted into `attachments_as_text` list
- Text extraction worked correctly
- But `full_context` never included the actual file content
- LLMs received filenames but not file contents → hallucinated responses

**Solution (main.py lines ~800-830):**
```python
# Build document content parts
document_content_parts = []
for att in attachments_as_text:
    doc_part = f"\n\n--- Document: {att['filename']} ---\n{att['text']}\n--- End Document ---"
    document_content_parts.append(doc_part)

# Inject into full context
full_context = base_context
if document_content_parts:
    full_context += "\n".join(document_content_parts)
```

**Result:** LLMs now receive actual file content and can reference it in responses.

### PDF Library Priority

```python
# file_analyzer.py tries libraries in order:
try:
    import fitz  # PyMuPDF - preferred
except ImportError:
    try:
        import pypdf  # fallback
    except ImportError:
        # Returns error: "No PDF library available"
```

---

## Image Analysis Pipeline

**Location:** `app/llm/gemini_vision.py` (simplified in v0.14.1)

### Functions

| Function | Purpose |
|----------|---------|
| `analyze_image(image_source, mime_type, user_prompt)` | Structured analysis (summary, tags, type) |
| `ask_about_image(image_source, user_question, mime_type, context)` | Q&A about image |
| `check_vision_available()` | Check if vision providers are configured |
| `get_vision_model_for_complexity(question)` | Select model tier based on question |

### Provider Fallback

1. Try Gemini Vision first
2. On failure (rate limit, quota, error), fall back to OpenAI GPT-4o
3. OpenAI does NOT support video (images only)

### Image Routing (v0.14.1 — SIMPLIFIED)

```text
Image Routing Priority:
1. Code + Images (no video) → CODE_MEDIUM/ORCHESTRATOR (code wins!)
2. Images only → IMAGE_COMPLEX (Gemini 2.5 Pro)

NO MORE FLASH for images — all images use Gemini 2.5 Pro
IMAGE_SIMPLE (Flash) reserved for legacy/manual override only
```

**Key Change:** If you upload `.py` + `.jpg`, it routes to **Sonnet** (code priority), not Gemini.

### Model Tiers (Legacy)

```python
# Still available for manual override, but classifier uses simplified routing
def _get_model_name(tier: str = "default") -> str:
    if tier == "fast":
        return os.getenv("GEMINI_VISION_MODEL_FAST", "gemini-2.0-flash")
    elif tier == "complex":
        return os.getenv("GEMINI_VISION_MODEL_COMPLEX", "gemini-2.5-pro")
    else:
        return os.getenv("GEMINI_VISION_MODEL", "gemini-2.0-flash")
```

### Media Routing Without User Message (v0.12.8)

Images/videos now route to Gemini even without a user message:

```python
# In main.py
if has_media:  # Route ALL media to Gemini
    vision_prompt = user_message if user_message else "Describe this image/video in detail."
```

---

## Video+Code Debug Pipeline

**Location:** `app/llm/router.py` (NEW in v0.14.1)

**Purpose:** 2-step pipeline for debugging code issues using video screen recordings

### Pipeline Flow

```text
User Request: Video recording + Code files + "Debug this"
    ↓
┌─────────────────────────────────────────────────────────────────┐
│ Step 1: Video Transcription (Gemini 3 Pro)                      │
│                                                                  │
│ - compute_modality_flags() detects video + code                 │
│ - Job classified as VIDEO_CODE_DEBUG                            │
│ - transcribe_video_for_context() called for each video          │
│ - Transcription prompt optimized for debugging:                 │
│   - Timeline of events                                          │
│   - Error messages visible on screen                            │
│   - UI actions and clicks                                       │
│   - Console/log output                                          │
│                                                                  │
│ Output: Structured text transcript                              │
└─────────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────────┐
│ Step 2: Code Analysis (Claude Sonnet)                           │
│                                                                  │
│ - Video transcripts injected as system context                  │
│ - Code files analyzed with transcript context                   │
│ - Sonnet identifies bugs based on:                              │
│   - Visual behavior from transcript                             │
│   - Code logic from attached files                              │
│                                                                  │
│ Output: Bug analysis and fix recommendations                    │
└─────────────────────────────────────────────────────────────────┘
    ↓
Final Answer to User
```

### Trigger Conditions

```python
# In job_classifier.py
flags = compute_modality_flags(attachments)

if flags["has_video"] and flags["has_code"]:
    return VIDEO_CODE_DEBUG  # Routes to 2-step pipeline
```

### Implementation

**Key Functions:**
- `compute_modality_flags(attachments)` — Detects video, image, code attachments
- `transcribe_video_for_context(video_path, custom_prompt)` — Gemini 3 Pro transcription
- `run_video_code_debug_pipeline(task, envelope)` — Orchestrates 2-step pipeline

**Logging:**
```
[router] VIDEO+CODE PIPELINE: video.code.debug → Gemini3 transcribe → Sonnet code
[video-code] Starting Video+Code debug pipeline
[video-code] Found 1 video(s), 1 code file(s)
[video-code] Step 1: Transcribing video: screen_recording.mp4
[video-transcribe] Transcribing video: screen_recording.mp4 (15.2MB)
[video-transcribe] Uploading video to Gemini File API...
[video-transcribe] Video ready, generating transcript with Gemini 3 Pro...
[video-transcribe] Transcript complete: 2847 chars
[video-code] Step 1 complete: 1 video transcript(s)
[video-code] Step 2: Calling Sonnet with code + transcripts
[video-code] Pipeline complete: 1523 chars
```

---

## Video Analysis Pipeline

**Location:** `app/llm/gemini_vision.py` (NEW in v0.12.7, enhanced v0.12.8, simplified v0.14.1)

### Video Detection

```python
# In file_analyzer.py
def is_video_mime_type(mime_type: str) -> bool:
    return mime_type.startswith("video/") if mime_type else False

VIDEO_EXTENSIONS = {".mp4", ".avi", ".mov", ".mkv", ".webm", ".m4v", ".wmv", ".flv"}
```

### Video Analysis Flow (v0.14.1 — SIMPLIFIED)

```text
1. Upload Detection
   - main.py detects video via is_video_mime_type()
   - Video saved to disk (data/files/{project_id}/{uuid}.ext)
   - NO text extraction attempted (binary file)

2. Routing (SIMPLIFIED in v0.14.1)
   - Video + Code → VIDEO_CODE_DEBUG (2-step pipeline)
   - ALL other videos → VIDEO_HEAVY (Gemini 3.0 Pro)
   - NO MORE FLASH for videos — all videos use Pro model

3. Analysis (analyze_video function)
   - Upload video to Gemini File API: genai.upload_file(path)
   - Wait for processing (poll state until not "PROCESSING")
   - Generate response: model.generate_content([video_file, prompt])
   - Cleanup: genai.delete_file(video_file.name)

4. Response
   - Returns: {answer, provider: "google", model: "gemini-3.0-pro-preview"}
   - Error handling for quota (429), processing failures
```

### Key Functions

```python
def analyze_video(
    video_path: Union[str, Path],
    user_question: Optional[str] = None,
    context: Optional[str] = None,
) -> dict:
    """
    Analyze a video file using Gemini Vision.
    
    Returns:
        dict with: answer, provider, model, error (optional)
    """

# NEW in v0.14.1
async def transcribe_video_for_context(
    video_path: Union[str, Path],
    custom_prompt: Optional[str] = None,
) -> str:
    """
    Transcribe video to text for use in multi-step pipelines.
    Uses Gemini 3 Pro for high-quality transcription.
    
    Returns:
        str: Structured text description of video content
    """
```

---

## File Analyzer Helper Functions

**Location:** `app/llm/file_analyzer.py` (Enhanced in v0.12.8)

### MIME Type Helpers

```python
def is_video_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is video."""
    return mime_type.startswith("video/") if mime_type else False

def is_audio_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is audio."""
    return mime_type.startswith("audio/") if mime_type else False

def is_image_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is image."""
    return mime_type.startswith("image/") if mime_type else False

def is_pdf_mime_type(mime_type: Optional[str]) -> bool:
    """Check if MIME type is PDF."""
    return mime_type == "application/pdf" if mime_type else False
```

### Text Extraction Wrapper (v0.12.8)

```python
def extract_text_content(file_path: str, mime_type: Optional[str] = None) -> Optional[str]:
    """
    Extract text content from a file.
    Wrapper around extract_text() that returns just the text or None.
    """
    text, error = extract_text(file_path=file_path)
    if error:
        logger.warning(f"Text extraction error: {error}")
    return text if text else None
```

### Document Functions (v0.12.8 Signature Fixes)

```python
def detect_document_type(
    content_or_path: Optional[str] = None,  # Can be text OR path
    filename: Optional[str] = None,
    mime_type: Optional[str] = None,
) -> str:
    """Detect document type from content or filename."""

def generate_document_summary(
    raw_text: Optional[str],  # Pre-extracted text
    filename: str,
    doc_type: str,
    llm_call: Callable[[str], str],  # LLM callback
    max_length: int = 500,
) -> str:
    """Generate a summary of document content using LLM."""

def parse_cv_with_llm(
    raw_text: Optional[str],
    filename: str,
    llm_call: Callable[[str], str],
) -> dict:
    """Parse CV/resume content into structured data using LLM."""
```

---

## Semantic Search System

**Location:** `app/embeddings/`

### Architecture

```text
1. Content indexed (notes, messages, files)
2. OpenAI text-embedding-3-small generates 1536-dim vectors
3. Stored in embeddings table
4. Search: Query → Embedding → Cosine similarity
5. Return top-k matches
```

### Auto-Indexing (v0.15.0)

- **Notes:** Indexed on create/update
- **Messages:** Indexed after LLM response
- **Files:** Indexed after text extraction

**Chunking:** Content split into 500-token chunks for better granularity

### Search Endpoints

- `POST /search_notes` — Search notes by semantic similarity
- `POST /search_messages` — Search messages by semantic similarity

**Request:**
```json
{
  "query": "How do I set up authentication?",
  "project_id": 1,
  "top_k": 5
}
```

**Response:**
```json
{
  "results": [
    {
      "entity_type": "note",
      "entity_id": 42,
      "chunk_text": "Authentication uses password-based...",
      "similarity": 0.87
    }
  ]
}
```

---

## Error Taxonomy

**Location:** `app/providers/registry.py` (Phase 4)

### Error Types

| Error | Description | Recovery |
|-------|-------------|----------|
| `AUTHENTICATION_ERROR` | Invalid API key | Check `.env` file |
| `RATE_LIMIT_ERROR` | Too many requests | Wait and retry |
| `CONTEXT_LENGTH_ERROR` | Input too long | Truncate context |
| `INVALID_REQUEST_ERROR` | Malformed request | Fix request params |
| `TIMEOUT_ERROR` | API timeout | Retry with backoff |
| `PROVIDER_ERROR` | Provider-specific error | Check provider status |

---

## Quick Start

### Starting Orb

```powershell
cd D:\orb-desktop
npm run electron:dev
```

### Manual Backend (Development Only)

```powershell
# Get master key
cd D:\orb-desktop
node -e "require('keytar').getPassword('OrbMasterKey','default').then(k=>console.log(k))"

# Start backend
cd D:\Orb
$env:ORB_MASTER_KEY = "your-43-char-key"
.\.venv\Scripts\Activate.ps1
uvicorn main:app --host 127.0.0.1 --port 8000
```

### Installing Dependencies

```powershell
cd D:\Orb
.venv\Scripts\pip install pymupdf Pillow --break-system-packages
```

---

## Troubleshooting

### Router Misrouting / False Positives (v0.12.17)

**Enable debug mode to diagnose:**

1. Add to `.env`: `ORB_ROUTER_DEBUG=1`
2. Restart backend: `.venv\Scripts\python main.py`
3. Send test message
4. Check console for `[router-debug]` output
5. Verify:
   - Message text is clean (no filenames injected)
   - Keyword matches are accurate
   - Attachment metadata is separate
   - Final routing decision is correct

**Common issues:**

- **Filenames in message:** Fixed in v0.12.17 — classifier receives clean message
- **Keyword false positives:** Refine keyword lists in `job_classifier.py`
- **Attachment type misdetection:** Check MIME type detection in `file_analyzer.py`

### Video Analysis Not Working

1. Check Gemini API key is set:
   ```powershell
   type D:\Orb\.env | findstr "GOOGLE_API"
   ```

2. Check model name is valid:
   ```powershell
   type D:\Orb\.env | findstr "GEMINI_VISION_MODEL_COMPLEX"
   # Should be: gemini-2.5-pro (NOT gemini-1.5-pro or gemini-3-pro-preview)
   ```

3. Check logs for video detection:
   ```
   [chat_with_attachments] Detected VIDEO: filename.mp4 (19922944 bytes)
   [vision] Uploading video to Gemini File API...
   [vision] Waiting for video processing...
   [vision] Video ready, generating response...
   [vision] Initialized Gemini model: gemini-2.5-pro (tier=complex)
   ```

4. Check for quota errors (429):
   ```
   [vision] Video analysis failed: 429 Resource exhausted
   ```

### Model Badge Not Showing

1. Verify `model` column exists in messages table:
   ```powershell
   sqlite3 D:\Orb\data\orb_memory.db "PRAGMA table_info(messages);"
   ```

2. If missing, run migration:
   ```powershell
   python scripts/add_model_column.py
   ```

3. Check that `memory/schemas.py` `MessageCreate` has `model` field

### Routing Not Working

1. Check console for classification logs:
   ```
   [classify] job_classifier: big_architecture -> anthropic/claude-opus-4-5-20251101
   [chat] Job type: big_architecture
   [chat] Response from: anthropic / claude-opus-4-5-20251101
   ```

2. For streaming:
   ```
   [stream_router] Classified: SMALL_CODE
   [stream_chat] Job-type routing: small_code -> anthropic/claude-sonnet-4-5-20250929
   ```

3. Clear Python cache if code changes not taking effect:
   ```powershell
   Remove-Item -Recurse -Force D:\Orb\app\llm\__pycache__
   ```

### Claude Opus 404 Error

If you see `404 - model: claude-opus-4-5-20250514`, update your .env:

```env
# Wrong:
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20250514

# Correct:
ANTHROPIC_OPUS_MODEL=claude-opus-4-5-20251101
```

### PDF Extraction Error

If you see `No PDF library available`:

```powershell
cd D:\Orb
.venv\Scripts\pip install pymupdf
```

### Pillow/PIL Missing

If Gemini vision fails with PIL import error:

```powershell
cd D:\Orb
.venv\Scripts\pip install Pillow
```

### Master Key Issues

- Key must be exactly 43 characters
- Delete and regenerate: `cmdkey /delete:OrbMasterKey/default`
- Restart Electron

### Phase 4 Validation Errors

- Check `modalities_in` (not `modalities`)
- Check `needs_tools` is `list[str]` (not `False`)
- Check `JobBudget` field names

### Debug Mode Not Working (v0.12.17)

**Check `.env` file:**
```env
ORB_ROUTER_DEBUG=1  # Must be exactly this
```

**Restart backend:**
```powershell
# Stop backend (Ctrl+C)
cd D:\Orb
.venv\Scripts\python main.py
```

**Verify console output contains `[router-debug]` lines**

---

## Testing Routing

### Test Prompts (v0.14.1)

| Prompt | Expected Job Type | Expected Provider |
|--------|-------------------|-------------------|
| "What's a good recipe for pasta?" | CHAT_LIGHT | OpenAI GPT-4.1-mini |
| "Write a Python binary search function" | CODE_MEDIUM | Anthropic Sonnet |
| "Design a microservices architecture for e-commerce" | ORCHESTRATOR | Anthropic Opus |
| "Review this code for bugs: def divide(a,b): return a/b" | CODE_MEDIUM | Anthropic Sonnet |
| "Debug this small bug" | CODE_MEDIUM | Anthropic Sonnet |
| [Upload image] "What is this?" | IMAGE_COMPLEX | Gemini 2.5 Pro |
| [Upload image] (no message) | IMAGE_COMPLEX | Gemini 2.5 Pro (default prompt) |
| [Upload video] "Analyze this video" | VIDEO_HEAVY | Gemini 3.0 Pro |
| [Upload video + .py] "Debug this" | VIDEO_CODE_DEBUG | Gemini3 → Sonnet |
| [Upload .py + .jpg] "Fix this bug" | CODE_MEDIUM | Anthropic Sonnet (code wins!) |
| [Upload doc] "Simple Summary" | TEXT_HEAVY | OpenAI GPT |

### Console Output to Watch

**Without debug mode:**
```
[classify] job_classifier: orchestrator -> anthropic/claude-opus-4-5-20251101 (keyword: architecture)
[chat] Job type: orchestrator
[router] Classifier: orchestrator → anthropic / claude-opus-4-5-20251101 (keyword match)
```

**With debug mode (v0.14.1):**
```
[router-debug] ======================================================================
[router-debug] CLASSIFICATION START
[router-debug] Message (first 200 chars): 'Design a microservices...'
[router-debug] Total attachments: 0
[router-debug] Section 3: No user override detected
[router-debug] Section 5: CODE DETECTION - Found 0 code file(s)
[router-debug]   Architecture check: True (matched: ['microservices', 'architecture'])
[router-debug]   → Returning ORCHESTRATOR (architecture design)
[router-debug] CLASSIFICATION COMPLETE
[router-debug]   Job Type: orchestrator
[router-debug]   Provider: anthropic
[router-debug]   Model: claude-opus-4-5-20251101
```

**Video+Code Pipeline (v0.14.1):**
```
[router] VIDEO+CODE PIPELINE: video.code.debug → Gemini3 transcribe → Sonnet code
[video-code] Starting Video+Code debug pipeline
[video-code] Found 1 video(s), 1 code file(s)
[video-code] Step 1: Transcribing video: screen_recording.mp4
[video-transcribe] Transcript complete: 2847 chars
[video-code] Step 2: Calling Sonnet with code + transcripts
[video-code] Pipeline complete: 1523 chars
```

---

## Future Capabilities

### Planned Features

- [ ] Multi-turn conversations with memory
- [ ] Tool integration (web search, code execution, file system)
- [ ] Advanced RAG with query rewriting
- [ ] Voice input/output
- [ ] Mobile app (React Native)
- [ ] Cloud sync for projects
- [ ] Team collaboration features
- [ ] API rate limiting per user
- [ ] Cost tracking and budgeting
- [ ] Custom model fine-tuning

### Phase 4 Completion

- [ ] Migrate `/chat` to `/jobs/execute`
- [ ] Enable artefact storage for all responses
- [ ] Tool integration via Phase 4 framework
- [ ] Provider registry as single source of truth

---

---

## Changelog

### v0.14.2 (10 December 2025) — Critical Pipeline Specification Modules

**MAJOR FEATURE: Critical Pipeline Specification (CPS) Modules**

Implemented 7 new modules from the Critical Pipeline Specification for enhanced routing, auditing, and graceful degradation:

| Module | Purpose | Status |
|--------|---------|--------|
| `file_classifier.py` | File type classification + stable [FILE_X] naming | ✅ Integrated |
| `audit_logger.py` | Audit trail logging for pipeline transparency | ✅ Integrated |
| `relationship_detector.py` | Detect relationships between files | ✅ Available |
| `preprocessor.py` | Task preprocessing + context building | ✅ Available |
| `token_budgeting.py` | Token budget allocation + truncation | ✅ Available |
| `task_extractor.py` | Multi-task extraction from messages | ✅ Available |
| `fallbacks.py` | Fallback chains + graceful degradation | ✅ Integrated |

**KEY FEATURES:**

**1. Stable File Naming ([FILE_X])**
- All multimodal prompts now receive stable file identifiers
- File map injected into system message: `[FILE_1] video.mp4 (video, 15MB)`
- LLM responses can reference files consistently

**2. MIXED_FILE Detection**
- PDFs and DOCX with embedded images now detected
- Routes to Gemini 2.5 Pro (vision required) instead of GPT
- New FileType: `MIXED_FILE`

**3. Fallback Chains**
- Graceful degradation when models fail
- Chains: code (Sonnet→Opus→GPT), vision (Gemini2.5→Flash→GPT), video (Gemini3→2.5)
- Video transcription failures handled without blocking

**4. Audit Trail Logging**
- High-stakes pipeline logs all model calls
- Trace IDs for debugging pipeline issues
- Event types: TASK_RECEIVED, MODEL_CALL_START, FALLBACK_TRIGGERED, etc.

**NEW ENVIRONMENT VARIABLES:**
```bash
ORB_AUDIT_ENABLED=1           # Enable audit logging (default: 1)
ORB_FALLBACK_ENABLED=1        # Enable fallback behavior (default: 1)
ORB_MAX_FALLBACK_ATTEMPTS=3   # Max fallback attempts (default: 3)
```

**FILES ADDED (copy to app/llm/):**
- `file_classifier.py` (~400 lines)
- `audit_logger.py` (~350 lines)
- `relationship_detector.py` (~300 lines)
- `preprocessor.py` (~450 lines)
- `token_budgeting.py` (~400 lines)
- `task_extractor.py` (~350 lines)
- `fallbacks.py` (~520 lines)

**FILES UPDATED:**
- `app/llm/job_classifier.py` (v0.14.2)
  - Imports file_classifier with fallback
  - `compute_modality_flags()` returns file_map and classification_result
  - Section 7.5: MIXED_FILE detection → IMAGE_COMPLEX routing
  - New helper: `get_file_map_for_attachments()`
  
- `app/llm/router.py` (v0.14.2)
  - Imports all 7 CPS modules with try/except fallback
  - `inject_file_map_into_messages()` for stable file references
  - `synthesize_envelope_from_task()` accepts file_map parameter
  - `run_video_code_debug_pipeline()` uses fallback handler
  - `run_high_stakes_with_critique()` logs audit trace
  - `get_routing_info()` returns module availability flags

- `app/llm/__init__.py` (v0.14.2)
  - Exports all CPS module functions and classes
  - Graceful fallback if modules unavailable

**BACKWARD COMPATIBILITY:**
✅ All modules optional - graceful degradation if unavailable
✅ Existing routing unchanged when modules not present
✅ No changes required to main.py or frontend
✅ Environment variables have sensible defaults

**TESTING:**
- PDF with images → Routes to Gemini 2.5 Pro (MIXED_FILE)
- Video + code → Pipeline with fallback handling
- High-stakes job → Audit trace logged
- Set ORB_ROUTER_DEBUG=1 → File map generation visible

**FUTURE WORK (modules available but not yet wired):**
- Relationship detection for routing decisions
- Multi-task extraction for parallel processing
- Preprocessing for context optimization
- Token budgeting for large inputs

### v0.14.1 (10 December 2025) — Unified Multimodal Routing + Video+Code Pipeline

**MAJOR FEATURE: Video+Code Debug Pipeline**
- New 2-step pipeline for debugging code with video screen recordings
- Step 1: Gemini 3 Pro transcribes video (timeline, errors, UI actions)
- Step 2: Sonnet analyzes code with video transcript context
- Trigger: `has_video AND has_code` attachments
- New job type: `VIDEO_CODE_DEBUG`

**SIMPLIFIED MULTIMODAL ROUTING:**
- ALL videos → VIDEO_HEAVY (Gemini 3.0 Pro) — no more Flash for videos
- ALL images → IMAGE_COMPLEX (Gemini 2.5 Pro) — no more Flash for images
- Code + images → CODE_MEDIUM/ORCHESTRATOR — code wins over images!
- IMAGE_SIMPLE (Flash) retained for legacy/manual override only

**MODALITY PRIORITY ORDER:**
```
1. Video+Code → VIDEO_CODE_DEBUG (Gemini3 → Sonnet)
2. Video only → VIDEO_HEAVY (Gemini 3)
3. Code (with or without images) → CODE_MEDIUM/ORCHESTRATOR
4. Images only → IMAGE_COMPLEX (Gemini 2.5)
5. Text → normal routing
```

**HIGH-STAKES VIDEO PRE-STEP:**
- Architecture/security jobs with video attachments now transcribe videos first
- Transcripts injected into Opus draft context
- Critique pipeline continues normally after pre-step

**NEW HELPER FUNCTIONS:**
- `compute_modality_flags(attachments)` — Central modality detection
- `transcribe_video_for_context(video_path, prompt)` — Gemini 3 Pro transcription
- `run_video_code_debug_pipeline(task, envelope)` — Pipeline orchestration

**Files Changed:**
- `app/llm/schemas.py` — Added VIDEO_CODE_DEBUG job type, fixed LLMTask.attachments type
- `app/llm/job_classifier.py` — compute_modality_flags(), simplified routing, code > images priority
- `app/llm/router.py` — Video+Code pipeline, high-stakes video pre-step
- `app/llm/gemini_vision.py` — transcribe_video_for_context() function

**Testing:**
- Video + .py → VIDEO_CODE_DEBUG pipeline triggers
- Single video → Gemini 3 Pro (not Flash)
- Single image → Gemini 2.5 Pro (not Flash)
- Code + image → Sonnet (code wins over images)
- Architecture + video → Opus with video transcripts

### v0.13.10 (09 December 2025) — High-Stakes Pipeline Job Type Fix

**BUG FIX:**
- Fixed high-stakes critique pipeline not triggering for architecture jobs
- **Root Cause:** `HIGH_STAKES_JOBS` set was missing several job types that should trigger critique
- **Impact:** Architecture design requests were being processed without the Gemini critique step

**Solution:**
Added missing job types to `HIGH_STAKES_JOBS` set in `router.py`:
```python
HIGH_STAKES_JOBS = {
    "architecture_design",
    "big_architecture", 
    "high_stakes_infra",
    "security_review",
    "privacy_sensitive_change",
    "security_sensitive_change",
    "complex_code_change",
    "implementation_plan",
    "spec_review",
    "architecture",
    "deep_planning",
    "orchestrator",
}
```

**Files Changed:**
- `app/llm/router.py` — Added missing job types to HIGH_STAKES_JOBS

### v0.13.9 (09 December 2025) — Minor Fixes

**Bug Fixes:**
- Minor logging improvements
- Code cleanup in job_classifier.py

### v0.13.8 (09 December 2025) — Override Detection False Positive Fix

**CRITICAL BUG FIX:**
- Fixed override detection triggering on ANY mention of model names in text, causing architecture planning prompts to incorrectly route to vision models
- **Root Cause:** Substring matching (`"use gemini" in message_lower`) matched documentation, examples, and technical specifications
- **Impact:** Architecture design requests with text like "Multi-image uploads route to Gemini 2.5 Pro Vision" were misclassified as user override commands, routing to Gemini Flash instead of Opus for architecture design

**Test Case That Failed (v0.13.7):**
```
Message: "Design high-stakes architecture... Multi-image uploads route to Gemini 2.5 Pro..."
Attachment: orb_architecture_map.md (text file)

Wrong Behavior:
  Job Type: image.simple
  Provider: google (Gemini Flash)
  Reason: "User requested Gemini explicitly"
  User Override: YES ❌

Expected Behavior:
  Job Type: orchestrator → architecture_design
  Provider: anthropic (Opus)
  User Override: NO ✓
```

**Solution: Strict Regex-Based Override Detection**

Replaced loose substring matching with strict regex patterns requiring commands at line start:

```python
# OLD (Broken)
if "use gemini" in message_lower:
    return override  # Matches anywhere

# NEW (Fixed)
if re.search(r'(?:^|\n)\s*USE\s+GEMINI\b', message, re.IGNORECASE):
    return override  # Only at line start
```

**Override Patterns:**
- `(?:^|\n)\s*OVERRIDE\s+(?:SEND\s+TO|USE)\s+<MODEL>\b`
- `(?:^|\n)\s*(?:FORCE|USE)\s+<MODEL>\b`

**Examples:**
- ✓ Triggers: `"OVERRIDE SEND TO GEMINI"`, `"Force Opus"`, `"USE GPT"` (at line start)
- ✗ No trigger: `"Multi-image uploads use Gemini..."`, `"The system uses Gemini for video"`, `"Example: \`USE GEMINI\`"`

**Files Changed:**
- `app/llm/job_classifier.py` (v0.13.8)
  - Line 175: Pass original `message` (not lowercased) to override detection
  - Lines 466-518: Replaced `_detect_user_override()` with strict regex version
  - Added comprehensive docstring with examples
  - Added debug logging: `[router-debug] Section 3: No user override detected`

**Result:**
✅ Architecture prompts with model mentions route correctly to Opus
✅ Explicit override commands still work when intended
✅ Documentation and examples don't trigger overrides
✅ False positives eliminated

**Testing:**
- Architecture planning + .md file → Routes to Opus (not Gemini)
- "OVERRIDE SEND TO GEMINI" command → Routes to Gemini (correct)
- Model mentions in text → No override detected (correct)

### v0.13.7 (09 December 2025) — Environment-Aware Architecture Critique

**CRITICAL FIX:**
- Fixed high-stakes critique pipeline over-engineering architecture designs with enterprise infrastructure (Kubernetes, VLANs, Docker orchestration) instead of respecting single-host, solo-developer deployment reality
- **Root Cause:** Generic critique prompt with no environment context → Gemini defaulted to maximal theoretical hardening assumptions
- **Impact:** Architecture designs were not implementable for solo developer on single workstation

**Test Case That Failed (v0.13.6):**
```
Input: Architecture design prompt + file upload

Opus Draft: Pragmatic single-host design
Gemini Critique: "Add Kubernetes, separate VLANs, external CI, egress proxies"
Opus Revision: Incorporates enterprise recommendations ❌

Result: Over-engineered, not implementable
```

**Solution 1: Separate Critique Prompts by Job Type**

Three specialized critique templates based on normalized job_type:

**Architecture Critique Template:**
- HARD-CODES environment constraints: single-host, local repos, solo dev, limited time/budget
- Explicitly forbids: Kubernetes, Docker orchestration, VLANs, external CI, separate VMs
- Prefers: File permissions, process isolation, Windows security features
- Highest priority: "OVER-ENGINEERING CHECK"
- Requires separation of "Future Hardening (Optional)" from core design

**Security Critique Template:**
- Maintains aggressive hardening focus (unchanged)
- Allowed to recommend enterprise infra if security-justified
- Focus: Threat modeling, attack surface analysis, defense in depth

**General Critique Template:**
- Fallback for other high-stakes types
- Focus: Correctness, completeness, risks

**Solution 2: Environment Context Injection**

```python
# v0.13.7: Get hard-coded deployment constraints
env_context = get_environment_context()  # Returns constraints dict

# Pass to architecture critique
critique_prompt = build_critique_prompt(
    draft_text=draft_result.content,
    original_request=original_request,
    job_type_str="architecture_design",
    env_context=env_context  # ← NEW
)
```

**Environment Context Dict:**
```python
{
    "deployment_type": "single_host",
    "os": "Windows 11",
    "repos": ["D:\\Orb\\", "D:\\SandboxOrb\\"],
    "team_size": "solo_developer",
    "constraints": {
        "no_kubernetes": True,
        "no_docker_orchestration": True,
        "no_multi_host": True,
        # ... 7 total
    },
    "forbidden_unless_explicit": [
        "Kubernetes", "Docker Swarm", "VLANs",
        "External CI", "Separate VMs", # ... 6 total
    ]
}
```

**Solution 3: stream_router Security Classification Fix**

Removed generic keywords from security classification to prevent false positives:

**Removed:**
- `"high stakes"`, `"high-stakes"`, `"high risk"`, `"high-risk"`, `"critical"`

**Kept (concrete security terms only):**
- `"security review"`, `"security audit"`, `"threat model"`, `"vulnerability"`

**Impact:**
- Architecture prompts with "high-stakes" no longer misclassify as SECURITY_REVIEW
- Only explicit security-focused terms trigger security classification

**Files Changed:**
- `app/llm/router.py` (v0.13.7)
  - Lines 120-190: Added `get_environment_context()` - Returns deployment constraints
  - Lines 220-380: Added three specialized critique prompt templates
  - Lines 625-645: Modified `call_gemini_critic()` to pass environment context
  - Comprehensive docstrings with examples
- `app/llm/stream_router.py` (v0.13.7)
  - Lines 240-270: Removed generic keywords from security classification
  - Added classification logging (first 200 chars of message)
  - Refined keyword lists for concrete security terms only

**Expected Behavior After Fix:**

| Scenario | v0.13.6 (Broken) | v0.13.7 (Fixed) |
|----------|------------------|-----------------|
| Architecture Design | Recommends K8s, VLANs, Docker | File permissions, process isolation, Windows security |
| Security Review | Aggressive hardening | Aggressive hardening (unchanged) |
| Classification | "High-stakes" → SECURITY_REVIEW | "High-stakes" → ARCHITECTURE_DESIGN |

**Logs:**
```
[critic] Passing environment context to architecture critique: single_host, solo_developer
[critic] Calling Gemini 3 Pro for critique of architecture_design task
[stream_router] Classifying message (first 200 chars): 'Design a high-stakes...'
[stream_router] Classified: ARCHITECTURE_DESIGN
```

**Result:**
✅ Architecture designs pragmatic and implementable for solo dev
✅ Security reviews maintain aggressive hardening
✅ No enterprise infra drift (K8s, VLANs) on architecture jobs
✅ Classification no longer over-triggers on generic "high-stakes" keywords

### v0.13.4 (07 December 2025) — Critique Pipeline Streaming Integration

**Features:**
- Integrated high-stakes critique pipeline into `/stream/chat` endpoint
- Full 3-step pipeline (Opus draft → Gemini critique → Opus revision) now works in streaming
- "Fake streaming" of final result (~80 char chunks) for UX consistency
- Imports critique functions from router.py (single source of truth)

**Implementation:**
- Added `generate_high_stakes_critique_stream()` in stream_router.py
- Added `chunk_text()` function for fake streaming
- Pipeline runs completely (blocking), then streams final answer
- No intermediate stages visible to frontend

**Files Changed:**
- `app/llm/stream_router.py` (v0.13.4, 848 lines)
  - Imported critique pipeline from router.py
  - Added streaming wrapper for high-stakes jobs
  - Fixed LLMTask schema (proper fields: job_type, messages, system_prompt)

**User Experience:**
- 45-60s silence while pipeline runs
- Then final revised answer streams normally
- Only final answer saved to database

### v0.13.6 (08 December 2025) — High-Stakes Job Type Normalization Fix

**CRITICAL BUG FIX:**
- Fixed high-stakes critique pipeline not triggering for architecture design requests with file uploads
- **Root Cause:** Classifier returned generic job type `"orchestrator"` for architecture work, but `is_high_stakes_job("orchestrator")` returned False because "orchestrator" wasn't in `HIGH_STAKES_JOB_TYPES` set
- **Impact:** Architecture design requests correctly routed to Anthropic Opus but received only initial draft instead of revised answer after critique pipeline
- **Observed:** Response in ~10s (single call) instead of 30-60s (3-call pipeline), no `[critic]` logs

**Solution: Job Type Normalization**
- Added `normalize_job_type_for_high_stakes()` function in router.py
- Maps generic "orchestrator" to specific high-stakes types based on classification reason keywords:
  - Architecture keywords → "architecture_design"
  - Security keywords → "security_review"
  - Infrastructure keywords → "high_stakes_infra"
  - Default: "architecture_design"
- Updated `HIGH_STAKES_JOB_TYPES` set to include "orchestrator"
- Modified `call_llm_async()` to normalize job type before checking if high-stakes

**Normalization Logic:**
```python
def normalize_job_type_for_high_stakes(job_type_str: str, reason: str = "") -> str:
    # Pass through if already specific
    if job_type_str in HIGH_STAKES_JOB_TYPES and job_type_str != "orchestrator":
        return job_type_str
    
    # Normalize orchestrator based on reason keywords
    if job_type_str == "orchestrator":
        reason_lower = reason.lower()
        if any(kw in reason_lower for kw in ["architecture", "system design", "architect"]):
            return "architecture_design"
        # ... other mappings
    
    return job_type_str
```

**Expected Logs (Fixed):**
```
[router] Normalized orchestrator → architecture_design (reason: Document with architecture design...)
[router] HIGH-STAKES PIPELINE: orchestrator → architecture_design
[critic] Starting high-stakes pipeline: architecture_design
[critic] Step 1: Generating Opus draft...
[critic] Opus draft complete: 15243 chars
[critic] Step 2: Generating Gemini critique...
[critic] Gemini critique complete: 2134 chars
[critic] Step 3: Generating Opus revision...
[critic] Opus revision complete: 16891 chars
[critic] High-stakes pipeline SUCCESS
```

**Files Changed:**
- `app/llm/router.py` (v0.13.6)
  - Lines ~140: Added "orchestrator" to HIGH_STAKES_JOB_TYPES
  - Lines ~145-195: Added normalize_job_type_for_high_stakes() function
  - Lines ~1100-1130: Modified call_llm_async() to normalize before high-stakes check
  - Lines ~1400-1430: Updated exports

**Backward Compatibility:**
✅ All existing functionality preserved
✅ No changes to main.py required
✅ Document content injection (v0.13.5) still works
✅ Anthropic system message fix (v0.13.5) still works
✅ Simple chats unaffected (not high-stakes)
✅ Code requests work normally
✅ 8-route classification unchanged

**Testing:**
- Architecture design + file upload → Triggers critique pipeline (30-60s delay)
- Logs show normalization: orchestrator → architecture_design
- Logs show [critic] entries for 3-step pipeline
- Final response is revised (not initial draft)

### v0.13.5 (07 December 2025) — Document Content Injection + Anthropic API Fix

**CRITICAL BUG FIX #1: Document Content Not Passed to LLM**
- Fixed uploaded files being extracted but not passed to LLM context
- **Root Cause:** `main.py` extracted document text into `attachments_as_text` list but never injected it into `full_context` sent to LLM
- **Impact:** LLMs received filenames but not file contents, causing hallucinated responses
- **Fix:** Added `document_content_parts` in main.py to build actual file text sections, injected into `full_context`

**Before (BROKEN):**
```python
full_context = base_context  # Only had base context, no file content
```

**After (FIXED):**
```python
# Build document content parts
document_content_parts = []
for att in attachments_as_text:
    doc_part = f"\n\n--- Document: {att['filename']} ---\n{att['text']}\n--- End Document ---"
    document_content_parts.append(doc_part)

# Inject into full context
full_context = base_context
if document_content_parts:
    full_context += "\n".join(document_content_parts)
```

**CRITICAL BUG FIX #2: Anthropic API 400 Error**
- Fixed Anthropic API rejecting requests with system messages in messages array
- **Root Cause:** System prompts were embedded in messages array, but Anthropic API requires separate top-level `system` parameter
- **Impact:** All Anthropic requests with system prompts failed with 400 Bad Request
- **Fix:** Modified `registry.py` to extract system messages from messages array and pass via top-level `system` parameter

**Before (BROKEN):**
```python
response = client.messages.create(
    messages=[{"role": "system", "content": "..."}, {"role": "user", "content": "..."}]  # INVALID
)
```

**After (FIXED):**
```python
# Extract system message
system_content = None
filtered_messages = []
for msg in messages:
    if msg["role"] == "system":
        system_content = msg["content"]
    else:
        filtered_messages.append(msg)

# Pass system separately
response = client.messages.create(
    system=system_content,  # Top-level parameter
    messages=filtered_messages  # No system messages in array
)
```

**Files Changed:**
- `main.py` (v0.13.5)
  - Lines ~800-820: Added document_content_parts logic
  - Lines ~830: Injected document content into full_context
- `app/llm/registry.py` (v0.13.5)
  - Lines ~200-250: Added system message extraction for Anthropic
  - Lines ~260: Pass system parameter separately

**Result:**
✅ File uploads now provide actual text to LLM
✅ Anthropic API calls work with system prompts
✅ No more 400 errors
✅ Architecture design requests work end-to-end

### v0.13.3 (07 December 2025) — Security Review Classification Fix

**Bug Fixes:**
- Fixed security reviews misclassifying as CODE_REVIEW instead of SECURITY_REVIEW
- Moved security keywords OUT of generic `review_keywords` list
- Added dedicated `security_keywords` list (19 keywords)

**Security Keywords Added:**
```
"security review", "security audit", "vulnerability", "exploit",
"sql injection", "xss", "csrf", "session fixation",
"privilege escalation", "authentication bypass",
"high stakes", "high-stakes", "penetration test", "threat model"
```

**Priority Classification:**
1. Security (PRIORITY 1) → SECURITY_REVIEW
2. Architecture → ARCHITECTURE_DESIGN
3. General Review → CODE_REVIEW

**Files Changed:**
- `app/llm/stream_router.py` (v0.13.3, 574 lines)

### v0.13.2.2 (07 December 2025) — Critique Pipeline Logging Fix

**Bug Fixes:**
- Added `print()` statements for all `[critic]` logs
- Python logging wasn't configured for console output
- Now uses both `print()` and `logger` for maximum visibility

**Changes:**
- All 9 pipeline stages now print to console
- Logs visible regardless of logging configuration
- `logger` still captures for files/production

**Files Changed:**
- `app/llm/router.py` (v0.13.2.2, 1,232 lines)

### v0.13.2.1 (07 December 2025) — Anthropic API Compatibility Fix

**Bug Fixes:**
- Fixed `call_opus_revision()` to not use `{"role": "system"}` in messages
- Anthropic API rejects system role in messages array
- System prompts must be passed via separate `system` parameter (different from OpenAI/Gemini)

**Implementation:**
- Moved all revision instructions into user message content
- Removed system message from revision_messages array
- Fixed 400 "Unexpected role 'system'" error

**Files Changed:**
- `app/llm/router.py` (v0.13.2.1, 1,223 lines)

### v0.13.2 (07 December 2025) — Initial Critique Pipeline (Non-Streaming)

**Major Feature:**
- Implemented critique pipeline in `/chat` endpoint (router.py)
- 3-step flow: Opus draft → Gemini 3 Pro critique → Opus revision
- High-stakes job detection and routing

**Files Changed:**
- `app/llm/router.py` (v0.13.2)

**Note:** Streaming integration came later in v0.13.4

### v0.12.18 (07 December 2025) — High-Stakes Critique Pipeline

**MAJOR FEATURE: 3-Step Critique Pipeline for High-Stakes Opus Outputs**

**Summary:**
Implemented automatic quality assurance system for critical work. When Claude Opus is used for high-stakes tasks (architecture design, security reviews, etc.), the system automatically:
1. Generates initial draft (Opus)
2. Requests independent critique (Gemini 3 Pro)
3. Produces revised answer incorporating critique (Opus)

User receives only the final revised answer.

**Trigger Conditions:**
- Provider: Anthropic (not OpenAI/Google)
- Model: Opus (not Sonnet)
- Job Type: Fine-grained string in HIGH_STAKES_JOB_TYPES set
  - `architecture_design`, `big_architecture`, `security_review`, `privacy_sensitive_change`,
    `complex_code_change`, `implementation_plan`, `spec_review`, `high_stakes_infra`,
    `architecture`, `deep_planning`
- Response Length: >= 1500 chars (~250 tokens)

**Key Implementation Details:**
- **CRITICAL:** Uses fine-grained job type STRING (e.g., `"security_review"`), NOT the coarse 5-type enum (`JobType.ORCHESTRATOR`)
- Reason: Enum collapses many meanings; cannot distinguish `security_review` from `casual_chat`
- `"orchestrator"` intentionally excluded from high-stakes set (too generic)
- Graceful degradation: Returns original Opus draft on any failure
- Cost controlled: Token limits on critic (1024) and revision (2048)
- Observable: All actions logged with `[critic]` prefix

**Files Changed:**
- `app/llm/router.py` (lines 90-803)
  - Added `HIGH_STAKES_JOB_TYPES` set (11 specific job types)
  - Added critique configuration constants (MIN_CRITIQUE_CHARS, token limits)
  - Added `call_gemini_critic()` async function
  - Added `call_opus_revision()` async function
  - Added `run_high_stakes_with_critique()` async function
  - Added critique pipeline check in `call_llm_async()`

**Environment Variables Added:**
- `GEMINI_OPUS_CRITIC_MODEL=gemini-3-pro` (Gemini critic model)
- `ORB_MIN_CRITIQUE_CHARS=1500` (minimum length to trigger critique)
- `GEMINI_CRITIC_MAX_TOKENS=1024` (max tokens for critique call)
- `OPUS_REVISION_MAX_TOKENS=2048` (max tokens for revision call)

**Logging:**
All critique actions use `[critic]` prefix:
- `[critic] High-stakes pipeline enabled: job_type=X model=Y`
- `[critic] Calling Gemini 3 Pro for critique of {job_type} task`
- `[critic] Gemini 3 Pro critique completed: X chars`
- `[critic] Calling Opus for revision using Gemini critique`
- `[critic] Opus revision complete: X chars`
- `[critic] Gemini critic failed; returning original Opus draft` (on failure)
- `[critic] Opus revision failed; returning original Opus draft` (on failure)

**Error Handling:**
Every failure point returns original Opus draft to user:
- Gemini critique fails → Return draft
- Gemini returns empty → Return draft
- Opus revision fails → Return draft
- Revision returns empty → Return draft

No user is ever blocked from receiving a response.

**Cost Impact:**
- Normal Opus call: ~$0.045
- With critique: ~$0.122 (2.7x overhead)
- Only applies to ~10% of requests (high-stakes only)
- Monthly overhead estimate: +$25-50 for typical usage

**Testing:**
5 verification test cases:
1. Simple summary → No critique (not high-stakes)
2. Architecture design → Critique triggers
3. Security review → Critique triggers
4. Short Opus response → No critique (too short)
5. Sonnet code → No critique (not Opus)

**Backward Compatibility:**
✅ Zero changes to classification logic
✅ Zero changes to job types or routing decisions
✅ Zero changes to main.py, job_classifier.py, schemas.py
✅ Zero changes to frontend
✅ Can be disabled by rolling back router.py

**Documentation:**
- `CRITIQUE_PIPELINE_DOCUMENTATION.md` (1,000+ lines)
- `CRITIQUE_PIPELINE_DEPLOYMENT_GUIDE.md` (500+ lines)
- `CRITIQUE_PIPELINE_SUMMARY.md` (quick reference)

**Status:** ✅ IMPLEMENTED, READY FOR DEPLOYMENT


### v0.12.17 (07 December 2025) — Message Injection Bug Fix + Debug Mode

**CRITICAL BUG FIX:**
- Fixed filename metadata being injected into user message for classification
- **Root Cause:** `main.py` lines 678-683 concatenated filenames like `[Uploaded: architecture_map.md]` into message text
- **Impact:** Filenames containing keywords (e.g., "architect") caused false positive routing
- **Example:** "Simple Summary" + file "architecture_map.md" incorrectly routed to Opus instead of GPT mini
- **Fix (3-part surgical change in main.py):**
  - Line 425: Initialize `attachment_metadata: List[dict] = []`
  - Line 700: Populate metadata dict in file upload loop
  - Line 744: Remove filename injection, keep message clean
  - Lines 770-773: Move file info to context (for LLM) instead of message (for classifier)
  - Line 790: Pass `attachment_metadata` to LLMTask separately

**Before (BROKEN):**
```python
full_message = f"{user_message}\n\n[User uploaded:]\n[Uploaded: architecture_map.md]..."
classify_message(full_message, None)  # Classifier sees filename → False positive
```

**After (FIXED):**
```python
full_message = user_message  # Clean message only
attachment_metadata = [{"filename": "architecture_map.md", ...}]
classify_message(full_message, attachment_metadata)  # Classifier sees metadata separately
```

**NEW FEATURE: Router Debug Mode (v0.12.17)**
- Added `ORB_ROUTER_DEBUG=1` environment flag for comprehensive routing visibility
- Debug logging in `job_classifier.py` and `router.py`
- Logs: message text, attachment metadata, keyword matches, routing decisions
- Example output format with section-by-section classification breakdown
- Performance impact: ~5-10ms per request (minimal)
- Zero behavioral change when disabled

**Emergency Patch:**
- Added missing `attachment_metadata` initialization (line 425)

**Files Changed:**
- `main.py` — Lines 425, 700, 744, 770-773, 790 (message injection fix)
- `job_classifier.py` — Debug logging instrumentation
- `router.py` — Debug logging instrumentation
- `.env.example` — Added ORB_ROUTER_DEBUG documentation

### v0.12.16 (06 December 2025) — Classifier Heuristics Fix

**Bug Fixes:**
- Fixed legacy classification function interfering with new classifier
- Router now respects job_classifier.py decisions without re-classification
- Removed `_classify_job_type()` from router.py (duplicate logic)
- Fixed attachment metadata being ignored in classification

### v0.12.15 (06 December 2025) — Attachment Metadata Passing

**Features:**
- Router now passes attachment metadata to classifier
- Classifier receives structured attachment info (filename, MIME, size)
- Fixed attachment detection not triggering vision routing

### v0.12.14 (06 December 2025) — Pre-Classification Respect

**Bug Fixes:**
- Router now respects pre-classified job_type from main.py
- Removed fallback that was overriding correct classifications
- Fixed double classification in `/chat_with_attachments`

### v0.12.13 (06 December 2025) — Environment Variable Override Fix

**Bug Fixes:**
- Fixed `OPENAI_MINI_MODEL` env var overriding code defaults
- Router now uses code defaults unless env var explicitly set
- Fixed GPT mini not being used for CHAT_LIGHT jobs

### v0.12.12 (06 December 2025) — Policy-Based Routing

**Features:**
- Implemented policy-based routing with `routing_policy.json`
- Supports override rules for specific job types
- Backward compatible with hard-coded routing

### v0.12.11 (06 December 2025) — Legacy Classifier Removal

**Bug Fixes:**
- Removed legacy `_classify_job_type()` from main.py
- All classification now uses `job_classifier.py`
- Fixed conflicts between old and new classification logic

### v0.12.10 (06 December 2025) — Provider Override Logic

**Features:**
- Added provider override detection
- Fixed routing respecting pre-classification

### v0.12.9 (06 December 2025) — Dead Model Name Fix

**Bug Fixes:**
- Fixed dead model names in routing tables
- Updated `gpt-4.1-mini` references (was `gpt-4o-mini`)
- Updated `claude-opus-4-5-20251101` (was `20250514`)
- Fixed model names in `job_classifier.py` and `router.py`

**Files Changed:**
- `app/llm/job_classifier.py` — Model name constants
- `app/llm/router.py` — Routing table definitions
- `.env.example` — Model name documentation

### v0.12.8 (06 December 2025) — Routing Refinements

**Features:**
- Added video deep-analysis semantic detection for small videos (<10MB)
- Added `VIDEO_DEEP_ANALYSIS_KEYWORDS` (18 keywords) in job_classifier.py
- Added `needs_deep_video_analysis()` function for semantic keyword matching
- Added `is_claude_allowed()` function to check allowed job types for Claude
- Added `_check_attachment_safety()` in router.py to prevent silent Claude fallback
- Media uploads now work without user message (uses default prompt)

**Model Updates:**
- Fixed Claude Opus model name: `claude-opus-4-5-20251101` (was incorrectly 20250514)
- Added `GEMINI_VIDEO_DEEP_MODEL` env var (default: gemini-3.0-pro-preview)

**Bug Fixes:**
- Fixed media routing when no user message provided (now uses default prompt)
- Fixed `extract_text_content` wrapper function returning tuple instead of string
- Fixed `detect_document_type` function signature mismatch
- Fixed `generate_document_summary` and `parse_cv_with_llm` function signatures
- Added defensive `isinstance(raw_text, str)` checks to prevent AttributeError
- Added MIME helper functions: `is_video_mime_type()`, `is_audio_mime_type()`, `is_image_mime_type()`, `is_pdf_mime_type()`

**Hard Rules Added:**
- Attachments without job_type never silently fall back to Claude → Route to GPT
- Only `code.medium` and `orchestrator` job types allowed for Claude with attachments

**Dependencies:**
- Added PyMuPDF (fitz) for PDF text extraction
- Added Pillow for Gemini vision image processing

### v0.12.7 (06 December 2025) — Video Support + Job Classifier

**Features:**
- Added `analyze_video()` function for video analysis via Gemini File API
- Added `is_video_mime_type()` and `is_audio_mime_type()` helpers
- Added `is_binary_file()` detection to skip text extraction for media files
- Created `app/llm/job_classifier.py` module for automatic job classification
- Integrated job_classifier into router.py `classify_and_route()`
- Updated main.py `_classify_job_type()` to use job_classifier
- Video routing with tiered model selection (>10MB → complex model)
- Binary files (video/audio/images) skip text extraction, route to vision

**Model Updates:**
- Default complex model: `gemini-2.5-pro` (gemini-1.5-pro deprecated April 2025)
- Default fast model: `gemini-2.0-flash`
- Added env var: `GEMINI_VISION_MODEL_COMPLEX`

**Bug Fixes:**
- Fixed video files being read as text (produced 50000 chars of garbage)
- Fixed routing to wrong provider when video detected

**Architecture Doc Updates (v21):**
- Reconciled v19 and v20 architecture maps
- Restored Key Endpoints table
- Restored Multi-LLM Orchestration provider roles table
- Restored Provider Registry capabilities detail
- Added Legacy Job Types & Migration section
- Documented LEGACY_TO_PRIMARY mapping

### v0.12.4 (05 December 2025) — Streaming Reasoning + Image Routing

**Features:**
- Streaming now extracts THINKING/ANSWER tags for reasoning panel
- Separate reasoning capture and display logic
- Image routing fixes for tiered model selection

**Bug Fixes:**
- Fixed image analysis not routing to correct vision model
- Fixed reasoning panel not populating during stream

### v0.12.3 (05 December 2025) — Attachments Endpoint Fix

**Bug Fixes:**
- Fixed /chat_with_attachments returning empty response
- Fixed file record creation for uploaded attachments

### v0.12.2 (04 December 2025) — Unified Routing

**Features:**
- Added automatic job-type classification from message content
- Both `/chat` and `/stream/chat` now use same routing logic
- Code/architecture/review prompts automatically route to Anthropic
- Casual chat routes to OpenAI
- Added `_classify_job_type()` function to `main.py` and `stream_router.py`
- Added `_select_provider_for_job_type()` function to `stream_router.py`

### v0.12.1 (03 December 2025) — Model Badge Fix

**Bug Fixes:**
- Added `model` column to Message ORM model
- `/chat` endpoint now returns and saves `model` field
- `/chat_with_attachments` endpoint now returns and saves `model` field
- Created migration script `scripts/add_model_column.py`

### v0.17.0 (02 December 2025) — Security Level 4

- Master key stored in Windows Credential Manager
- Password used only for authentication
- Electron manages master key lifecycle

### v0.16.0 (01 December 2025)

- Reasoning panel for chain-of-thought
- Model badges
- Markdown rendering

### v0.15.0 (30 November 2025)

- Semantic search with embeddings
- Auto-indexing


---

## Testing Checklist

After updates, verify:

- [ ] `model` column exists in messages table
- [ ] `/chat` returns both `provider` and `model`
- [ ] Messages saved to DB include `model` column
- [ ] Model badge displays in UI
- [ ] Non-stream chat routes code prompts to Anthropic
- [ ] Non-stream chat routes casual prompts to OpenAI
- [ ] Streaming chat routes code prompts to Anthropic
- [ ] Streaming chat routes casual prompts to OpenAI
- [ ] Binary files skip text extraction
- [ ] Console shows classification logs with job_classifier
- [ ] Attachments without job_type don't silently fall back to Claude
- [ ] Phase 4 endpoints work (if enabled)
- [ ] PDF extraction works (PyMuPDF installed)
- [ ] Claude Opus model name is correct (20251101)
- [ ] **Router debug mode works (ORB_ROUTER_DEBUG=1)** (v0.12.17)
- [ ] **Filenames not injected into message text** (v0.12.17)
- [ ] **Attachment metadata passed separately to classifier** (v0.12.17)
- [ ] **Debug output shows clean messages and separate attachment info** (v0.12.17)

**Multimodal Routing (v0.14.1 — SIMPLIFIED):**

| Test | Expected Routing | Expected Model |
|------|------------------|----------------|
| Text-only: "Summarize" + .txt | CHAT_LIGHT/TEXT_HEAVY | GPT |
| Single image: "Analyze" + .jpg | IMAGE_COMPLEX | Gemini 2.5 Pro |
| Single video: "Analyze" + .mp4 | VIDEO_HEAVY | Gemini 3.0 Pro |
| Video+Code: "Debug" + .mp4 + .py | VIDEO_CODE_DEBUG | Gemini3 → Sonnet |
| Code+Image: "Fix bug" + .py + .jpg | CODE_MEDIUM | Sonnet (code wins!) |
| Multiple images: + .png + .jpg | IMAGE_COMPLEX | Gemini 2.5 Pro |
| Architecture + video | High-stakes + video pre-step | Opus (with transcripts) |

- [ ] ALL videos → VIDEO_HEAVY (Gemini 3.0 Pro) — no more Flash
- [ ] ALL images → IMAGE_COMPLEX (Gemini 2.5 Pro) — no more Flash
- [ ] Code + images → CODE_MEDIUM (Sonnet) — code wins over images
- [ ] Video + code → VIDEO_CODE_DEBUG pipeline triggers
- [ ] Image upload without message uses default prompt

**Video+Code Debug Pipeline (v0.14.1):**
- [ ] Video + .py file → Triggers VIDEO_CODE_DEBUG
- [ ] Logs show: `[router] VIDEO+CODE PIPELINE: video.code.debug → Gemini3 transcribe → Sonnet code`
- [ ] Logs show: `[video-code] Starting Video+Code debug pipeline`
- [ ] Logs show: `[video-code] Step 1: Transcribing video: {filename}`
- [ ] Logs show: `[video-transcribe] Transcript complete: X chars`
- [ ] Logs show: `[video-code] Step 2: Calling Sonnet with code + transcripts`
- [ ] Final response contains code analysis based on video context

**High-Stakes Video Pre-Step (v0.14.1):**
- [ ] Architecture design + video attachment → Video transcribed first
- [ ] Logs show: `[critic] Pre-step: Transcribing X video(s) for high-stakes context`
- [ ] Opus draft receives video transcripts as system context
- [ ] Critique pipeline continues normally after pre-step

**High-Stakes Critique Pipeline (v0.12.18 - v0.13.4):**

**Non-Streaming (/chat endpoint):**
- [ ] Simple summary + doc attachment → No critique (not high-stakes)
- [ ] Architecture design request → Opus + critique pipeline triggers
- [ ] Security review request → Opus + critique pipeline triggers
- [ ] Short Opus response (<1500 chars) → No critique (length threshold)
- [ ] Sonnet code task → No critique (not Opus)
- [ ] Critique failure → User receives draft (graceful degradation)
- [ ] Logs show `[critic]` prefix for all critique actions
- [ ] Debug mode shows full critique pipeline execution
- [ ] High-stakes job types checked via fine-grained STRING (not enum)
- [ ] `"orchestrator"` normalizes to specific high-stakes types (v0.13.6)
- [ ] Environment variables loaded: GEMINI_OPUS_CRITIC_MODEL, ORB_MIN_CRITIQUE_CHARS, etc.

**Streaming (/stream/chat endpoint) - v0.13.4:**
- [ ] Security review in streaming → Classified as SECURITY_REVIEW (v0.13.3)
- [ ] Security review → Routes through critique pipeline
- [ ] Console shows: `[stream] High-stakes detected: routing through critique pipeline`
- [ ] Console shows all 9 [critic] log stages (v0.13.2.2)
- [ ] 45-60s delay before response starts streaming (pipeline running)
- [ ] Final answer streams normally after pipeline completes
- [ ] Only final revised answer saved to database (not draft/critique)
- [ ] "SECURITY REVIEW REQUEST" → Triggers critique (not CODE_REVIEW)
- [ ] Security keywords match: "security review", "vulnerability", "sql injection", etc.

**Logging Visibility (v0.13.2.2):**
- [ ] `print()` statements visible in console for all [critic] logs
- [ ] Logs show: "High-stakes pipeline enabled"
- [ ] Logs show: "Opus draft complete: X chars"
- [ ] Logs show: "Calling Gemini 3 Pro for critique"
- [ ] Logs show: "Gemini 3 Pro critique completed: X chars"
- [ ] Logs show: "Calling Opus for revision"
- [ ] Logs show: "Opus revision complete: X chars"
- [ ] Logs visible regardless of Python logging configuration

**Document Content Injection (v0.13.5):**
- [ ] File upload + message → LLM receives actual file text (not just filename)
- [ ] Check logs: `full_context` contains `--- Document: filename.ext ---` sections
- [ ] Document text properly extracted and injected into context
- [ ] Multiple file uploads → All file contents present in context
- [ ] Architecture design + file → LLM can reference file content in response

**Anthropic API Fix (v0.13.5):**
- [ ] Anthropic requests with system prompts → No 400 errors
- [ ] System message extracted from messages array
- [ ] System message passed as top-level `system` parameter
- [ ] Response from Anthropic succeeds with system prompts

**Job Type Normalization (v0.13.6):**
- [ ] Architecture design + file upload → Routes to Opus
- [ ] Classifier returns `job_type="orchestrator"` with reason containing "architecture"
- [ ] Normalization: orchestrator → architecture_design (logs show this)
- [ ] High-stakes check: `is_high_stakes_job("architecture_design")` → True
- [ ] Critique pipeline triggers (30-60s delay, [critic] logs)
- [ ] Final response is revised (not initial draft)
- [ ] Logs show: `[router] Normalized orchestrator → architecture_design`
- [ ] Logs show: `[router] HIGH-STAKES PIPELINE: orchestrator → architecture_design`
- [ ] Security keywords → Normalizes to "security_review"
- [ ] Infrastructure keywords → Normalizes to "high_stakes_infra"
- [ ] Default orchestrator → Normalizes to "architecture_design"

**Gemini 3 Pro Configuration:**
- [ ] Model: `gemini-3-pro-preview` (requires -preview suffix)
- [ ] Google API key has billing enabled
- [ ] No "model not found" errors in logs

**Environment-Aware Architecture Critique (v0.13.7):**
- [ ] Architecture design job → Uses architecture critique template (not generic)
- [ ] Logs show: `[critic] Passing environment context to architecture critique: single_host, solo_developer`
- [ ] Architecture critique response: Pragmatic, single-host design
- [ ] Architecture critique response: Uses file permissions, process isolation, Windows security
- [ ] Architecture critique response: Does NOT recommend K8s, VLANs, Docker orchestration, external CI
- [ ] If enterprise infra mentioned → Clearly marked as "Future Hardening (Optional)" section
- [ ] Security review job → Uses security critique template (aggressive hardening, unchanged)
- [ ] Security review → May recommend enterprise infra if security-justified (correct behavior)
- [ ] Over-engineering: Architecture critique explicitly flags K8s/VLANs as over-engineered if not requested
- [ ] Environment context dict passed to `build_critique_prompt()` for architecture jobs

**stream_router Security Classification Fix (v0.13.7):**
- [ ] "High-stakes architecture design" → Classified as ARCHITECTURE_DESIGN (not SECURITY_REVIEW)
- [ ] "Critical architecture planning" → Classified as ARCHITECTURE_DESIGN (not SECURITY_REVIEW)
- [ ] "Security review of auth system" → Classified as SECURITY_REVIEW (correct)
- [ ] Logs show classification: `[stream_router] Classifying message (first 200 chars): ...`
- [ ] Logs show: `[stream_router] Classified: ARCHITECTURE_DESIGN` (for architecture jobs)
- [ ] Generic importance terms ("high-stakes", "critical") don't trigger SECURITY_REVIEW
- [ ] Only explicit security terms trigger SECURITY_REVIEW classification

**Override Detection Fix (v0.13.8):**
- [ ] Architecture prompt + "gemini" mentions → No override detected, routes to Opus
- [ ] Message: "Multi-image uploads route to Gemini 2.5 Pro..." → No override, routes based on content
- [ ] Message: "The system uses Gemini for video" → No override, routes based on content
- [ ] Message: "Add overrides like \`OVERRIDE SEND TO GEMINI\`" → No override (example in backticks)
- [ ] Explicit command: "OVERRIDE SEND TO GEMINI" → Override detected, routes to Gemini
- [ ] Explicit command: "Force Opus" → Override detected, routes to Opus
- [ ] Explicit command: "USE GPT" → Override detected, routes to GPT
- [ ] Logs show: `[router-debug] Section 3: No user override detected` (when no override)
- [ ] Logs show: `[router-debug] Section 3: USER OVERRIDE DETECTED - ...` (when override present)
- [ ] Architecture planning + .md file → Routes to orchestrator → architecture_design → Opus
- [ ] Documentation/examples with model names → No false positives

**Critical Pipeline Specification Modules (v0.14.2):**

**File Classification + Stable Naming:**
- [ ] Upload PDF with embedded images → Routes to IMAGE_COMPLEX (MIXED_FILE detected)
- [ ] Upload multiple files → File map generated with [FILE_1], [FILE_2], etc.
- [ ] Logs show: `[router-debug] File map: === FILE REFERENCE MAP ===`
- [ ] System prompt contains file reference map
- [ ] LLM response can reference files using [FILE_X] identifiers

**MIXED_FILE Routing:**

| Test | Expected Classification | Expected Model |
|------|------------------------|----------------|
| PDF (text only) | TEXT_HEAVY | GPT |
| PDF (with charts/images) | MIXED_FILE → IMAGE_COMPLEX | Gemini 2.5 Pro |
| DOCX (text only) | TEXT_HEAVY | GPT |
| DOCX (with images) | MIXED_FILE → IMAGE_COMPLEX | Gemini 2.5 Pro |

- [ ] pdf_image_count > 0 → MIXED_FILE classification
- [ ] Logs show: `[router-debug] Section 7.5: MIXED_FILE detected`
- [ ] Routes to Gemini 2.5 Pro (vision required)

**Fallback Handling:**
- [ ] Video transcription failure → Continues with code files only (if present)
- [ ] Model unavailable → Falls back to next in chain
- [ ] Logs show: `[fallback] VIDEO_TRANSCRIPTION_FAILED, action: DEGRADE_GRACEFULLY`
- [ ] Logs show: `[fallback] MODEL_UNAVAILABLE, trying next: {fallback_model}`
- [ ] FallbackResult.fallback_used = True when fallback occurred

**Audit Logging:**
- [ ] High-stakes job → Audit trace created
- [ ] Each model call logged with trace_id
- [ ] Logs show: `[audit] trace_id={id} event=MODEL_CALL_START`
- [ ] Logs show: `[audit] trace_id={id} event=PIPELINE_COMPLETE`
- [ ] ORB_AUDIT_ENABLED=0 → No audit logs

**Module Availability:**
- [ ] Missing file_classifier.py → FILE_CLASSIFIER_AVAILABLE=False, routing works
- [ ] Missing audit_logger.py → AUDIT_AVAILABLE=False, pipeline works
- [ ] Missing fallbacks.py → FALLBACKS_AVAILABLE=False, errors not caught gracefully
- [ ] get_routing_info() returns correct availability flags

**Environment Variables:**
- [ ] ORB_AUDIT_ENABLED=1 → Audit logging enabled
- [ ] ORB_FALLBACK_ENABLED=1 → Fallback behavior enabled
- [ ] ORB_MAX_FALLBACK_ATTEMPTS=3 → Max 3 attempts before giving up
- [ ] Variables have sensible defaults when not set

---

*Document maintained as part of Orb development. Update with each architectural change.*
