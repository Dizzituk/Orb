# Orb Architecture Map

**Version:** 0.11.0 (Architecture Map v9)  
**Last Updated:** 30 November 2025  
**Purpose:** Canonical architecture reference for the Orb multi-LLM AI assistant system

---

## Table of Contents

1. [System Overview](#system-overview)  
2. [Component Architecture](#component-architecture)  
3. [Backend (`Orb`)](#backend-orb)  
4. [Desktop Client (`orb-desktop`)](#desktop-client-orb-desktop)  
5. [Data Layer](#data-layer)  
6. [Authentication System](#authentication-system)  
7. [Database Encryption](#database-encryption)  
8. [Multi-LLM Orchestration](#multi-llm-orchestration)  
9. [DateTime Context](#datetime-context)  
10. [Streaming Responses](#streaming-responses)  
11. [Web Search](#web-search)  
12. [File Ingestion Pipeline](#file-ingestion-pipeline)  
13. [Image Analysis Pipeline](#image-analysis-pipeline)  
14. [Semantic Search System](#semantic-search-system)  
15. [Future Capabilities](#future-capabilities)  
16. [Changelog](#changelog)

---

## System Overview

Orb is a personal AI assistant built as a multi-component system with specialized LLM roles:

- **GPT (OpenAI)** â€” Fast/lightweight reasoning, conversational interface, linguistics, embeddings  
- **Claude (Anthropic)** â€” Flagship engineer, complex code, architecture design  
- **Gemini (Google)** â€” Critic, reviewer, analyst, vision specialist, web search  

### Core Vision

Different AI models specialize in distinct roles while sharing a unified memory layer, enabling:

- Persistent knowledge management across conversations  
- Task coordination and project organization  
- Job-type-based automatic model routing  
- File upload with text extraction and document analysis  
- CV parsing with structured data extraction  
- Image analysis via Gemini Vision  
- **Semantic search (RAG) with vector embeddings**
- Password-based authentication with session tokens
- Project selector for multi-project management
- Database encryption at rest
- Streaming LLM responses
- Web search with real-time grounding
- Automatic datetime context in all LLM calls

### Technology Stack

**Backend:**

- FastAPI (Python 3.13)  
- SQLAlchemy ORM  
- SQLite database  
- Pydantic schemas  
- bcrypt for password hashing  
- cryptography (Fernet) for field-level encryption  
- `python-multipart` for `multipart/form-data` file uploads  
- `python-docx` for DOCX text extraction  
- `PyMuPDF` (fitz) for PDF text extraction  
- `google-genai` for Gemini Web Search (new SDK, preferred)  
- `google-generativeai` for Gemini Vision + fallback (old SDK)  
- **OpenAI `text-embedding-3-small` for semantic embeddings**

**Desktop Client:**

- Electron (v33.0.0)  
- React 18 with TypeScript 5  
- Vite 5 (build tool and dev server)  
- Custom React hooks for state management  

**Platform:** Windows 11

---

## Component Architecture

```text
D:/
â”œâ”€â”€ Orb/                        # Backend (FastAPI server)
â”‚   â”œâ”€â”€ main.py                 # Application entrypoint, chat endpoints, RAG
â”‚   â”œâ”€â”€ .env                    # API keys (OPENAI, ANTHROPIC, GOOGLE)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ db.py               # Database configuration
â”‚   â”‚   â”œâ”€â”€ auth/               # Authentication module (password-based)
â”‚   â”‚   â”œâ”€â”€ crypto/             # Encryption module
â”‚   â”‚   â”œâ”€â”€ memory/             # Memory subsystem (projects/notes/tasks/files/messages)
â”‚   â”‚   â”œâ”€â”€ llm/                # LLM routing + vision + streaming + web search
â”‚   â”‚   â””â”€â”€ embeddings/         # Semantic search with vector embeddings
â”‚   â”œâ”€â”€ data/                   # SQLite DB + file storage + auth config
â”‚   â”‚   â”œâ”€â”€ orb_memory.db       # SQLite database (encrypted fields + embeddings)
â”‚   â”‚   â”œâ”€â”€ auth.json           # Password hash + session storage
â”‚   â”‚   â”œâ”€â”€ encryption_salt.bin # Salt for encryption key derivation
â”‚   â”‚   â””â”€â”€ files/              # Per-project file storage
â”‚   â”‚       â””â”€â”€ {project_id}/   # Uploaded project files
â”‚   â””â”€â”€ scripts/
â”‚       â””â”€â”€ encrypt_existing_data.py  # Migration script for encryption
â”‚
â”œâ”€â”€ orb-desktop/                # Electron + React desktop client
â”‚   â”œâ”€â”€ main.js                 # Electron main process
â”‚   â”œâ”€â”€ index.html              # React mount point + CSP config
â”‚   â”œâ”€â”€ package.json            # Dependencies + scripts
â”‚   â”œâ”€â”€ vite.config.ts          # Vite configuration
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ main.tsx            # React entry point
â”‚       â”œâ”€â”€ App.tsx             # Root component with auth + chat modes
â”‚       â”œâ”€â”€ components/         # React components
â”‚       â”‚   â”œâ”€â”€ AuthScreen.tsx  # Password setup/login UI
â”‚       â”‚   â”œâ”€â”€ ProjectSelector.tsx # Project dropdown
â”‚       â”‚   â”œâ”€â”€ Header.tsx      # App header with mode toggle
â”‚       â”‚   â”œâ”€â”€ ChatWindow.tsx  # Chat messages with streaming
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ hooks/              # Custom React hooks
â”‚       â”‚   â”œâ”€â”€ useAuth.ts      # Password authentication
â”‚       â”‚   â”œâ”€â”€ useProjects.ts  # Project management
â”‚       â”‚   â”œâ”€â”€ useChat.ts      # Chat state + streaming + web search
â”‚       â”‚   â””â”€â”€ ...
â”‚       â”œâ”€â”€ services/           # API client layer
â”‚       â””â”€â”€ styles/             # CSS styles
â”‚
â””â”€â”€ orb-electron-data/          # Electron userData directory
```

---

## Backend (`Orb`)

**Location:** `D:/Orb/`  
**Framework:** FastAPI  
**Entry Point:** `main.py`  
**Version:** 0.11.0

### Directory Structure

```text
Orb/
â”œâ”€â”€ main.py                 # FastAPI app, endpoints, context building, semantic RAG
â”œâ”€â”€ .env                    # API keys and configuration
â”œâ”€â”€ start_orb_backend.bat   # Windows startup script
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db.py               # Database engine, session factory, Base
â”‚   â”‚
â”‚   â”œâ”€â”€ auth/               # Authentication module
â”‚   â”‚   â”œâ”€â”€ __init__.py     # Exports: require_auth, login, setup_password, etc.
â”‚   â”‚   â”œâ”€â”€ config.py       # Password hashing, session tokens, encryption init
â”‚   â”‚   â”œâ”€â”€ middleware.py   # FastAPI auth dependency
â”‚   â”‚   â””â”€â”€ router.py       # Auth endpoints (/auth/*)
â”‚   â”‚
â”‚   â”œâ”€â”€ crypto/             # Encryption module
â”‚   â”‚   â”œâ”€â”€ __init__.py     # Exports: encrypt_string, decrypt_string, etc.
â”‚   â”‚   â”œâ”€â”€ encryption.py   # Fernet encryption with PBKDF2 key derivation
â”‚   â”‚   â””â”€â”€ types.py        # SQLAlchemy EncryptedText, EncryptedJSON types
â”‚   â”‚
â”‚   â”œâ”€â”€ memory/             # Memory subsystem
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ models.py       # SQLAlchemy ORM models (with encrypted columns)
â”‚   â”‚   â”œâ”€â”€ schemas.py      # Pydantic request/response schemas
â”‚   â”‚   â”œâ”€â”€ service.py      # Business logic (CRUD operations)
â”‚   â”‚   â””â”€â”€ router.py       # FastAPI route handlers (protected)
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                # LLM routing module
â”‚   â”‚   â”œâ”€â”€ __init__.py     # Exports: call_llm, analyze_image, etc.
â”‚   â”‚   â”œâ”€â”€ schemas.py      # JobType enum, LLMTask/LLMResult models
â”‚   â”‚   â”œâ”€â”€ clients.py      # Provider API wrappers + datetime context
â”‚   â”‚   â”œâ”€â”€ router.py       # Main router with call_llm()
â”‚   â”‚   â”œâ”€â”€ gemini_vision.py # Gemini Vision for image analysis
â”‚   â”‚   â”œâ”€â”€ file_analyzer.py # Text extraction and document analysis
â”‚   â”‚   â”œâ”€â”€ streaming.py    # Streaming LLM responses + datetime context
â”‚   â”‚   â”œâ”€â”€ stream_router.py # SSE streaming endpoint
â”‚   â”‚   â”œâ”€â”€ web_search.py   # Gemini web grounding + datetime context
â”‚   â”‚   â””â”€â”€ web_search_router.py # Web search endpoint
â”‚   â”‚
â”‚   â””â”€â”€ embeddings/         # Semantic search module (NEW)
â”‚       â”œâ”€â”€ __init__.py     # Exports: generate_embedding, search_embeddings, etc.
â”‚       â”œâ”€â”€ models.py       # Embedding SQLAlchemy model
â”‚       â”œâ”€â”€ schemas.py      # Search request/response schemas
â”‚       â”œâ”€â”€ service.py      # Embedding generation, storage, similarity search
â”‚       â””â”€â”€ router.py       # /embeddings/* and /memory/search endpoints
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ orb_memory.db       # SQLite database (encrypted fields + embeddings table)
â”‚   â”œâ”€â”€ auth.json           # Password hash + session config
â”‚   â”œâ”€â”€ encryption_salt.bin # Salt for key derivation
â”‚   â”œâ”€â”€ orb_memory_backup_pre_encryption.db  # Pre-encryption backup
â”‚   â””â”€â”€ files/              # Uploaded project files
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ encrypt_existing_data.py  # Encrypt existing DB data
â”‚
â””â”€â”€ static/                 # Web-based test interface
```

### Environment Configuration (`.env`)

```env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...
GEMINI_VISION_MODEL=gemini-2.0-flash
```

### Core Endpoints

| Endpoint | Method | Auth | Description |
|----------|--------|------|-------------|
| `/` | GET | No | Static UI |
| `/ping` | GET | No | Health check |
| `/auth/status` | GET | No | Check auth type and status |
| `/auth/setup` | POST | No | Set up password (first time) |
| `/auth/login` | POST | No | Login with password |
| `/auth/logout` | POST | Yes | Logout (invalidate session) |
| `/auth/check` | GET | Yes | Verify session validity |
| `/auth/change-password` | POST | Yes | Change password |
| `/chat` | POST | Yes | Main chat with semantic RAG |
| `/chat_with_attachments` | POST | Yes | File upload with analysis + auto-indexing |
| `/stream/chat` | POST | Yes | Streaming chat (SSE) |
| `/stream/providers` | GET | Yes | Available streaming providers |
| `/search/status` | GET | Yes | Web search availability |
| `/search/query` | POST | Yes | Web search with grounding |
| `/llm` | POST | Yes | Direct LLM call |
| `/providers` | GET | Yes | Available providers |
| `/memory/*` | * | Yes | All memory CRUD |
| `/memory/search` | POST | Yes | **Semantic search** |
| `/embeddings/index` | POST | Yes | **Index new content** |
| `/embeddings/reindex` | POST | Yes | **Re-index all content** |
| `/embeddings/status/{project_id}` | GET | Yes | **Embedding statistics** |
| `/embeddings/index/note/{note_id}` | POST | Yes | **Index single note** |
| `/embeddings/index/file/{file_id}` | POST | Yes | **Index single file** |

---

## Authentication System

### Overview

Orb uses **password-based authentication** with session tokens. Passwords are hashed with bcrypt and stored locally. Session tokens are generated on login and stored in localStorage.

### Token Formats

- **Session tokens:** `orb_session_<64 hex chars>` (current)
- **Legacy API keys:** `orb_<32 hex chars>` (migration support)

### Auth Flow

1. Backend checks `data/auth.json` for `password_hash`
2. Frontend checks `/auth/status` â†’ returns `{configured, auth_type}`
3. If not configured â†’ setup screen (create password)
4. If configured â†’ login screen (enter password)
5. POST `/auth/login` â†’ returns `session_token`
6. Token stored in localStorage (key: `orb_session_token`)
7. All protected requests include `Authorization: Bearer <token>`

### Auth Files

```text
data/
â”œâ”€â”€ auth.json              # Password hash + session data
â””â”€â”€ encryption_salt.bin    # Salt for encryption key derivation
```

---

## Database Encryption

### Overview

Orb uses **field-level encryption** with Fernet (symmetric AES-128-CBC). The encryption key is derived from the user's password using PBKDF2 with 480,000 iterations.

### Encrypted Fields

| Table | Field | Encrypted |
|-------|-------|-----------|
| notes | content | âœ… |
| tasks | description | âœ… |
| messages | content | âœ… |
| files | description | âœ… |
| document_contents | raw_text | âœ… |
| document_contents | summary | âœ… |
| document_contents | structured_data | âœ… |
| embeddings | content | âœ… |
| embeddings | embedding | âŒ (vectors are meaningless without context) |

### Encryption Flow

1. User logs in with password
2. `set_encryption_key(password)` derives key via PBKDF2
3. Key stored in memory (global `_encryption_manager`)
4. SQLAlchemy `EncryptedText` type auto-encrypts on write, decrypts on read
5. On logout, `clear_encryption_key()` wipes key from memory

---

## Multi-LLM Orchestration

### Job Types

| Job Type | Primary Provider | Reviewed |
|----------|-----------------|----------|
| `casual_chat` | GPT | No |
| `quick_question` | GPT | No |
| `linguistics` | GPT | No |
| `bug_fix` | GPT â†’ Claude | No |
| `code_review` | Claude | No |
| `architecture` | Claude | Yes (Gemini) |
| `complex_code` | Claude | Yes (Gemini) |
| `analysis` | Gemini | No |
| `critique` | Gemini | No |

### Routing Logic

```python
def call_llm(task: LLMTask) -> LLMResult:
    job_type = task.job_type
    
    if job_type in GPT_ONLY_JOBS:
        return call_openai(...)
    elif job_type in CLAUDE_PRIMARY_JOBS:
        result = call_anthropic(...)
        if job_type in HIGH_STAKES_JOBS:
            review = call_gemini(critique_prompt)
            result.critic_review = review
        return result
    elif job_type in GEMINI_JOBS:
        return call_gemini(...)
```

---

## DateTime Context

All LLM calls include current date/time in the system prompt:

```
Current date and time: Saturday, November 30, 2025 at 6:45 PM (GMT)
```

Implemented in:
- `app/llm/clients.py` â€” `enhance_system_prompt()`
- `app/llm/streaming.py` â€” streaming responses
- `app/llm/web_search.py` â€” web search queries

---

## Streaming Responses

### SSE Protocol

```
event: metadata
data: {"provider": "openai", "model": "gpt-4o-mini"}

event: token
data: {"content": "Hello"}

event: token
data: {"content": " world"}

event: done
data: {"total_tokens": 25}
```

### Endpoint

```
POST /stream/chat
Content-Type: application/json
Authorization: Bearer <token>

{
  "project_id": 1,
  "message": "Hello",
  "provider": "openai"
}
```

---

## Web Search

### Overview

Web search uses Gemini with Google Search grounding via the `google-genai` SDK.

### Request

```
POST /search/query
{
  "query": "What is the current Bitcoin price?",
  "project_id": 1
}
```

### Response

```json
{
  "answer": "The current Bitcoin price is approximately $97,000...",
  "sources": [
    {"title": "CoinGecko", "url": "https://..."},
    {"title": "CoinMarketCap", "url": "https://..."}
  ],
  "grounding_used": true
}
```

---

## File Ingestion Pipeline

```text
POST /chat_with_attachments
       â”‚
       â”œâ”€â”€ Validate project exists
       â”‚
       â”œâ”€â”€ For each file:
       â”‚     â”‚
       â”‚     â”œâ”€â”€ Save to data/files/{project_id}/{uuid}.ext
       â”‚     â”‚
       â”‚     â”œâ”€â”€ Extract text (PyMuPDF for PDF, python-docx for DOCX)
       â”‚     â”‚
       â”‚     â”œâ”€â”€ Detect document type (cv, report, notes, etc.)
       â”‚     â”‚
       â”‚     â”œâ”€â”€ If CV â†’ parse_cv_with_llm() â†’ structured JSON
       â”‚     â”‚
       â”‚     â”œâ”€â”€ Generate summary via LLM
       â”‚     â”‚
       â”‚     â”œâ”€â”€ Create File + DocumentContent records (encrypted)
       â”‚     â”‚
       â”‚     â””â”€â”€ Auto-index embeddings for semantic search
       â”‚
       â””â”€â”€ Return ChatResponse with attachment summaries
```

---

## Image Analysis Pipeline

```text
POST /chat_with_attachments (with image)
       â”‚
       â”œâ”€â”€ Validate project exists
       â”‚
       â”œâ”€â”€ Save to data/files/{project_id}/{uuid}.png
       â”‚
       â”œâ”€â”€ Detect image/* MIME type
       â”‚
       â”œâ”€â”€ analyze_image(file_bytes, mime_type, user_prompt)
       â”‚         â”‚
       â”‚         â”œâ”€â”€ Initialize Gemini model
       â”‚         â”œâ”€â”€ Send image + prompt to Gemini Vision
       â”‚         â””â”€â”€ Parse JSON response: {summary, tags, type}
       â”‚
       â”œâ”€â”€ Create File + DocumentContent records (encrypted)
       â”‚
       â”œâ”€â”€ Auto-index embeddings for semantic search
       â”‚
       â””â”€â”€ Return ChatResponse with image summary
```

---

## Semantic Search System

### Overview

Orb uses **vector embeddings** for semantic search (RAG). Content is embedded using OpenAI's `text-embedding-3-small` model (1536 dimensions) and stored in SQLite as JSON arrays. Cosine similarity is computed in Python for retrieval.

### Architecture

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INDEXING FLOW                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Note/Message/File                                          â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  chunk_text() â”€â”€â–º Split into ~400 token chunks              â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  generate_embedding() â”€â”€â–º OpenAI text-embedding-3-small     â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  store_embedding() â”€â”€â–º SQLite embeddings table              â”‚
â”‚        â”‚              (content encrypted, vector as JSON)   â”‚
â”‚        â–¼                                                    â”‚
â”‚  Ready for search                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SEARCH FLOW                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  User Query                                                 â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  generate_embedding() â”€â”€â–º Embed query text                  â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  Load all project embeddings from SQLite                    â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  cosine_similarity() â”€â”€â–º Compute similarity scores          â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  Sort by score, return top_k results                        â”‚
â”‚        â”‚                                                    â”‚
â”‚        â–¼                                                    â”‚
â”‚  Inject into LLM context as "RELEVANT CONTEXT"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Database Schema

```sql
CREATE TABLE embeddings (
    id INTEGER PRIMARY KEY,
    project_id INTEGER NOT NULL,
    source_type VARCHAR(20) NOT NULL,  -- "note", "message", "file"
    source_id INTEGER NOT NULL,
    chunk_index INTEGER DEFAULT 0,
    content TEXT NOT NULL,              -- Encrypted source text
    embedding TEXT NOT NULL,            -- JSON array of floats
    created_at DATETIME
);

CREATE INDEX ix_embeddings_project_source 
ON embeddings (project_id, source_type, source_id);
```

### Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/memory/search` | POST | Semantic search across project content |
| `/embeddings/index` | POST | Index new (unindexed) content |
| `/embeddings/reindex` | POST | Delete and recreate all embeddings |
| `/embeddings/status/{project_id}` | GET | Get embedding counts by type |
| `/embeddings/index/note/{note_id}` | POST | Index single note |
| `/embeddings/index/file/{file_id}` | POST | Index single file |

### Search Request

```json
POST /memory/search
{
  "project_id": 1,
  "query": "What programming skills does the candidate have?",
  "top_k": 5,
  "source_types": ["file", "note"]
}
```

### Search Response

```json
{
  "query": "What programming skills does the candidate have?",
  "results": [
    {
      "source_type": "file",
      "source_id": 3,
      "chunk_index": 0,
      "content": "Skills: Python, JavaScript, React, Node.js...",
      "similarity": 0.8734
    },
    {
      "source_type": "note",
      "source_id": 12,
      "chunk_index": 0,
      "content": "Candidate has 5 years of Python experience...",
      "similarity": 0.7821
    }
  ],
  "total_searched": 47
}
```

### Chat Integration

The `/chat` endpoint automatically uses semantic search when `use_semantic_search: true` (default):

1. User sends message
2. `build_semantic_context()` searches embeddings for relevant content
3. Top results injected into system prompt as "RELEVANT CONTEXT"
4. LLM generates response with awareness of retrieved content
5. Falls back to keyword-based search if no embeddings exist

### Auto-Indexing

Files uploaded via `/chat_with_attachments` are automatically indexed after storage:

```python
# After storing DocumentContent
embeddings_service.index_document(db, doc, force=True)
```

---

## Future Capabilities

### Security Roadmap

- âœ… **Phase 1: Password Authentication** â€” Completed
- âœ… **Phase 2: Database Encryption** â€” Completed (field-level)
- **Phase 3: Secure Key Storage** â€” Electron secure storage / Windows Credential Manager
- **Phase 4: Audit Logging** â€” Track all API access with timestamps

### Feature Roadmap

- âœ… **Streaming Responses** â€” Completed
- âœ… **Web Search Grounding** â€” Completed (with google-genai SDK)
- âœ… **DateTime Context** â€” Completed (all LLM calls)
- âœ… **Semantic Search Over Notes & Files** â€” Completed (embedding-based RAG)
- **Markdown Rendering** â€” Render code blocks, bold, lists
- **Chat History Load** â€” Load previous messages on project switch
- **Settings Panel** â€” Configure providers, models, API keys in UI
- **Theme Toggle** â€” Light/dark mode switch
- **Background Indexing** â€” Auto-index on note/message creation

---

## Changelog

### v0.11.0 â€” Semantic Search System (30 Nov 2025)

**Backend:**

- **Semantic Search Module (`app/embeddings/`):**
  - New `Embedding` SQLAlchemy model with project/source indexing
  - OpenAI `text-embedding-3-small` integration (1536 dimensions)
  - Text chunking with ~400 token chunks and 50 token overlap
  - Cosine similarity search in Python (no vector DB required)
  - Embeddings stored as JSON arrays in SQLite
  - Content field encrypted, vectors stored plaintext

- **New Endpoints:**
  - `POST /memory/search` â€” Semantic search with similarity scores
  - `POST /embeddings/index` â€” Index new content only
  - `POST /embeddings/reindex` â€” Re-index all project content
  - `GET /embeddings/status/{project_id}` â€” Embedding statistics
  - `POST /embeddings/index/note/{note_id}` â€” Index single note
  - `POST /embeddings/index/file/{file_id}` â€” Index single file

- **Chat Integration:**
  - `/chat` now uses semantic search by default (`use_semantic_search: true`)
  - `build_semantic_context()` retrieves top-k relevant chunks
  - Falls back to keyword-based search if no embeddings exist
  - Context injected as "RELEVANT CONTEXT" in system prompt

- **Auto-Indexing:**
  - Files uploaded via `/chat_with_attachments` auto-indexed after storage
  - Embeddings created immediately for new documents

- **Database:**
  - New `embeddings` table with composite index
  - `init_db()` updated to include embeddings model

**Files Added:**
- `app/embeddings/__init__.py`
- `app/embeddings/models.py`
- `app/embeddings/schemas.py`
- `app/embeddings/service.py`
- `app/embeddings/router.py`

**Files Modified:**
- `app/db.py` â€” Import embeddings models
- `main.py` â€” Add embeddings router, semantic context building

### v0.10.1 â€” DateTime Context + Web Search SDK Fix (30 Nov 2025)

**Backend:**

- **DateTime Context:**
  - All LLM calls now include current date/time in system prompt
  - `get_datetime_context()` and `enhance_system_prompt()` helpers
  - Applied to: `clients.py`, `streaming.py`, `web_search.py`
  - Format: "Current date and time: Saturday, November 30, 2025 at 6:45 PM (GMT)"

- **Web Search SDK Migration:**
  - New SDK: `google-genai` (preferred, full grounding support)
  - Old SDK: `google-generativeai` (fallback, no grounding)
  - Tool format: `Tool(google_search=GoogleSearch())` instead of deprecated `google_search_retrieval`
  - Automatic fallback if new SDK not installed

- **clients.py Fixes:**
  - Fixed usage dict format (removed model string from usage)
  - Added `call_google` alias for backwards compatibility

**Dependencies:**

- Added: `google-genai` (optional, for full web search grounding)

### v0.10.0 â€” Database Encryption + Streaming + Web Search (30 Nov 2025)

**Backend:**

- **Database Encryption:**
  - New `app/crypto/` module with Fernet encryption
  - PBKDF2 key derivation from password (480,000 iterations)
  - `EncryptedText` and `EncryptedJSON` SQLAlchemy column types
  - Encrypted: messages, notes, tasks, document content
  - Migration script: `scripts/encrypt_existing_data.py`
  - Salt stored in `data/encryption_salt.bin`
  - Encryption key initialized on login, cleared on logout

- **Streaming Responses:**
  - New `app/llm/streaming.py` module
  - Server-Sent Events (SSE) endpoint: `/stream/chat`
  - Multi-provider support: OpenAI, Anthropic, Gemini
  - Metadata, token, done, and error event types

- **Web Search:**
  - New `app/llm/web_search.py` module
  - Gemini grounding with Google Search
  - Endpoint: `/search/query`
  - Source extraction and citation

**Frontend:**

- **Chat Modes:**
  - Mode toggle button in header (ğŸ’¬ Chat / âš¡ Stream / ğŸ” Search)
  - Streaming content display with blinking cursor
  - Cancel stream button during generation
  - Web search with `ğŸ”` prefix on queries

- **useChat Hook Updates:**
  - `sendMessageStreaming()` for streaming responses
  - `sendWebSearch()` for grounded search
  - `isStreaming` and `streamingContent` state
  - `cancelStream()` to abort generation

- **ChatWindow Updates:**
  - Streaming message display
  - Auto-scroll during streaming
  - Input disabled while streaming

**Dependencies:**

- Added: `cryptography` (Fernet encryption)
- Updated: `google-generativeai` (for grounding)

### v0.9.0 â€” Password Authentication + Project Selector (30 Nov 2025)

- Password-based authentication with bcrypt
- Session token system
- Project selector component
- UI fixes (scroll, drag-drop)

### v0.8.0 â€” API Authentication (30 Nov 2025)

- Initial API key authentication system

### v0.7.0 â€” React Migration + File Ingestion (29-30 Nov 2025)

- React 18 + TypeScript 5 migration
- File upload with text extraction
- CV parsing and Gemini Vision

---

## Quick Start

### Backend

```powershell
cd D:\Orb
py -3.13 -m uvicorn main:app --reload
```

### Desktop Client

```powershell
cd D:\orb-desktop
npm run electron:dev
```

### First-Time Setup

1. Start backend and frontend
2. App shows "Welcome to Orb" screen
3. Create a password (min 4 characters)
4. Enter the main app
5. Create your first project

### Install Web Search Grounding

```powershell
py -3.13 -m pip install google-genai
# Restart backend to enable full grounding
```

### Encrypt Existing Data

```powershell
cd D:\Orb
py -3.13 scripts/encrypt_existing_data.py
# Enter password when prompted
# Type "yes" to confirm
```

### Index Existing Content for Semantic Search

```powershell
# Login first to get token
$login = Invoke-RestMethod -Uri "http://localhost:8000/auth/login" `
    -Method POST `
    -Headers @{"Content-Type"="application/json"} `
    -Body '{"password": "YOUR_PASSWORD"}'

# Reindex project (replace project_id as needed)
Invoke-RestMethod -Uri "http://localhost:8000/embeddings/reindex" `
    -Method POST `
    -Headers @{"Authorization"="Bearer $($login.session_token)"; "Content-Type"="application/json"} `
    -Body '{"project_id": 1}'
```

---

## Troubleshooting

### Semantic Search Issues

**No search results:**
- Check if embeddings exist: `GET /embeddings/status/{project_id}`
- Run indexing: `POST /embeddings/reindex` with `{"project_id": X}`
- Verify OPENAI_API_KEY is set

**"OpenAI package not installed":**
```powershell
py -3.13 -m pip install openai
```

**Slow search on large projects:**
- Search is O(n) where n = number of embeddings
- Consider reducing `top_k` parameter
- Future: implement FAISS or similar for faster search

**Embeddings not created for uploads:**
- Check backend logs for indexing errors
- Verify DocumentContent was created for the file
- Manually index: `POST /embeddings/index/file/{file_id}`

### Encryption Issues

**"Decryption failed":**
- Wrong password entered
- Password was changed without re-encrypting data
- Run encryption script again with correct password

**"crypto module not available":**
```powershell
py -3.13 -m pip install cryptography
```

**Data shows as "ENC:..." in responses:**
- Encryption key not initialized
- Try logging out and back in

### Streaming Issues

**Stream doesn't start:**
- Check backend logs for errors
- Verify provider API keys are set
- Try a different provider

**Stream disconnects:**
- Network timeout
- Check backend is still running

### Web Search Issues

**"Using old SDK (no grounding)":**
```powershell
py -3.13 -m pip install google-genai
# Restart backend
```

**"google_search_retrieval is not supported":**
- Old SDK issue, install `google-genai`

**No sources in response:**
- Not all queries trigger grounding
- Try more specific, factual queries

**Wrong date/time in responses:**
- Verify backend was restarted after update
- Check `clients.py` has `enhance_system_prompt()` calls

### Backend Won't Start

**"No module named X":**
```powershell
py -3.13 -m pip install uvicorn fastapi bcrypt sqlalchemy python-multipart aiofiles cryptography google-generativeai google-genai openai
```

**Import errors after update:**
- Check for function name mismatches
- Verify `call_google` alias exists in `clients.py`

### Authentication Issues

**Forgot Password:**
```powershell
del D:\Orb\data\auth.json
del D:\Orb\data\encryption_salt.bin
# Restart backend, then set up new password
# Note: Encrypted data will be unreadable with new password
```
