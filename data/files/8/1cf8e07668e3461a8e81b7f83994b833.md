# Orb Architecture Map

**Version:** 0.12.2 (Architecture Map v19)  
**Last Updated:** 04 December 2025  
**Purpose:** Canonical architecture reference for the Orb multi-LLM AI assistant system

---

## Table of Contents

1. [System Overview](#system-overview)  
2. [Component Architecture](#component-architecture)  
3. [Backend (`Orb`)](#backend-orb)  
4. [Desktop Client (`orb-desktop`)](#desktop-client-orb-desktop)  
5. [Data Layer](#data-layer)  
6. [Authentication System](#authentication-system)  
7. [Database Encryption](#database-encryption)  
8. [Multi-LLM Orchestration](#multi-llm-orchestration)  
9. [Routing Policy System](#routing-policy-system)  
10. [Job-Type Classification](#job-type-classification)  
11. [Phase 4: Job Engine & Artefacts](#phase-4-job-engine--artefacts)  
12. [Provider Registry](#provider-registry)  
13. [DateTime Context](#datetime-context)  
14. [Streaming Responses](#streaming-responses)  
15. [Reasoning Panel](#reasoning-panel)  
16. [Markdown Rendering](#markdown-rendering)  
17. [Web Search](#web-search)  
18. [Chat History](#chat-history)  
19. [File Ingestion Pipeline](#file-ingestion-pipeline)  
20. [Image Analysis Pipeline](#image-analysis-pipeline)  
21. [Semantic Search System](#semantic-search-system)  
22. [Error Taxonomy](#error-taxonomy)  
23. [Quick Start](#quick-start)  
24. [Troubleshooting](#troubleshooting)  
25. [Future Capabilities](#future-capabilities)  
26. [Changelog](#changelog)

---

## System Overview

Orb is a personal AI assistant built as a multi-component system with specialized LLM roles:

- **GPT (OpenAI)** — Fast/lightweight reasoning, conversational interface, linguistics, embeddings  
- **Claude (Anthropic)** — Flagship engineer, complex code, architecture design  
- **Gemini (Google)** — Critic, reviewer, analyst, vision specialist, web search  

### Core Vision

Different AI models specialize in distinct roles while sharing a unified memory layer, enabling:

- Persistent knowledge management across conversations  
- Task coordination and project organization  
- **Automatic job-type classification from message content**
- Job-type-based automatic model routing  
- Policy-driven routing with JSON configuration
- File upload with text extraction and document analysis  
- CV parsing with structured data extraction  
- Image analysis via Gemini Vision  
- Semantic search (RAG) with vector embeddings
- Auto-indexing of notes, messages, and files into embeddings
- Password-based authentication with session tokens
- Project selector for multi-project management
- Database encryption at rest (Security Level 4: Master Key)
- Streaming LLM responses with routing
- Web search with real-time grounding
- Automatic datetime context in all LLM calls
- Chat history persistence and loading
- Markdown rendering for assistant messages with syntax highlighting
- Reasoning/thinking panel showing LLM chain-of-thought
- Model badges identifying which LLM answered each message
- Phase 4: Structured job system with envelopes, results, and artefacts (optional)

### Technology Stack

**Backend:**

- FastAPI (Python 3.13)  
- SQLAlchemy ORM  
- SQLite database  
- Pydantic schemas  
- bcrypt for password hashing  
- cryptography (Fernet) for field-level encryption  
- `python-multipart` for `multipart/form-data` file uploads  
- `python-docx` for DOCX text extraction  
- `PyMuPDF` (fitz) for PDF text extraction  
- `google-genai` for Gemini Web Search (new SDK, preferred)  
- `google-generativeai` for Gemini Vision + fallback (old SDK)  
- OpenAI `text-embedding-3-small` for semantic embeddings

**Desktop Client:**

- Electron (v33.0.0)  
- React 18 with TypeScript 5  
- Vite 5 (build tool and dev server)  
- Custom React hooks for state management  
- `keytar` for Windows Credential Manager integration
- `react-markdown` for markdown rendering
- `remark-gfm` for GitHub-flavored markdown
- `react-syntax-highlighter` for code block highlighting

**Platform:** Windows 11

---

## Component Architecture

```text
D:/
├── Orb/                        # Backend (FastAPI server)
│   ├── main.py                 # Application entrypoint, chat endpoints, RAG, classification
│   ├── .env                    # API keys (OPENAI, ANTHROPIC, GOOGLE)
│   ├── app/
│   │   ├── db.py               # Database configuration
│   │   ├── auth/               # Authentication module (password-based)
│   │   ├── crypto/             # Encryption module (master key)
│   │   ├── memory/             # Memory subsystem (projects/notes/tasks/files/messages)
│   │   ├── llm/                # LLM routing + policy + vision + streaming + web search
│   │   ├── embeddings/         # Semantic search with vector embeddings + auto-indexing
│   │   ├── jobs/               # Phase 4: Job engine (OPTIONAL)
│   │   ├── artefacts/          # Phase 4: Artefact storage (OPTIONAL)
│   │   ├── providers/          # Phase 4: Provider registry (OPTIONAL)
│   │   └── tools/              # Tool integrations
│   ├── data/                   # SQLite DB + file storage + auth config
│   │   ├── orb_memory.db       # SQLite database
│   │   ├── auth.json           # Password hash + session storage
│   │   ├── encryption_salt.bin # Legacy key derivation (migration only)
│   │   ├── routing_policy.json # LLM routing policy configuration
│   │   └── files/              # Per-project file storage
│   └── scripts/                # Migration and utility scripts
│
├── orb-desktop/                # Electron + React desktop client
│   ├── main.js                 # Electron main process
│   ├── src/
│   │   ├── App.tsx             # Root component
│   │   ├── components/         # React components
│   │   ├── hooks/              # Custom React hooks
│   │   ├── services/           # API client layer
│   │   │   └── api.ts          # HTTP client
│   │   ├── types/              # TypeScript interfaces
│   │   │   └── index.ts        # Message, ChatResponse, etc.
│   │   └── styles/             # CSS styles
│   └── ...
│
└── Windows Credential Manager
    └── OrbMasterKey/default    # 32-byte master key (URL-safe base64)
```

### Data Flow (Three Paths with Unified Routing)

```text
┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 1: Non-Streaming Chat (/chat, /chat_with_attachments)                  │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /chat                                                             │
│ main.py /chat endpoint                                                       │
│    ↓ _classify_job_type() — auto-detect from message content                │
│    ↓ builds LLMTask with classified job_type                                │
│ app/llm/router.py call_llm() → call_llm_async()                             │
│    ↓ _select_provider_for_job_type() — route based on job type              │
│    ↓ synthesizes JobEnvelope                                                │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs (OpenAI / Anthropic / Google)                                 │
│    ↓                                                                        │
│ Returns: ChatResponse { provider, model, reply, ... }                       │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 2: Streaming Chat (/stream/chat) — NOW WITH ROUTING (v0.12.2)          │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /stream/chat                                                      │
│ app/llm/stream_router.py                                                    │
│    ↓ _classify_job_type() — auto-detect from message content                │
│    ↓ _select_provider_for_job_type() — route based on job type              │
│ app/llm/streaming.py stream_llm(provider, model)                            │
│    ↓ streaming to selected provider                                         │
│ Provider APIs (streaming)                                                    │
│    ↓                                                                        │
│ SSE Events: metadata → tokens → done                                        │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│ PATH 3: Phase 4 Jobs (/jobs) — OPTIONAL                                     │
├─────────────────────────────────────────────────────────────────────────────┤
│ Frontend                                                                     │
│    ↓ POST /jobs/create                                                      │
│ app/jobs/router.py                                                          │
│    ↓ creates JobEnvelope                                                    │
│ app/jobs/engine.py                                                          │
│    ↓ orchestrates job                                                       │
│ app/providers/registry.py llm_call()                                        │
│    ↓                                                                        │
│ Provider APIs                                                                │
└─────────────────────────────────────────────────────────────────────────────┘
```

**v0.12.2 Change:** Streaming now uses the same routing logic as non-streaming. Both paths classify messages and route to appropriate providers.

---

## Backend (`Orb`)

**Location:** `D:/Orb/`  
**Framework:** FastAPI  
**Entry Point:** `main.py`  
**Version:** 0.12.2

### Directory Structure

```text
Orb/
├── main.py                 # FastAPI app, endpoints, context building, _classify_job_type()
├── .env                    # API keys and configuration
├── app/
│   ├── db.py               # Database engine, session factory
│   ├── auth/               # Authentication module
│   ├── crypto/             # Encryption module (Security Level 4)
│   ├── memory/             # Memory subsystem
│   │   ├── models.py       # SQLAlchemy ORM models
│   │   ├── schemas.py      # Pydantic request/response schemas
│   │   ├── service.py      # Business logic (CRUD)
│   │   └── router.py       # FastAPI route handlers
│   ├── llm/                # LLM routing module
│   │   ├── schemas.py      # JobType, LLMTask, LLMResult, RoutingOptions, RoutingConfig
│   │   ├── clients.py      # Provider API wrappers (via registry)
│   │   ├── router.py       # Main router with call_llm(), _select_provider_for_job_type()
│   │   ├── stream_router.py# Streaming endpoint with routing (_classify_job_type)
│   │   ├── streaming.py    # Stream generators for each provider
│   │   ├── policy.py       # JSON-based routing policy
│   │   ├── vision.py       # Image analysis helpers
│   │   └── web_search_router.py # Web search endpoints
│   ├── embeddings/         # Semantic search module
│   │   ├── models.py       # Embedding ORM model
│   │   ├── service.py      # Indexing and search logic
│   │   └── router.py       # Embeddings API endpoints
│   ├── jobs/               # Phase 4 (optional)
│   │   ├── schemas.py      # JobEnvelope, JobResult, etc.
│   │   ├── engine.py       # Job orchestration
│   │   └── router.py       # Jobs API endpoints
│   ├── artefacts/          # Phase 4 (optional)
│   │   ├── schemas.py      # Artefact schemas
│   │   ├── service.py      # Artefact CRUD
│   │   └── router.py       # Artefacts API endpoints
│   └── providers/          # Phase 4 (optional)
│       └── registry.py     # Single source of truth for LLM calls
├── data/
│   ├── orb_memory.db       # SQLite database
│   ├── auth.json           # Password hash + session tokens
│   └── files/              # Uploaded files (per-project)
└── scripts/
    └── add_model_column.py # Migration script
```

### Key Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/ping` | GET | Health check |
| `/auth/setup` | POST | Set password |
| `/auth/validate` | POST | Login |
| `/chat` | POST | Non-streaming chat (with routing) |
| `/stream/chat` | POST | Streaming chat (with routing) |
| `/chat_with_attachments` | POST | Upload files + chat |
| `/llm` | POST | Direct LLM call |
| `/memory/projects` | GET/POST | Project management |
| `/memory/messages` | GET | Message history |
| `/embeddings/search` | POST | Semantic search |
| `/search/web` | POST | Web search |
| `/jobs/*` | * | Phase 4 (optional) |
| `/artefacts/*` | * | Phase 4 (optional) |

---

## Multi-LLM Orchestration

### Provider Roles

| Provider | Primary Use Cases |
|----------|-------------------|
| **OpenAI (GPT)** | Chat, embeddings, fast responses, casual questions |
| **Anthropic (Claude)** | Complex code, architecture, engineering, code review |
| **Gemini (Google)** | Vision, web search, review/critique |

### Default Models (Environment Variables)

| Provider | Environment Variable | Default |
|----------|---------------------|---------|
| OpenAI | `OPENAI_DEFAULT_MODEL` | `gpt-4.1-mini` |
| Anthropic | `ANTHROPIC_DEFAULT_MODEL` | `claude-sonnet-4-5-20250929` |
| Gemini | `GEMINI_DEFAULT_MODEL` | `gemini-2.0-flash` |

---

## Job-Type Classification

**New in v0.12.2:** Automatic message classification routes requests to appropriate providers.

### Classification Function (`_classify_job_type`)

Located in both `main.py` and `stream_router.py`:

```python
def _classify_job_type(message: str, requested_type: str) -> JobType:
    """
    Classify message to determine appropriate job type.
    Priority: explicit request > message content analysis > default
    """
```

### Classification Keywords

| Category | Keywords | Result | Provider |
|----------|----------|--------|----------|
| **Architecture** | architect, system design, microservice, infrastructure, scalab, database schema | `ARCHITECTURE_DESIGN` | Anthropic |
| **Code Review** | review this, code review, find bugs, security audit, critique | `CODE_REVIEW` | Anthropic |
| **Code Writing** | write function, implement, debug, refactor, ```code blocks``` | `SIMPLE_CODE_CHANGE` | Anthropic |
| **Complex Code** | complete implementation, from scratch, production-ready | `COMPLEX_CODE_CHANGE` | Anthropic |
| **Language-Specific** | python, javascript, react, fastapi, sqlalchemy | `SIMPLE_CODE_CHANGE` | Anthropic |
| **Default** | (none of the above) | `CASUAL_CHAT` | OpenAI |

### Routing Flow

```text
User Message
    ↓
_classify_job_type(message, requested_type)
    ↓
JobType enum (e.g., ARCHITECTURE_DESIGN)
    ↓
_select_provider_for_job_type(job_type)
    ↓
(provider_id, model_id) tuple
    ↓
LLM call to selected provider
```

---

## Routing Policy System

JSON-based configuration for job-type-to-provider routing.

### Job Type Categories (from `app/llm/schemas.py RoutingConfig`)

**GPT_ONLY_JOBS (→ OpenAI):**
- `casual_chat`, `quick_question`, `summary`, `explanation`
- `note_cleanup`, `copywriting`, `prompt_shaping`
- `summarization`, `rewriting`, `documentation`, `research`

**CLAUDE_PRIMARY_JOBS (→ Anthropic):**
- `complex_code_change`, `codegen_full_file`, `architecture_design`
- `code_review`, `spec_review`, `refactor`, `implementation_plan`

**MEDIUM_DEV_JOBS (→ SMART_PROVIDER, default Anthropic):**
- `simple_code_change`, `small_bugfix`

**HIGH_STAKES_JOBS (→ Anthropic):**
- `high_stakes_infra`, `security_sensitive_change`
- `privacy_sensitive_change`, `public_app_packaging`

**GEMINI_JOBS (→ Google):**
- `image_analysis`, `screenshot_analysis`, `video_analysis`
- `vision`, `ui_analysis`, `document_analysis`, `ocr`
- `web_search`, `critique`, `analysis`, `cv_parsing`

---

## Phase 4: Job Engine & Artefacts

**Status:** OPTIONAL (requires `ORB_ENABLE_PHASE4=true`)

### Provider Registry (app/providers/registry.py)

Single source of truth for LLM calls in Phase 4:

- API key encryption/decryption
- Rate limiting (per-provider, per-minute)
- Retry logic with exponential backoff
- Bounded usage log via `deque(maxlen=5000)`
- Usage tracking with job/session/project context

### Key Schema Fixes in v0.12.1

```python
# JobBudget field names:
JobBudget(
    max_tokens=8000,
    max_cost_estimate=1.0,
    max_wall_time_seconds=60,
)

# JobEnvelope:
modalities_in=[Modality.TEXT]  # NOT 'modalities'
needs_tools=[]                 # Must be list[str], NOT False

# Use RoutingOptions for per-task options, not RoutingConfig
task = LLMTask(
    routing=RoutingOptions(),  # NOT RoutingConfig()
)
```

---

## Streaming Responses

### SSE Event Types

| Event | Data | Purpose |
|-------|------|---------|
| `metadata` | `{type: "metadata", provider: "...", model: "..."}` | First event |
| `token` | `{type: "token", content: "..."}` | Incremental response |
| `done` | `{type: "done", provider: "...", model: "...", total_length: N}` | Complete |
| `error` | `{type: "error", error: "..."}` | Failed |

### Reasoning Tags

```xml
<THINKING>
Your reasoning process here...
</THINKING>
<ANSWER>
Your visible answer here...
</ANSWER>
```

### Streaming Routing (v0.12.2)

Streaming now supports full routing:

1. `stream_router.py` receives request
2. `_classify_job_type()` analyzes message content
3. `_select_provider_for_job_type()` determines provider/model
4. `streaming.py` streams from selected provider
5. Provider/model sent in metadata event

---

## Quick Start

### Starting Orb

```powershell
cd D:\orb-desktop
npm run electron:dev
```

### Manual Backend (Development Only)

```powershell
# Get master key
cd D:\orb-desktop
node -e "require('keytar').getPassword('OrbMasterKey','default').then(k=>console.log(k))"

# Start backend
cd D:\Orb
$env:ORB_MASTER_KEY = "your-43-char-key"
.\.venv\Scripts\Activate.ps1
uvicorn main:app --host 127.0.0.1 --port 8000
```

---

## Troubleshooting

### Model Badge Not Showing

1. Verify `model` column exists in messages table:
   ```powershell
   sqlite3 D:\Orb\data\orb_memory.db "PRAGMA table_info(messages);"
   ```

2. If missing, run migration:
   ```powershell
   python scripts/add_model_column.py
   ```

3. Check that `memory/schemas.py` `MessageCreate` has `model` field

### Routing Not Working

1. Check console for classification logs:
   ```
   [classify] Detected ARCHITECTURE_DESIGN
   [chat] Job type: architecture_design (requested: casual_chat)
   [chat] Response from: anthropic / claude-sonnet-4-5-20250929
   ```

2. For streaming:
   ```
   [stream_router] Classified: SIMPLE_CODE_CHANGE
   [stream_chat] Job-type routing: simple_code_change -> anthropic/claude-sonnet-4-5-20250929
   ```

3. Verify RoutingConfig in `app/llm/schemas.py` has correct job type categories

### Master Key Issues

- Key must be exactly 43 characters
- Delete and regenerate: `cmdkey /delete:OrbMasterKey/default`
- Restart Electron

### Phase 4 Validation Errors

- Check `modalities_in` (not `modalities`)
- Check `needs_tools` is `list[str]` (not `False`)
- Check `JobBudget` field names

---

## Testing Routing

### Test Prompts (Both Chat and Stream modes)

| Prompt | Expected Classification | Expected Provider |
|--------|------------------------|-------------------|
| "What's a good recipe for pasta?" | CASUAL_CHAT | OpenAI |
| "Write a Python binary search function" | SIMPLE_CODE_CHANGE | Anthropic |
| "Design a microservices architecture for e-commerce" | ARCHITECTURE_DESIGN | Anthropic |
| "Review this code for bugs: def divide(a,b): return a/b" | CODE_REVIEW | Anthropic |
| "Debug this code - it's throwing an index error" | SIMPLE_CODE_CHANGE | Anthropic |
| "Explain how photosynthesis works" | CASUAL_CHAT | OpenAI |

### Console Output to Watch

```
[stream_router] Classified: ARCHITECTURE_DESIGN
[stream_chat] Job-type routing: architecture_design -> anthropic/claude-sonnet-4-5-20250929
[stream_chat] Starting stream: provider=anthropic, model=claude-sonnet-4-5-20250929
```

---

## Changelog

### v0.12.2 (04 December 2025) — Unified Routing

**Features:**
- Added automatic job-type classification from message content
- Both `/chat` and `/stream/chat` now use same routing logic
- Code/architecture/review prompts automatically route to Anthropic
- Casual chat routes to OpenAI
- Added `_classify_job_type()` function to `main.py` and `stream_router.py`
- Added `_select_provider_for_job_type()` function to `stream_router.py`

**Architecture Doc Updates (v19):**
- Updated Data Flow diagram to show routing in streaming path
- Added Job-Type Classification section
- Updated default models to use environment variables
- Added routing troubleshooting section
- Added test prompts table

### v0.12.1 (03 December 2025) — Model Badge Fix

**Bug Fixes:**
- Added `model` column to Message ORM model (was missing, causing data loss)
- `/chat` endpoint now returns and saves `model` field
- `/chat_with_attachments` endpoint now returns and saves `model` field
- `/llm` endpoint now returns `model` field
- Created migration script `scripts/add_model_column.py`

**Architecture Doc Corrections (v18):**
- Fixed Data Flow diagram to show three separate paths
- Clarified streaming bypasses router.py and registry.py
- Aligned version numbers to 0.12.1
- Added migration instructions for `model` column

### v0.17.0 (02 December 2025) — Security Level 4

- Master key stored in Windows Credential Manager
- Password used only for authentication
- Electron manages master key lifecycle

### v0.16.0 (01 December 2025)

- Reasoning panel for chain-of-thought
- Model badges
- Markdown rendering

### v0.15.0 (30 November 2025)

- Semantic search with embeddings
- Auto-indexing

---

## Testing Checklist

After updates, verify:

- [ ] `model` column exists in messages table
- [ ] `/chat` returns both `provider` and `model`
- [ ] Messages saved to DB include `model` column
- [ ] Model badge displays in UI
- [ ] Non-stream chat routes code prompts to Anthropic
- [ ] Non-stream chat routes casual prompts to OpenAI
- [ ] Streaming chat routes code prompts to Anthropic
- [ ] Streaming chat routes casual prompts to OpenAI
- [ ] Console shows classification logs
- [ ] Phase 4 endpoints work (if enabled)

---

*Document maintained as part of Orb development. Update with each architectural change.*
